
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "Mediumish Template for Jekyll",
    "body": "This website is built with Jekyll and Mediumish template for Jekyll. It's for demonstration purposes, no real content can be found. Mediumish template for Jekyll is compatible with Github pages, in fact even this demo is created with Github Pages and hosted with Github.  Documentation: Please, read the docs here. Questions or bug reports?: Head over to our Github repository! Buy me a coffeeThank you for your support! Your donation helps me to maintain and improve Mediumish . Buy me a coffee Documentation"
    }, {
    "id": 2,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                           Disaster Recovery — Projetando e Gerenciando Arquiteturas Multi-Region na AWS com Terraform                              :               Este artigo foi o mais longo e cansativo que escrevi em muito tempo, então considere esse disclaimer como um pedido de desculpas escrito após a. . . :                                                                                                                                                                       Matheus Fidelis                                15 Jul 2023                                                                                                                                                                                                                                                                                                                        Karpenter — Estratégias para resiliência no uso de Spot Instances em produção                              :               Introdução Esse é o segundo artigo que eu publico sobre Karpenter. Dessa vez decidi trazer um ponto de vista bem legal que é a adoção. . . :                                                                                                                                                                       Matheus Fidelis                                09 Oct 2022                                                                                                                                  All Stories:                                                                                                     Disaster Recovery — Projetando e Gerenciando Arquiteturas Multi-Region na AWS com Terraform              :       Este artigo foi o mais longo e cansativo que escrevi em muito tempo, então considere esse disclaimer como um pedido de desculpas escrito após a finalização do mesmo. Recomendo que. . . :                                                                               Matheus Fidelis                15 Jul 2023                                                                                      &lt;img class= img-fluid lazyimg  src= data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=  data-src= /assets/images/argo-logo-vg. png  alt= Argo-Rollouts —  Qual a forma mais simples de executar Canary Releases e Blue/Green Deployments no Kubernetes?  &gt;                                              Argo-Rollouts —  Qual a forma mais simples de executar Canary Releases e Blue/Green Deployments no Kubernetes?               :       O Deploy em ambientes Cloud Native pode ser, se não é, a parte mais desafiadora no dia a dia do ciclo de vida de um software, principalmente se a atualização. . . :                                                                               Matheus Fidelis                02 Apr 2023                                                                                                                                    Karpenter — Estratégias para resiliência no uso de Spot Instances em produção              :       Introdução Esse é o segundo artigo que eu publico sobre Karpenter. Dessa vez decidi trazer um ponto de vista bem legal que é a adoção de uso de Spots em. . . :                                                                               Matheus Fidelis                09 Oct 2022                                                                                                                                    Provisionando um cluster de EKS sem Node Groups com Karpenter              :       A proposta dessa PoC é criar e gerenciar um cluster de EKS utilizando apenas (ou quase) o Karpenter como provisionamento de recursos computacionais pro Workload produtivo, tirando a necessidade de. . . :                                                                               Matheus Fidelis                05 Aug 2022                                                                                                                                    Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS              :       Na sua casa você pode usar o que você quiser, aqui hoje vamos usar Istio. Sem tempo pra chorar irmão…:                                                                               Matheus Fidelis                13 Nov 2021                                            "
    }, {
    "id": 4,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/disaster-recovery-projetando-e-gerenciando-arquiteturas-multi-region-na-aws-com-terraform/",
    "title": "Disaster Recovery — Projetando e Gerenciando Arquiteturas Multi-Region na AWS com Terraform",
    "body": "2023/07/15 - Este artigo foi o mais longo e cansativo que escrevi em muito tempo, então considere esse disclaimer como um pedido de desculpas escrito após a finalização do mesmo. Recomendo que leia aos poucos, com calma. Entrei em um hiperfoco violento que me fez cuspir tudo que estava na minha cabeça de uma vez, então peço perdão por isso e insisto que você não desista dessa leitura pela extensão. Refleti muito se deveria lança-lo por partes menores, mesmo não gostando desse modelo. Na verdade nada me deixa mais decepcionado do que estar empolgado na leitura de um artigo e do nada ele acabar no “like pra parte 2”. Então salve esse cara nos seus favoritos e por favor, não desista dele.  Terraform e Disaster RecoveryO Disaster Recovery (Recuperação de Desastres) é uma estratégia essencial para garantir a continuidade dos negócios e a resiliência de sistemas críticos em caso de falhas ou interrupções inesperadas. Com o aumento da dependência de serviços e aplicações na nuvem, a adoção de soluções de recuperação de desastres se tornou ainda mais crucial para proteger os dados e garantir a disponibilidade dos serviços. Além de ser uma poderosa ferramenta de Infraestrutura como Código, o Terraform pode desempenhar um papel fundamental na implementação de estratégias de chaveamento (failover) e recuperação de desastres (DR) na nuvem. Com sua capacidade de gerenciar recursos de infraestrutura em várias regiões e provedores de nuvem, o Terraform se torna uma escolha natural para automatizar a criação e a configuração de ambientes de chaveamento e DR altamente resilientes. Neste artigo, exploraremos como o Terraform pode ser utilizado como uma ferramenta abrangente para orquestrar ambientes de chaveamento e DR na AWS (Amazon Web Services). Veremos como podemos aproveitar a flexibilidade e a facilidade de uso do Terraform para criar arquiteturas de failover que garantem a continuidade dos serviços em caso de falhas ou interrupções inesperadas. Como exemplo prático, implementaremos uma proposta funcional de uma arquitetura reduzida que pode se manter funcionando em casos de abordagens ativo/ativo ou ativo/passivo, fazendo uso de algumas estratégias e serviços que possuam features de replicação, balanceamento chaveamento automático, e trabalhando de forma espelhadas entre duas regiões da AWS.  Premissas e estratégias da nossa arquitetura: É possível pensar e filosofar em centenas de possibilidade de se projetar uma estrutura em nuvem, porém a que vamos escolher aqui tem o objetivo central de se manter o mais simples possível e aproveitar o máximo de coisas “as a service” que a AWS oferece por meio dos seus produtos. Nesse caso, vamos assumir de antemão as seguintes premissas:  Vamos dar prioridade para o máximo de serviços Serverless possível. Nossa estratégia será desenhada em torno dessas soluções para mantermos o máximo de simplicidade possível. Por serem altamente gerenciados pelos provedores de nuvem, esses serviços costumam ser mais rápidos e fáceis de provisionar e utilizar.  Vamos dar extrema prioridade para serviços que ofereçam replicação de dados, principalmente que funcionem de forma bilateral e nos facilite na virada e no retorno entre a região primária e de DR de forma rápida.  Voltando ao tópico anterior, a AWS possui vários e vários serviços que possibilitam trabalhar em Multi-Region de alguma forma, porém a maioria deles serão despriorizados por necessitarem de algum tipo de intervenção manual para o DR, ou que não possibilitem trabalhar ATIVO/ATIVO, como no caso de serviços que mantenham uma região Read Only, Replica e etc.  Iremos presumir que a estratégia de DR seja executada manualmente e com base em certos critérios estabelecidos de maneira lúdica previamente. Uma forma bonita de dizer que pro DR acontecer, será necessário executar o Terraform por algum lugar, por alguém, por algum motivo. Terraform em Multi-Region: Configurar o Terraform para trabalhar em multi região na AWS é um passo crucial dentro desta proposta para estabelecer uma estratégia eficaz de Disaster Recovery. Através do uso de variáveis e providers específicos para cada região, você pode garantir que seus recursos críticos sejam replicados e disponibilizados em regiões geograficamente distantes, aumentando a resiliência e a disponibilidade da sua aplicação. O Terraform suporta múltiplas definições do mesmo provider, sendo diferenciado por um alias para ser referenciado posteriormente. Nesse caso, vamos estabelecer dois providers idênticos da AWS e criar os alias primary e disaster-recovery. Para essa demonstração iremos assumir a região primary seja em São Paulo (sa-east-1) e a região de disaster-recovery sendo em Norte Virginia (us-east-1). 123456789provider  aws  { alias =  primary  region =  sa-east-1 }provider  aws  { alias =  disaster-recovery  region =  us-east-1 } Recursos Modulares — Especialistas ou Genéricos: Para que esta prova de conceito funcione, precisaremos investir em uma arquitetura de Infraestrutura como Código Modular, ou criar objetos de IaC (Infraestrutura como Código) que resolvem problemas de forma padronizada e que podem ser reutilizados diversas vezes, com base em parâmetros de entrada. Isso nos permitirá fornecer o mesmo recurso em ambas as regiões com a mesma linha de base de configuração.  E como fazemos isso?: Para este exemplo, utilizaremos variáveis que são construídas por meio de maps, para então usarmos a função lookup para recuperar os valores. Neste caso, a chave dos maps será o nome da região, ou seja, sa-east-1 e us-east-1. Poderia ser primary ou secondary, active ou passive, ou qualquer outra denominação que faça sentido. 123456789101112131415161718192021222324variable  vpc_cidr  { type = map(any) default = {  us-east-1 =  10. 0. 0. 0/16   sa-east-1 =  172. 0. 0. 0/16  }}variable  public_subnet_1a  { type = map(any) default = {  us-east-1 =  10. 0. 0. 0/20   sa-east-1 =  172. 0. 0. 0/20  }}variable  public_subnet_1b  { type = map(any) default = {  us-east-1 =  10. 0. 16. 0/20   sa-east-1 =  172. 0. 16. 0/20  }}// . . . Nesse caso, encapsulamos a criação de toda a rede da VPC em um módulo e utilizamos o mesmo como fonte duas vezes, passando os mesmos inputs, porém com a busca pela chave da região específica no lookup. Como definimos no primeiro passo os dois provedores da AWS com seus respectivos alias, precisamos informar ao módulo qual deles deve ser utilizado para criar os recursos. 1234567891011121314151617181920212223242526272829303132333435363738module  vpc_sa_east_1  { source =  . /modules/vpc  providers = {  aws = aws. primary } project_name =  primary  vpc_cidr   = lookup(var. vpc_cidr,  sa-east-1 ) public_subnet_1a = lookup(var. public_subnet_1a,  sa-east-1 ) public_subnet_1b = lookup(var. public_subnet_1b,  sa-east-1 ) public_subnet_1c = lookup(var. public_subnet_1c,  sa-east-1 ) private_subnet_1a = lookup(var. private_subnet_1a,  sa-east-1 ) private_subnet_1b = lookup(var. private_subnet_1b,  sa-east-1 ) private_subnet_1c = lookup(var. private_subnet_1c,  sa-east-1 )}module  vpc_us_east_1  { source =  . /modules/vpc  providers = {  aws = aws. disaster-recovery } project_name =  disaster-recovery  vpc_cidr   = lookup(var. vpc_cidr,  us-east-1 ) public_subnet_1a = lookup(var. public_subnet_1a,  us-east-1 ) public_subnet_1b = lookup(var. public_subnet_1b,  us-east-1 ) public_subnet_1c = lookup(var. public_subnet_1c,  us-east-1 ) private_subnet_1a = lookup(var. private_subnet_1a,  us-east-1 ) private_subnet_1b = lookup(var. private_subnet_1b,  us-east-1 ) private_subnet_1c = lookup(var. private_subnet_1c,  us-east-1 )} Uma pausa do decoro: Eu sei que quem acompanha os meus artigos anteriores vai se sentir um pouco estranho lendo este. Estou intencionalmente tentando escrever algo de forma mais séria, e isso vai acontecer. Mas antes, preciso de uma pausa antes de continuarmos para alinhar algumas premissas:  Puts, mas eu prefiro trabalhar com Workspaces / Tfvars em vez de modular: O céu é o seu limite! Prefiro separar tudo e trabalhar com tfvars separadas em vez de lookups: Te amo, você é incrível! Acho que fazer um módulo que entrega nas duas regiões é melhor: segura na mão do pai e vai Acho isso ruim, prefiro separar tudo e disponibilizar os outputs de outras formas: confia no seu potencial, você é incrível No mais: Apega-se mais na mensagem do que na cor da meia do carteiro. Voltamos… Parte 1: Ingress e o Fluxo SíncronoNa primeira parte deste artigo, vamos nos concentrar em como funcionará o fluxo síncrono de consumo do nosso serviço, considerando a necessidade de consumir uma API REST que é atendida por N aplicações (nesse escopo reduzido, apenas uma). O nosso fluxo síncrono é bastante simples, e o caminho para atender a uma requisição será o seguinte:  Route53: Faremos o apontamento e chaveamento de DNS por meio do Route53. Ele nos permite gerenciar e redirecionar o tráfego de forma eficiente para o local correto. Aprenderemos como utilizá-lo para chavear o tráfego entre a região primária e as de disaster recovery.  Custom Domain Name: Utilizaremos o Custom Domain Name para gerenciar o domínio e o certificado HTTPS/SSL da nossa API. Isso garantirá a segurança das comunicações entre o cliente e a nossa infraestrutura.  API Gateway Regional: A exposição da API na DMZ (zona desmilitarizada, vulgo internet, terra de ninguém) será realizada por meio do API Gateway Regional. Ele nos permitirá disponibilizar a API para o consumo externo.  Network Load Balancer: O Network Load Balancer funcionará como um VPC Link, direcionando o tráfego externo para dentro da nossa VPC (Virtual Private Cloud). Ele será responsável por encaminhar as requisições recebidas do API Gateway na internet para dentro da nossa infraestrutura de forma segura e correta.  Application Load Balancer: A gestão da camada 7 das aplicações que compõem o nosso Workload será feita pelo ALB, Application Load Balancer. Ele nos permitirá distribuir o tráfego de maneira eficiente entre os diferentes containers que executam a nossa aplicação REST. Essa camada 7 é responsável por processar solicitações HTTP e HTTPS.  Aplicação REST: A aplicação REST será executada em containers, que podem estar em qualquer lugar, nesse caso, utilizaremos o ECS (Elastic Container Service). O ECS nos oferece um ambiente flexível para executar nossos containers, garantindo escalabilidade e disponibilidade de forma estupidamente simples. Seguindo a premissa inicial utilizando a VPC de exemplo, vamos entregar todos os recursos de forma duplicada, usando as duas configurações de provedores configurados nas duas zonas de disponibilidade.  1. 1 — API Gateway Regional: Neste exemplo, optamos por encapsular todos os mapeamentos da nossa API dentro de um módulo dedicado do API Gateway. Ao contrário de um módulo genérico que poderia ser replicado para diversos cenários, o objetivo deste módulo é fornecer todos os mapeamentos para todo o workload. Essa abordagem simplifica o encapsulamento de todas as definições do OpenAPI 3. 0 e facilita o entendimento da proposta. É importante ressaltar que também é possível criar um módulo que aceite o OpenAPI de forma genérica, o que seria uma opção viável em um ambiente de trabalho. Optei por seguir dessa forma por praticidade do exemplo. É importante considerar que o API Gateway é um recurso provisionado fora da VPC, o que significa que não temos controle físico direto sobre ele, como saber em qual Zona de Disponibilidade de determinada região ele está alocado. Nesse caso específico, é necessário fazer o deployment do API Gateway em modo Regional. Embora o API Gateway seja um recurso externo à VPC, o modo Regional de implantação nos permite ter uma visibilidade clara sobre a região em que a distribuição será entregue, o que é fundamental para garantir o controle que estamos almejando para o chaveamento dos workloads. Para mais detalhes, o módulo do gateway da aplicação está aqui. 1234567891011121314151617181920212223module  api_gateway_app_demo_sa_east_1  { source =  . /modules/api-gateway-app-demo  providers = {  aws = aws. primary } gateway_name =  app-demo  stage_name  =  prod  vpc_link   = module. cluster_sa_east_1. vpc_link}module  api_gateway_app_demo_us_east_1  { source =  . /modules/api-gateway-app-demo  providers = {  aws = aws. disaster-recovery } gateway_name =  app-demo  stage_name  =  prod  vpc_link   = module. cluster_us_east_1. vpc_link} 1. 2 — ACM Regional: Para garantir um deployment confiável, também faremos o deploy do ACM (AWS Certificate Manager). O ACM é um serviço gerenciado pela AWS que permite provisionar, gerenciar e implantar certificados SSL/TLS (Secure Sockets Layer/Transport Layer Security) para uso em serviços da AWS, incluindo o API Gateway. Ao utilizar o ACM, facilitamos a aquisição, implantação e renovação automática de certificados SSL/TLS, garantindo comunicações seguras entre os clientes e os recursos da AWS. Nesse caso, utilizaremos o ACM para garantir a segurança do domínio em um nível regional. É importante ressaltar que as duas regiões provisionarão certificados com o mesmo nome. Essa duplicação é necessária para que possamos associar os Custom Domain Names e API Gateways específicos de cada região. Exploraremos esse tópico no próximo tópico do artigo. 12345678910111213141516171819202122module  acm_sa_east_1  { source =  . /modules/acm  providers = {  aws = aws. primary } domain_name   = format( *. %s , var. route53_domain) route53_zone_id = var. route53_hosted_zone}module  acm_us_east_1  { source =  . /modules/acm  providers = {  aws = aws. disaster-recovery } domain_name   = format( *. %s , var. route53_domain) route53_zone_id = var. route53_hosted_zone} 1. 3 — Custom Domain Name: O Custom Domain Name é a porta de entrada funcionando como um roteador de API Gateways, é onde vamos definir um domínio human-like e realizar os devidos mapeamentos. Nesse exemplo, vamos fazer um redirecionamento completo de tudo que bater nesse domínio, mas é uma das capacidades do Custom Domain Name permitir que diversos API Gateways sejam expostos através de um único domínio.  12345678910111213141516171819202122232425262728293031323334353637module  custom_domain_sa_east_1  { source =  . /modules/api-gateway-custom-domain  providers = {  aws = aws. primary } acm_arn         = module. acm_sa_east_1. arn api_gateway_domain_name = var. api_gateway_domain base_path_mappings   = [  {   base_path =  / ,   api_id  = module. api_gateway_app_demo_sa_east_1. id,   stage   = module. api_gateway_app_demo_sa_east_1. stage,  } ]}module  custom_domain_us_east_1  { source =  . /modules/api-gateway-custom-domain  providers = {  aws = aws. disaster-recovery } acm_arn         = module. acm_us_east_1. arn api_gateway_domain_name = var. api_gateway_domain base_path_mappings   = [  {   base_path =  / ,   api_id  = module. api_gateway_app_demo_us_east_1. id,   stage   = module. api_gateway_app_demo_us_east_1. stage,  } ]} 1. 4 — Route53, Gestão de DNS: Uma parte fundamental para uma estratégia de Disaster Recovery em multi-região é a capacidade de realizar o chaveamento (failover) entre os sites de forma automática e rápida. Para realizar essa tarefa, vamos utilizar o Amazon Route53, que é o serviço de DNS da AWS, junto ao Terraform para automatizar o processo de failover. Vamos assumir que nossa zona já está criada e vamos referenciá-la com base na variável hosted_zone_id para criar os recursos de chaveamento. 1234567891011121314variable  route53_hosted_zone  { type  = string default =  Z102505525LUE9SZ7HWTY }variable  route53_domain  { type  = string default =  msfidelis. com. br }variable  api_gateway_domain  { type  = string default =  api. msfidelis. com. br } Chaveamento entre as Regiões da AWS: Com base nas premissas iniciais, a proposta é fornecer um chaveamento simples, no qual apenas um commit, uma rodada de pipeline, um apply, seja possível redirecionar todo o tráfego para uma ou outra região de DR. Vamos manipular uma variável chamada state, onde em cada chave de região vamos manter as strings ACTIVE e PASSIVE. Com base nesses valores vamos decidir muita coisa. 1234567variable  state  { type = map(any) default = {   sa-east-1  :  ACTIVE ,   us-east-1  :  PASSIVE , }}Com base no valor dessa variável, podemos definir por exemplo o quanto queremos setar de peso entre os&nbsp;records 1lookup(var. state,  região ) ==  ACTIVE  ? 100 : 0Com base no valor ACTIVE, podemos configurar no Route53 um redirecionamento com weight policy de peso 100, e para o valor PASSIVE um peso 0. Invertendo os valores dessas variáveis, temos a flexibilidade de desabilitar o roteamento de DNS da região sa-east-1 e direcionar todo o tráfego para us-east-1, conforme os pesos atribuídos. Adicionalmente, é viável configurar ambas as regiões como ACTIVE, o que possibilita a implementação de um balanceamento Round Robin na resolução de nomes, distribuindo as cargas de trabalho entre as duas regiões de forma equilibrada. 123456789101112131415161718192021222324252627282930313233343536373839404142resource  aws_route53_record   primary  { provider = aws. primary zone_id = var. route53_hosted_zone name  = var. api_gateway_domain type  =  A  weighted_routing_policy { &lt;-------------  weight = lookup(var. state,  sa-east-1 ) ==  ACTIVE  ? 100 : 0 } set_identifier =  primary  alias {  evaluate_target_health = true  name          = module. custom_domain_sa_east_1. regional_domain_name  zone_id        = module. custom_domain_sa_east_1. regional_zone_id }}resource  aws_route53_record   dr  { provider = aws. disaster-recovery zone_id = var. route53_hosted_zone name  = var. api_gateway_domain type  =  A  weighted_routing_policy { &lt;-------------  weight = lookup(var. state,  us-east-1 ) ==  ACTIVE  ? 100 : 0 } set_identifier =  disaster-recovery  alias {  evaluate_target_health = true  name          = module. custom_domain_us_east_1. regional_domain_name  zone_id        = module. custom_domain_us_east_1. regional_zone_id }} Exemplo do DR desligado: 1234567variable  state  { type = map(any) default = {   sa-east-1  :  ACTIVE ,   us-east-1  :  PASSIVE , }} Exemplo do DR&nbsp;Ligado: 1234567variable  state  { type = map(any) default = {   sa-east-1  :  PASSIVE ,   us-east-1  :  ACTIVE , }} Exemplo do DR Ativo/Ativo: 1234567variable  state  { type = map(any) default = {   sa-east-1  :  ACTIVE ,   us-east-1  :  ACTIVE , }} Parte 2: ComputingParte crucial pra uma estratégia de DR funcionar, é escolher como rodar nossas aplicações. Essa decisão precisa levar em conta o quão replicável a forma como publicamos e fazemos a governança desses serviços é. No o modelo computacional que cabe como uma luva são containers, e principalmente opções Serverless como o AWS Lambda, e na intersecção entre os dois modelos, o AWS Fargate que tem opções de rodar no modelo de EKS (Elastic Kubernetes Service) e de ECS (Elastic Container Service). Como você vai fazer e qual modelo e tecnologia você vai usar é necessário uma análise cuidadosa a respeito das skills do time e do nível de complexidade que você quer atingir ou evitar com sua arquitetura no fim do dia. Neste exemplo vamos utilizar containers, rodando em ECS Fargate. Poderia ser um EKS, Nomad, o Próprio Lambda tranquilamente. O importante é que consigamos replicar nosso serviço em qualquer lugar, isso inclui poder parametrizar os recursos externos por meio de variáveis ambiente, gestão de secrets para que o serviço em si funcione de forma padronizada, porém consumindo diferentes recursos em diferentes apontamentos. A partir daqui vou reduzir os exemplos de código, porém você pode acompanhar tudo que foi desenvolvido nessa PoC através deste repo, e vou deixando pontualmente os links para os respectivos módulos. Modulo do Cluster ECS Disponível Aqui Modulo do Service ECS Disponível Aqui 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859module  cluster_sa_east_1  { source =  . /modules/cluster  providers = {  aws = aws. primary } cluster_name =  my-demo  vpc_id        = module. vpc_sa_east_1. vpc_id subnets       = module. vpc_sa_east_1. private_subnets route53_private_zone = var. route53_private_zone}// . . . module  app_demo_sa_east_1  { source =  . /modules/service  providers = {  aws = aws. primary } vpc_id = module. vpc_sa_east_1. vpc_id cluster_name = module. cluster_sa_east_1. cluster_name route53_zone = module. cluster_sa_east_1. private_zone service_name =  sales-api  service_image =  fidelissauro/sales-rest-api:latest  service_port = 8080 service_hostname = [  format( app-demo. %s , var. route53_private_zone) ]// . . .  envs = [  {   name :  AWS_REGION ,   value :  sa-east-1   },  {   name :  DYNAMO_SALES_TABLE ,   value : aws_dynamodb_table. sales. name  },  {   name :  SNS_SALES_PROCESSING_TOPIC ,   value : module. sales_sns_sa_east_1. arn  },  {   name :  SSM_PARAMETER_STORE_STATE ,   value : module. ssm_parameter_state_sa_east_1. name  } ]} Parte 3: Dados - A real dificuldadeTalvez a parte mais importante e complexa de todo plano de Disaster Recovery, trabalhar com os dados. É fundamental garantir a integridade dos dados durante o processo de recuperação. Os dados devem ser replicados ou backup em tempo real para garantir que não haja perda ou corrupção de dados durante uma interrupção. E mais difícil que virar para o DR, e garantir que os dados estarão lá disponíveis para serem consumidos, mesmo que com algum delay ou com consistência eventual, é voltar ao estado original. Voltar ao Site/Região/DC primário com os dados que foram alterados durante o tempo de atividade da Região de DR é extremamente complexo, porém dentro do ferramental que a AWS nos dispõe é razoavelmente simples se tomarmos as decisões arquiteturais corretas. Nessa parte do artigo, vamos detalhar como criar um fluxo de dados resiliente e eficiente, pensando em replicação bilateral de todas as fontes de dados, offload de processamento, mensageria e eventos, tudo pensado para que exista uma sincronia e replicação com tempo considerável das duas regiões, para que seja possível chavear, deschavear e manter as duas ativas ofertando os mesmos dados de ambas as partes. Nesse parte vamos tornar possível:  SNS e SQS Multi-Region DynamoDB Global Tables Two-way replication no S3No final, iremos chegar em uma solução parecida com essa entre as duas Regiões.  3. 1 — Eventos e Mensageria; SNS e SQS Multi Region: No nosso fluxo hipotético presume que nossa aplicação REST, durante o registro de uma venda, salve um registro de venda no Dynamo, publique uma mensagem para processamento posterior desse item. Para essa solução vamos inserir um fluxo:  App REST -&gt; SNS Topic -&gt; SQS Queue -&gt; Aplicação Worker -&gt; DynamoPorém para garantir as premissas de replicação, e garantir que uma mensagem recebida por uma região seja replicada automaticamente para a outra, precisamos fazer uso do SNS Cross-Region Delivery. O SNS Cross-Region Delivery (Entrega Inter-regional do SNS) é um recurso fornecido pelo Amazon Simple Notification Service (SNS) que permite enviar notificações entre regiões na AWS. O SNS é um serviço gerenciado pela AWS que permite enviar mensagens para várias plataformas, como aplicativos móveis, e-mails, SMS e endpoints HTTP, e neste caso, o Amazon SQS, e essa configuração deve ser feita de forma bilateral, em ambos os tópicos SNS e ambas as filas SQS.  12345678910111213141516171819202122232425262728resource  aws_sqs_queue_policy   main  { queue_url = aws_sqs_queue. main. id policy  = &lt;&lt;EOF{  Version :  2012-10-17 ,  Statement : [  {    Effect :  Allow ,    Principal :  * ,    Action : [     sqs:SendMessage    ],    Resource : [     ${aws_sqs_queue. main. arn}    ],    Condition : {     ArnLike : {      aws:SourceArn : [       arn:aws:sns:sa-east-1:${data. aws_caller_identity. current. account_id}:${var. sns_topic_name} ,       arn:aws:sns:us-east-1:${data. aws_caller_identity. current. account_id}:${var. sns_topic_name}      ]    }   }  } ]}EOF}123456789101112resource  aws_sqs_queue   main  { name           = var. queue_name delay_seconds       = var. delay_seconds max_message_size     = var. max_message_size message_retention_seconds = var. message_retention_seconds receive_wait_time_seconds = var. receive_wait_time_seconds visibility_timeout_seconds = var. visibility_timeout_seconds redrive_policy = jsonencode({  deadLetterTargetArn = aws_sqs_queue. dlq. arn  maxReceiveCount   = var. dlq_redrive_max_receive_count })}E em seguida criar um tópico SNS fazendo subscribe das filas de&nbsp;SQS 12345678910111213141516171819resource  aws_sns_topic   main  { name = format( %s , var. name) #  fifo_topic         = true #  content_based_deduplication = true}resource  aws_sns_topic_subscription   primary  { protocol       =  sqs  raw_message_delivery = true topic_arn      = aws_sns_topic. main. arn endpoint       = var. sqs_queue}resource  aws_sns_topic_subscription   replica  { protocol       =  sqs  raw_message_delivery = true topic_arn      = aws_sns_topic. main. arn endpoint       = var. sqs_queue_replica}Fechando um pouco o capô do carro, e ligando ele pra botar pra funcionar, nossa declaração de módulos trabalharia de forma com que ambas as declarações dos providers recebesse respectivamente qual seria a queue primária e a replica, alternando entre ambos. Módulo do SQS disponível aqui Módulo do SNS disponível aqui 1234567891011121314151617181920212223242526272829303132module  sales_processing_queue_sa_east_1  { source =  . /modules/sqs-queue  providers = {  aws = aws. primary } queue_name          =  sales-processing-queue  delay_seconds         = 0 max_message_size       = 2048 message_retention_seconds   = 86400 receive_wait_time_seconds   = 10 dlq_redrive_max_receive_count = 4 visibility_timeout_seconds  = 60 sns_topic_name = var. sales_sns_topic_name}//. . module  sales_sns_sa_east_1  { source =  . /modules/sns-multiregion-sqs-delivery  providers = {  aws = aws. primary } name       =  sales-processing-topic  sqs_queue     = module. sales_processing_queue_sa_east_1. sqs_queue_arn sqs_queue_replica = module. sales_processing_queue_us_east_1. sqs_queue_arn}// . . . Independente do uso de um modulo, desse modulo, de outro modulo, ou fazer na mão, o esperado é que o tópico SNS tenha sempre duas subscriptions, para as queues das duas regiões como no print a seguir, e a devida subscription do SNS de cada região deve aparecer nas configurações das duas filas SQS mostradas.  3. 2 — DynamoDB Global Tables: As DynamoDB Global Tables (Tabelas Globais do DynamoDB) são uma funcionalidade do serviço que permitem que você crie e mantenha tabelas do DynamoDB automaticamente replicadas e sincronizadas em várias regiões da AWS. Perfeito para nossa proposta de DR. Com as DynamoDB Global Tables, você pode ter cópias de tabelas em várias regiões da AWS, garantindo replicação e failover automático, permitindo que as apps leiam e gravem dados de forma local em cada região.  Elas são particularmente úteis quando você precisa espelhar sua aplicação entre diversas regiões e disponibilizá-las paralelamente, utilizando a mesma base de dados. Por exemplo, em caso de Weighted Routing Policy utilizado neste exemplo entre Regiões, onde teremos Round Robin do consumo de recursos em caso de ACTIVE/ACTIVE, ou num caso legal as Geolocation routing policy, onde você pode disponibilizar o workload geograficamente mais proximo do cliente, por exemplo:  Quem está no Brasil, acessa São Paulo Quem está em Miami, acessa VirginiaMas no nosso caso de Disaster Recovery, podemos contar que teremos um banco de dados replicado de forma bilateral com baixa latência e consistência eventual, independente das regiões que eu estiver manipulando esse dado. O provisionamento de uma Global Table é simples, e não requer provisionamento duplicado com base em módulos, por isso não iremos fazer uso desse artifício nesse recurso. Os pontos de atenção para o provisonamento adequado é a necessidade de habilitar o DynamoDB Streams no provisionamento da tabela informando o valor booleano no parâmetro stream_enabled e informando o stream_view_type como NEW_AND_OLD_IMAGES para que por meio do stream a replicação aconteça em todas as replicas. 123456789101112131415161718192021222324252627282930313233343536373839resource  aws_dynamodb_table   sales  { provider = aws. primary hash_key =  id  name       =  sales  stream_enabled  = true stream_view_type =  NEW_AND_OLD_IMAGES  read_capacity = lookup(var. dynamodb_sales,  read_min ) write_capacity = lookup(var. dynamodb_sales,  write_min ) billing_mode  = lookup(var. dynamodb_sales,  billing_mode ) point_in_time_recovery {  enabled = lookup(var. dynamodb_sales,  point_in_time_recovery ) } attribute {  name =  id   type =  S  } server_side_encryption {  enabled   = true  kms_key_arn = module. cluster_sa_east_1. kms_key } lifecycle {  ignore_changes = [   read_capacity,   write_capacity,   replica  ] }}// Omitindo configs de autoscalingEm seguida precisaremos criar um outro recurso chamado aws_dynamodb_table_replica informando qual tabela será replicada e em qual região. Nesse caso precisaremos informar o provider do terraform da zona de&nbsp;DR. 12345678910111213resource  aws_dynamodb_table_replica   sales  { provider     = aws. disaster-recovery global_table_arn = aws_dynamodb_table. sales. arn kms_key_arn   = module. cluster_us_east_1. kms_key depends_on = [  aws_appautoscaling_target. sales_read,  aws_appautoscaling_target. sales_write,  aws_appautoscaling_policy. sales_read,  aws_appautoscaling_policy. sales_write ]}Para testar, vamos consumir nossa API do produto hipotético tentando criar uma venda, em seguida executar um Scan na tabela nas duas regiões para garantir que ele está&nbsp;lá. 123456789101112❯ curl -X POST https://api. msfidelis. com. br/sales -d '{ product : registro que viajou entre duas regiões ,  amount : 666. 00}' -iHTTP/2 201date: Wed, 12 Jul 2023 00:22:26 GMTcontent-type: application/json; charset=utf-8content-length: 128x-amzn-requestid: bb696387-4a2c-46b4-b68c-82adbefc9fd1x-amzn-remapped-content-length: 128x-amzn-remapped-connection: keep-alivex-amz-apigw-id: H7LKVG88mjQEO6Q=x-amzn-remapped-date: Wed, 12 Jul 2023 00:22:26 GMT{ id : d2673f99-d632-453d-aedd-13a30fc3bc78 , product : registro que viajou entre duas regiões , amount :666, processed :false} 3. 3 — S3 Two Way Replication: Nosso produto hipotético, após nosso worker consumir a mensagem no SQS de uma nova venda, ele faz uma série de processamento também hipotético, atualiza a flag de processamento no DynamoDB e em seguida sobe o registro para o S3 para Backup e ingestão futura de um Data Lake hipotético do Athena. (Não iremos abordar o Amazon Athena por enquanto) O Amazon S3 (Simple Storage Service) é um serviço popular de armazenamento de objetos oferecido pela AWS. Ele fornece uma solução escalável, segura e durável para armazenar e recuperar dados de forma eficiente. Uma das funcionalidades avançadas do Amazon S3 é o S3 Two Way Replication, que oferece uma abordagem bidirecional para replicar dados entre buckets do S3 em regiões diferentes. A Replicação Bidirecional do S3 Two Way permite a replicação contínua e síncrona dos objetos armazenados em um bucket do S3 para um bucket correspondente em outra região. Isso significa que qualquer alteração, inclusão ou exclusão feita em um objeto em um bucket será automaticamente replicada para o bucket de destino em tempo real. Com essa abordagem, você pode manter cópias atualizadas dos seus dados em regiões distintas, aumentando a disponibilidade e a durabilidade dos objetos armazenados no S3. Para o nosso objetivo de DR e replicação de dados, essa solução serve muito bem. Podemos escrever e ler dos dois buckets da replicação e contar que ambas as modificações serão espelhadas na região vizinha. Nesse caso, o provisionamento é bem simples. Vamos também utilizar um módulo onde iremos provisionar dois buckets com as mesmas configurações nas duas regiões, primária e disaster-recovery. 1234567891011121314151617181920module  bucket_sa_east_1  { source =  . /modules/s3_bucket  providers = {  aws = aws. primary } bucket_name_prefix =  processed-sale }module  bucket_us_east_1  { source =  . /modules/s3_bucket  providers = {  aws = aws. disaster-recovery } bucket_name_prefix =  processed-sale }Devemos prepara uma IAM Role que permita ser assumida pelo S3 com as devidas permissões de replicação e manipulação de&nbsp;objetos. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// Replication IAMdata  aws_iam_policy_document   assume_role  { statement {  effect =  Allow   principals {   type    =  Service    identifiers = [ s3. amazonaws. com ]  }  actions = [ sts:AssumeRole ] }}resource  aws_iam_role   replication  { provider      = aws. primary name        = format( sales-s3-replication ) assume_role_policy = data. aws_iam_policy_document. assume_role. json}data  aws_iam_policy_document   replication  { statement {  effect =  Allow   actions = [    s3:GetReplicationConfiguration ,    s3:ListBucket ,  ]  resources = [   module. bucket_sa_east_1. arn,   module. bucket_us_east_1. arn,  ] } statement {  effect =  Allow   actions = [    s3:GetObjectVersionForReplication ,    s3:GetObjectVersionAcl ,    s3:GetObjectVersionTagging ,    s3:ReplicateObject ,    s3:ReplicateDelete ,    s3:ReplicateTags ,  ]  resources = [    ${module. bucket_sa_east_1. arn}/* ,    ${module. bucket_us_east_1. arn}/*   ] }}resource  aws_iam_policy   replication  { provider = aws. primary name   = format( sales-s3-replication ) policy  = data. aws_iam_policy_document. replication. json}resource  aws_iam_role_policy_attachment   replication  { provider  = aws. primary role    = aws_iam_role. replication. name policy_arn = aws_iam_policy. replication. arn}Podemos duplicar o recurso aws_s3_bucket_replication_configuration alternando em cada um o bucket origem e&nbsp;destino. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354resource  aws_s3_bucket_replication_configuration   primary  { provider = aws. primary role  = aws_iam_role. replication. arn bucket = module. bucket_sa_east_1. id rule {  id =  sales   filter {   prefix =  sales   }  delete_marker_replication {   status =  Enabled   }  status =  Enabled   destination {   bucket    = module. bucket_us_east_1. arn   storage_class =  STANDARD   } }}resource  aws_s3_bucket_replication_configuration   disaster_recovery  { provider = aws. disaster-recovery role  = aws_iam_role. replication. arn bucket = module. bucket_us_east_1. id rule {  id =  sales   filter {   prefix =  sales   }  delete_marker_replication {   status =  Enabled   }  status =  Enabled   destination {   bucket    = module. bucket_sa_east_1. arn   storage_class =  STANDARD   } }}Vamos consumir novamente a aplicação inserindo um novo hipotético registro de venda de um&nbsp;produto. 123456789101112❯ curl -X POST https://api. msfidelis. com. br/sales -d '{ product : registro que viajou entre duas regiões e caiu em dois buckets ,  amount : 333. 00}' -iHTTP/2 201date: Wed, 12 Jul 2023 00:44:43 GMTcontent-type: application/json; charset=utf-8content-length: 151x-amzn-requestid: 7194a8c3-e4f1-48f2-baea-77c347174130x-amzn-remapped-content-length: 151x-amzn-remapped-connection: keep-alivex-amz-apigw-id: H7ObVER4mjQEH_w=x-amzn-remapped-date: Wed, 12 Jul 2023 00:44:43 GMT{ id : db218024-3974-4ad7-9490-2c88560298de , product : registro que viajou entre duas regiões e caiu em dois buckets , amount :333, processed :false}Agora vamos conferir no bucket se existe o registro db218024-3974-4ad7-9490-2c88560298de. json em ambas as regiões.   Parte 4: Escrevendo Código Multi Region e Sugestões ArquiteturaisAchou mesmo que é só subir infra que resolve? Achou errado, amigo. Trabalhar em Multiregião é uma escolha, um projeto, um objetivo, não um hotfix, então devemos trabalhar nossas aplicações para que as mesmas sejam resilientes não só a falhas mas também para trabalhar de forma inteligente esse chaveamento de regiões. Temos diversos padrões que podem nos ajudar, nesse artigo trabalharemos os seguintes:  Feature Toggle com Parameter Store Dry-Run Idempotência 4. 1 — Feature Toggle com Parameter Store: Um Feature Toggle, também conhecido como Feature Flag dependendo pra quem você pergunta, é um mecanismo de controle que permite habilitar ou desabilitar recursos ou funcionalidades específicas em um software, sem a necessidade de fazer uma nova implantação ou alteração na codebase. É uma técnica comumente utilizada no desenvolvimento de software para ativar ou desativar recursos de forma flexível e controlada, possibilitando a entrega contínua e incremental de novas funcionalidades. Com um Feature Toggle, você pode ocultar uma funcionalidade em produção enquanto ainda está em desenvolvimento, permitindo que você teste, experimente e valide essa funcionalidade em um ambiente de produção controlado. No nosso caso, iremos utilizá-lo para desabilitar ou habilitar o processamento inteiro de uma região e das aplicações que compõe o nosso Workload de vendas em tempo de DR. O Parameter Store é um serviço gerenciado pela AWS que oferece armazenamento seguro e gerenciamento de parâmetros e configurações. Ele permite armazenar e recuperar informações sensíveis, como senhas, chaves, strings de conexão e outros valores de configuração, de forma centralizada e segura. Uma forma inteligente de trabalhar com Feature Toggle no Parameter Store é fazer com que pragmaticamente, a aplicação sempre consulte o valor do parâmetro de X em X tempo e salve em memória por um determinado período de tempo. Sempre que esse registro expirar, a aplicação consiga consultar e atualizar o parâmetro de forma global para o runtime. Por exemplo, durante uma determinada interação eu consulto o valor do parameter store que contém o valor do estado do meu Site/Região, e digo para salvar essa informação em cache em memória por 30 segundos. 12ssm_site_state_parameter := os. Getenv( SSM_PARAMETER_STORE_STATE )site_state, err := parameter_store. GetParamValue(ssm_site_state_parameter, 30) 12345678910111213141516package memory_cacheimport (	 time 	 github. com/patrickmn/go-cache )var instance *cache. Cachefunc GetInstance() *cache. Cache {	if instance == nil {		instance = cache. New(5*time. Minute, 10*time. Minute)	}	return instance}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package parameter_storeimport (	 fmt 	 os 	 time 	 sales-worker/pkg/log 	 sales-worker/pkg/memory_cache 	 github. com/aws/aws-sdk-go/aws 	 github. com/aws/aws-sdk-go/aws/session 	 github. com/aws/aws-sdk-go/service/ssm )func GetParamValue(parameter string, cacheTime int64) (string, error) {	m := memory_cache. GetInstance()	log := log. Instance()	if cacheTime &gt; 0 {		if value, found := m. Get(parameter); found {			log. Info( Parameter found in cache )			return fmt. Sprint(value), nil		}	}	sess, err := session. NewSession(&amp;aws. Config{		Region: aws. String(os. Getenv( AWS_REGION )),	})	if err != nil {		return   , err	}	svc := ssm. New(sess)	result, err := svc. GetParameter(&amp;ssm. GetParameterInput{		Name:      aws. String(parameter),		WithDecryption: aws. Bool(false),	})	if err != nil {		return   , err	}	if cacheTime &gt; 0 {		m. Set(parameter, *result. Parameter. Value, time. Second*time. Duration(cacheTime))	}	return fmt. Sprint(*result. Parameter. Value), nil}E voltando para a parte de infraestrutura que anda junto com esse tipo de lógica, no nosso workload hipotético vamos salvar o valor da variável state do Terraform no Parameter Store, e com base nessa informação vamos desenvolver nossos fluxos, com base que se essa variável for trocada, no deploy em sequencia todas as aplicações sejam informadas quase que automaticamente de como elas devem se comportar. 1234567variable  state  { type = map(any) default = {   sa-east-1  :  ACTIVE ,   us-east-1  :  PASSIVE , }}Com base nisso vamos duplicar também o parâmeter store nas duas regiões utilizando o mesmo&nbsp;nome. 123456789101112131415161718192021module  ssm_parameter_state_sa_east_1  { source =  . /modules/parameter_store  providers = {  aws = aws. primary } name = format( /%s/site/state , var. project_name) value = lookup(var. state,  sa-east-1 )}module  ssm_parameter_state_us_east_1  { source =  . /modules/parameter_store  providers = {  aws = aws. disaster-recovery } name = format( /%s/site/state , var. project_name) value = lookup(var. state,  us-east-1 )}Podemos conferir via painel o resultado das duas operações, tanto na Região Primária (sa-east-1) quanto na de Disaster Recovery (us-east-1).  Trabalhando dessa forma, podemos recuperar nosso estado no início de um fluxo e trabalhá-lo até o final do mesmo, fazendo sempre uma validação do estado para saber se precisamos trabalhar o registro ou não. Detalharemos isso no próximo passo. 1234 if state !=  ACTIVE  { return nil } continueOqueEstavaFazendo() 4. 2 — Dry-Run, processamento café com leite: Dry-Run, também conhecido como teste simulado ou simulação de execução, é uma prática comum no desenvolvimento de software e testes. Consiste em executar um programa, algoritmo ou conjunto de instruções em um ambiente simulado, sem a efetiva execução das ações ou operações previstas. Durante um Dry-Run, o código é analisado e executado passo a passo, e os resultados são simulados sem realizar alterações no estado real do sistema ou afetar os dados existentes. O objetivo principal é identificar erros, validar a lógica do programa e verificar a saída esperada, tudo isso sem impactar o ambiente de produção ou introduzir mudanças irreversíveis. Para essa arquitetura, vamos sair da definição da literatura e fazer com que o princípio do Dry-Run simplesmente não cometa nenhuma alteração, mesmo que consuma as mensagens e instruções com base no estado do feature toggle. No nosso Workload hipotético, após a API Rest salvar a venda no DynamoDB, ela publica uma mensagem no tópico do SNS e o mesmo envia para o SQS em ambas as regiões. Essa mensagem deveria ser consumida pelos workers de ambas as regiões, e esses workers devem fazer um processamento falso, atualizar a flag de processado na tabela do DynamoDB e salvar o item no S3 conforme já explicado. Por mais que a mensagem seja replicada em ambas as regiões, é um desperdício de dinheiro e poder computacional literalmente processar a mensagem duas vezes. Para isso, vamos utilizar o Feature Toggle do estado da Região para dizer para o nosso worker se ele deve processar a mensagem de fato ou executar um Dry-Run na mesma, ou simplesmente descartá-la e esperar que o processamento seja realizado e replicado para ela pela região ativa. Um exemplo de um processamento de Feature Flag e Dry-Run Basicamente, de tempos em tempos atualizamos o estado do nosso runtime com o definido no Parameter Store e com base nesse estado, checamos se o Site/Região está ativo ou passivo. Caso esteja com o estado ACTIVE, executa o processamento em N ações descritas anteriormente, e caso esteja PASSIVE, executa o Dry-Run, remove a mensagem da fila em seguida. Exemplo do comportamento do Worker quando a região está ativa Exemplo do comportamento do Worker quando a região está passiva  4. 3 — Idempotência: Talvez o padrão arquitetônico mais importante quando falamos de qualquer coisa que se propõe a ser resiliente e distribuída. Na arquitetura de software, a idempotência é um conceito importante que descreve a propriedade de uma operação ou função que pode ser aplicada repetidamente sem alterar o resultado além da primeira aplicação. Em outras palavras, uma operação idempotente produz o mesmo resultado, independentemente do número de vezes que é executada. A idempotência é uma característica desejável em sistemas distribuídos e transações de software, pois garante que a reexecução de uma operação não resulte em efeitos colaterais indesejados ou em um estado inconsistente do sistema. Isso é particularmente importante quando ocorrem falhas de rede, erros transientes ou reexecuções automáticas devido a mecanismos de recuperação de erros. Existem várias formas de estender a capacidade de um padrão de idempotência. Alguns exemplos fora desse assunto de Disaster Recovery que são naturalmente idempotentes:  Operações de Atualização de Estado: Em APIs REST, as operações de atualização de estado geralmente seguem o princípio da idempotência. Por exemplo, se uma requisição DELETE, PUT ou PATCH for repetida várias vezes, o estado final do recurso será o mesmo.  Operações de Pagamento na Indústria Financeira: Em sistemas de pagamentos online, as transações de pagamento são geralmente idempotentes. Isso significa que, se uma transação de pagamento for repetida devido a um problema de comunicação ou a uma resposta de confirmação perdida, ela não resultará em cobranças duplicadas ao usuário. Porém, nesse exemplo, iremos implementar uma idempotência pragmática com base no ID único gerado ao longo de cada venda criada através da nossa API. Nesse caso, iremos controlar a idempotência garantindo um certo tempo de replicação dos recursos em uma tabela do DynamoDB exclusivamente preparada para isso. O objetivo desse processo de idempotência no nosso workload hipotético é evitar o reprocessamento de uma venda criada e disponibilizada para consumo durante uma virada ou uma possível duplicação de dados, ou tentativa de reprocessamento por algum outro mecanismo não tratado aqui. Exemplo do fluxo de Idempotência de Processamento Vamos estender o diagrama que vimos no Dry-Run. Nesse caso, após validarmos que estamos numa região ativa, chegamos na tabela do DynamoDB de idempotência procurando pelo ID da venda. Caso ela já tenha sido processada, descartamos a mensagem. Caso o ID ainda não esteja presente na tabela, realizamos todos os fluxos de processamento que tratamos aqui e no final salvamos o mesmo na tabela de idempotência, também rodando como Global Table.  Por exemplo: 12345678910111213141516 idempotency, err := dao. CheckIdempotency(sale. ID) if err != nil { return err } if idempotency { log. Info().  Str( Region , aws_region).  Str( State , state).  Int( Thread , thread).  Str( Sale , sale. ID).  Msg( Sale already processed, item found in idempotency table ) return nil }// Continua Parte 5: Testando o Fluxo de Disaster RecoveryAgora chegou a parte legal. Fizemos muitos desenhos bonitinhos, módulos lindos, explicamos um monte de paradigmas arquiteturais filosóficos, e agora precisamos validar de fato o funcionamento desse exemplo. Vamos injetar carga utilizando o k6, uma ferramenta já conhecida de outros carnavais aqui no Medium. Injetaremos carga e monitoraremos o comportamento dos recursos das duas regiões via CloudWatch.  A ideia é fazer todo o espelhamento e chaveamento via Terraform. Então vamos estruturar essa parte com base em passos de formiga, exemplificando ação e reação do workload. 5. 1 — Status Inicial:  Inicialmente, temos o workload atendendo e performando de forma prevista, somente na zona primária. Temos um contador de eventos de vendas e de requests por minuto no API Gateway para ambas as regiões. Em paralelo, temos o registro de replicação das nossas tabelas do DynamoDB. Como estamos escrevendo e lendo da tabela na região primária (sa-east-1), devemos ver o fluxo de replicação indo para a região de DR (us-east-1). Resumindo, nosso estado de configuração do chaveamento no Terraform está da seguinte forma: chaveando 100% do tráfego para São Paulo e mantendo nosso feature toggle em ACTIVE para o mesmo. 1234567variable  state  { type = map(any) default = {   sa-east-1  :  ACTIVE ,   us-east-1  :  PASSIVE , }} 5. 2 — Chaveando o&nbsp;DR: Vamos alterar o estado no Terraform para desligar a zona de São Paulo e direcionar o tráfego diretamente para a região de Virginia. 1234567variable  state  { type = map(any) default = {   sa-east-1  :  PASSIVE ,   us-east-1  :  ACTIVE , }}Independente de onde e como você está executando isso, no fim essa mudança irá refletir com base em um&nbsp;apply 1terraform apply --auto-approve Após aplicar as alterações com o Terraform, chaveando a Região primária para o DR sem interromper o teste de carga, podemos ver o início imediato da operação da região de DR, que começa a contabilizar eventos de vendas por ela iniciados, entrada de fluxo no API Gateway e a inversão do sentido de replicação das tabelas do DynamoDB.  Nesse cenário, começamos a utilizar os recursos da zona de DR como ACTIVE, enviando todo o tráfego da API para lá e desabilitando a zona primária em São Paulo, colocando-a como PASSIVE. Acompanhamos também uma métrica de replicação das mensagens. Esse pode ser um bom indicador para o chaveamento de retorno para a Região Primária.  Parte 6: Estratégias AdicionaisComo mencionado no início do artigo, algumas soluções e produtos da AWS projetados para Disaster Recovery foram desconsiderados durante a concepção deste texto. No entanto, vale a pena mencionar algumas das opções disponíveis.  6. 1 — Route53 Failover: O Route53 Failover é um recurso que esteve perto de ser incluído, se não fosse pela parametrização de feature flag. Usando o Route53 Failover, é possível redirecionar o tráfego de maneira inteligente entre várias regiões, assegurando que as solicitações dos usuários sejam redirecionadas para uma região de backup caso a primária fique indisponível. Isso é extremamente valioso em cenários de disaster recovery, onde manter a continuidade dos negócios é crítico. O funcionamento do Route53 Failover é baseado em health checks configurados para monitorar a saúde das instâncias ou endpoints em cada região. Se um recurso falhar, o Route53 pode automaticamente redirecionar o tráfego para outra região que esteja operacional. Em caso de falha na região primária, o tráfego é redirecionado automaticamente para uma região secundária. A implementação desta estratégia é ideal para operações que possam funcionar de forma ATIVO/ATIVO, onde uma região pode gerenciar as solicitações de outra a qualquer momento.  Exemplo da implementação em Terraform dos Healthchecks do Route53. 1234567891011121314151617resource  aws_route53_health_check   primary  { fqdn       = var. api_gateway_dns_primary  port       = 443 type       =  HTTP   resource_path  =  /healthcheck   request_interval = 30  failure_threshold = 3 }resource  aws_route53_health_check   secondary  { fqdn       = var. api_gateway_dns_secondary port       = 443 type       =  HTTP  resource_path  =  /healthcheck  request_interval = 30 failure_threshold = 3}12345678910111213141516171819202122232425262728293031resource  aws_route53_record   primary  { zone_id = var. route53_hosted_zone name  = var. api_gateway_domain type  =  A  ttl   = 60  failover_routing_policy {  type     =  PRIMARY   # Identificando que e o Registro Primário  ttl_override = 60  } set_identifier =  primary  health_check_id = aws_route53_health_check. primary. id}resource  aws_route53_record   secondary  { zone_id = var. route53_hosted_zone name  = var. api_gateway_domain type  =  A  ttl   = 60 failover_routing_policy {  type     =  SECONDARY  # Identificando que e o Registro Secundário  ttl_override = 60 } set_identifier =  secondary  health_check_id = aws_route53_health_check. secondary. id}               Configurar failover de DNS - Amazon Route&nbsp;53    :     Quando houver mais de um recurso executando a mesma função, (por exemplo, mais de um servidor HTTP ou servidor de e-mail), você poderá configurar o Amazon Route 53 para verificar a integridade dos seus recursos e responder às consultas de DNS usando somente os recursos íntegros. Por exemplo, suponhamos que seu site, example. com, esteja hospedado em seis servidores distribuídos por três datacenters ao redor do mundo (dois servidores em cada). Você pode configurar o Route 53 para verificar a integridade desses servidores e responder às consultas de DNS de example. com usando somente os servidores atualmente considerados como íntegros.           docs. aws. amazon. com    6. 2 — Elasticache Multi Region com Global Datastores: O Global Datastore é um recurso do Amazon ElastiCache, um serviço gerenciado de cache na nuvem da Amazon Web Services (AWS). Ele permite a replicação automática e síncrona de dados entre regiões geográficas distintas, proporcionando uma solução de cache altamente disponível e resiliente. Configurar um Global Datastore significa criar um ambiente de cache que é distribuído em várias regiões da AWS. Isso beneficia aplicações ou serviços que operam em escala global, exigindo acesso rápido a dados em cache, não importando onde os usuários estão localizados. Com o Global Datastore, o ElastiCache cuida da replicação dos dados em tempo real entre as regiões de forma automática, assegurando que os dados estejam sempre atualizados e acessíveis em todos os locais. Este recurso não foi incluído no artigo pois os replication groups não suportam escrita bilateral nos clusters de replicação de outras regiões, apenas permitindo promoção automática em caso de failover provocado por uma falha no primário.  Exemplo de Implementação em Terraform 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647resource  aws_elasticache_global_replication_group   main  { providers = {  aws = aws. primary } global_replication_group_id_suffix = var. project_name primary_replication_group_id    = aws_elasticache_replication_group. primary. id engine_version =  6. 2 }resource  aws_elasticache_replication_group   primary  { providers = {  aws = aws. primary } replication_group_id =  ${var. project_name}-primary  description     =  primary replication group  engine     =  redis  engine_version =  6. 0  node_type   =  cache. m5. large  num_cache_clusters = 1 lifecycle {  ignore_changes = [engine_version] }}resource  aws_elasticache_replication_group   secondary  { providers = {  aws = aws. disaster-recovery } replication_group_id    =  ${var. project_name}-secondary  description         =  secondary replication group  global_replication_group_id = aws_elasticache_global_replication_group. example. global_replication_group_id num_cache_clusters = 1 lifecycle {  ignore_changes = [engine_version] }}               Replication across AWS Regions using global datastores - Amazon ElastiCache for Redis    :     Replicate data across AWS Regions using the Global Datastore feature in Amazon ElastiCache for Redis.           docs. aws. amazon. com    6. 3 — RDS Cross Region Read Replicas: O Cross-Region Read Replicas do Amazon RDS (Relational Database Service) é um recurso que permite replicar dados de um banco de dados em uma região para outras regiões. Isso melhora a disponibilidade, desempenho e resiliência de bancos de dados globais. Ao usar Cross-Region Read Replicas, você pode criar cópias de leitura do seu banco de dados em regiões distantes, que são úteis para consultas intensivas, carga distribuída e redução de latência. O recurso é limitado a operações de leitura, não permitindo escritas bilaterais.                Cross-Region read replicas - Amazon Relational Database Service    :     View the AWS Region and Amazon RDS DB engine version support for the cross-region read replicas feature.           docs. aws. amazon. com    6. 4 — Aurora Global Database: O Aurora Global Database é um recurso do Amazon Aurora, compatível com MySQL e PostgreSQL. Permite a criação de um ambiente de banco de dados com replicação automática entre múltiplas regiões. Você pode criar um cluster primário e replicar dados para até cinco regiões secundárias. As secundárias são leitura ou requerem intervenção manual para outros usos, o que também limitou sua inclusão neste artigo.                 Using Amazon Aurora global databases - Amazon Aurora    :     Learn how to create and work with Amazon Aurora global databases. An Aurora global database is made up of multiple Aurora DB clusters in different AWS Regions. The result is low-latency global reads and disaster recovery if an AWS Region experiences an outage.           docs. aws. amazon. com    6. 5 — Secrets Manager Multi-Region Replication: A replicação Multi-Region do Secrets Manager permite replicar secrets para regiões adicionais para DR, conformidade ou baixa latência. A replicação é automática: atualizações em uma região primária são replicadas para todas as secundárias. Não foi considerado para o artigo por não ter uso em contextos não relacionados à gestão de segredos e por ainda não estar disponível para Terraform.                                   How to replicate secrets in AWS Secrets Manager to multiple Regions | Amazon Web Services    :     On March 3, 2021, we launched a new feature for AWS Secrets Manager that makes it possible for you to replicate secrets across multiple AWS Regions. You can give your multi-Region applications access to replicated secrets in the required Regions and rely on Secrets Manager to keep the replicas in sync with the primary secret. […]          aws. amazon. com    6. 6 — Mirror Maker e MSK: O Mirror Maker é uma ferramenta do Amazon Managed Streaming for Apache Kafka (MSK), que permite replicar tópicos e partições de um cluster para outro. O Mirror Maker replica dados do Kafka entre regiões, suportando DR, leitura de baixa latência e distribuição de carga. Para replicar usando o Mirror Maker, é necessário configurar um link entre VPCs de diferentes regiões.                Migrating to an Amazon MSK Cluster - Amazon Managed Streaming for Apache Kafka    :     Use Apache Kafka's MirrorMaker to migrate your cluster.           docs. aws. amazon. com                  Setting up Mirror Maker    :     Tutorial to set up Kafka Mirror Maker. A feature in Kafka allowing maintaining a replica of a Kafka cluster in a separate data centre.           www. instaclustr. com    ConclusãoMinha conclusão é que esse artigo foi muito extenso e cansativo. E a porcentagem de pessoas que vão chegar até esse ponto deve ser muito baixa. Se você chegou até aqui, saiba que eu estou muito feliz, e por favor, me deixe saber disso. Esse foi de longe o artigo mais extenso e cansativo que eu já escrevi nos últimos anos, e espero de coração que ajude na firmação de conceitos e a pensar em estratégias paupáveis para o seu contexto depois de ver os exemplos daqui. E um agradecimento de coração a todos os revisores que dedicaram seu tempo pra avaliar o artigo.  Repositórios do Artigo:  GitHub - msfidelis/aws-multi-region-disaster-recovery: Example to explain how to implement minimal multi-region architecture on AWS with disaster recovery GitHub - msfidelis/aws-multi-region-disaster-recovery-apps: Apps to aws-multi-region-disaster-recovery example Links e Referências:  Terraform — Global Replication Groups (https://registry. terraform. io/providers/hashicorp/aws/latest/docs/resources/elasticache_global_replication_group) Terraform – Lookup (https://developer. hashicorp. com/terraform/language/functions/lookup) Terraform – Maps(https://developer. hashicorp. com/terraform/language/expressions/types#maps-objects) AWS Disaster Recovery Workshop (https://disaster-recovery. workshop. aws/en/) Creating Disaster Recovery Mechanisms Using Amazon Route 53 (https://aws. amazon. com/blogs/networking-and-content-delivery/creating-disaster-recovery-mechanisms-using-amazon-route-53/) Resilience in Amazon Route53 (https://docs. aws. amazon. com/Route53/latest/DeveloperGuide/disaster-recovery-resiliency. html) Amazon DynamoDB Global Tables (https://aws. amazon. com/dynamodb/global-tables/) AWS Disaster Recovery Strategies (https://xebia. com/blog/aws-disaster-recovery-strategies-poc-with-terraform/) How To Build a Custom Disaster Recovery Process for AWS Applications (https://www. encora. com/insights/how-to-build-a-custom-disaster-recovery-process-for-aws-applications) Google Disaster recovery planning guide (https://cloud. google. com/architecture/dr-scenarios-planning-guide) Disaster Recovery for Multi-Region Kafka at Uber (https://www. uber. com/en-KW/blog/kafka/) Obrigado aos Revisores:  Rafael - (@ raffasarts) Caio Delgado — (@ caiodelgadonew) Bernardo — (@ indiepagodeiro) Kaleb — (@ kalves_rohan) Luis Garavatti — (@ lhgaravatti) Luiz Aoqui – (@ luiz_aoqui)Me sigam no Twitter para acompanhar as paradinhas que eu compartilho por lá! Te ajudei de alguma forma? Me pague um café (Mentira, todos os valores doados nessa chave são dobrados por mim e destinados a ongs de apoio e resgate animal Chave Pix: fe60fe92-ecba-4165-be5a-3dccf8a06bfc "
    }, {
    "id": 6,
    "url": "http://localhost:4000/argo-rollouts-qual-a-forma-mais-simples-de-executar-canary-releases-e-blue-green-deployments-no/",
    "title": "Argo-Rollouts — "Qual a forma mais simples de executar Canary Releases e Blue/Green Deployments no Kubernetes?"",
    "body": "2023/04/02 - O Deploy em ambientes Cloud Native pode ser, se não é, a parte mais desafiadora no dia a dia do ciclo de vida de um software, principalmente se a atualização é realizada em aplicações criticas que possuem volumes consideráveis de transações. Nesse contexto, possuímos uma vasta gama de ferramental para gerenciar deployments em ambientes de Kubernetes, algumas melhores, outras mais modestas e poucas simples e sucintas. Existe um ponto de atenção muito importante quando falamos de realizar um deploy em produção: O rollback. Uma frase simples, mas que mudou minha forma de pensar sobre qualquer coisa que eu colocaria a mão quando trato desse tema veio do meu mestre Fernando Ike. “Mais importante que entregar rápido, é voltar rápido”  — Algo assim. Nesses dias um colega de trabalho me fez uma pergunta que me deixou pensativo das ideia, e que pra responder de forma decente, prometi escrever esse artigo:  “Qual a forma mais simples de executar um Canary ou um Blue Green no Kubernetes?” Executar Canary e Blue / Green eu conheço algumas formas de fazer. Provavelmente você também. Experiência com ferramentas, truques no kubectl e tudo mais não falta por ai. Mas beleza, mas qual das várias possibilidades é a mais simples? O Argo Rollouts: O Argo Rollouts é uma ferramenta de automação de implantação em clusters Kubernetes que fornece recursos avançados de controle de versão. Ele é projetado para ajudar os desenvolvedores a gerenciar o ciclo de vida de seus aplicativos de maneira mais eficiente e confiável, permitindo que eles implantem novas versões de aplicativos com mais segurança e menos tempo de inatividade. E antes de tudo, de forma simples. O Argo Rollouts é uma extensão do Argo CD, outra ferramenta popular de automação de implantação de Kubernetes. Ele é construído em cima do controlador de implantação nativo do Kubernetes e é compatível com várias plataformas em nuvem, incluindo AWS, Google Cloud e Microsoft Azure. O Argo CD é talvez a ferramenta favorita da maioria das pessoas, eu tento não ser fã de tooling, mas provavelmente é a minha também. Porém, não é simples. As vezes precisamos dar um simples upgrade do que já temos nativamente pra resolver a maioria dos problemas. O Argo-Rollouts talvez tenha sido uma resposta a isso.  Premissas: A proposta desse post é mostrar o funcionamento básico do Argo-Rollouts resolvendo problemas reais, de forma que consiga ser adaptado ao maior numero de contextos possíveis. Com excessão do argo-rollouts, não vou focar em nenhuma outra ferramenta que por ventura possa aparecer nesse post. Todas as possíveis soluções para os problemas apresentados serão apresentados de duas formas:  Os exemplos desse artigo serão apresentados em forma de perguntas ou requisitos hipotéticos, seguido da implementação que resolveria a questão. Gosto desse tipo de abordagem.  Utilizarei o modo mais “manual” possível para que as logicas possam ser reaproveitados de forma genérica em qualquer tipo de orquestrador de pipelines que eventualmente faça entregas de software num cluster Kubernetes.  No final apresentando um componente adicional, da dashboard do argo-rollouts, que pode ser um grande aliado na hora de separar os processos de CI/CD e deixar a progressão dos deploys mais customizáveis e cautelosos. Instalação do Kubectl Plugin: A arquitetura de manipulação do Argo Rollouts funciona primeiramente como um client / server. Basicamente, caso você não escreva passos de deployment que se viram sozinhos, baseados em tempo e etc, você precisará utilizar o plugin do kubectl para o rollouts para promover, abortar ou dar rollback de versões. Então entende-se que esse plugin precisa estar instalado no seu orquestrador de pipelines caso necessite gerenciar o ciclo dessa forma. A instalação, siga o Installation Guide oficial do Argo Rollouts.                                   GitHub - ysk24ok/jekyll-linkpreview: Jekyll plugin to generate link preview    :     Jekyll plugin to generate link preview. Contribute to ysk24ok/jekyll-linkpreview development by creating an account on GitHub.           github. com    Instalação do Argo-Rollouts: A instalação da ferramenta possui algumas alternativas, mas nesse post iremos abordar o método via Helm. Para as demais, você pode acessar o manual de instalação.  Instalação via Helm: A instalação via helm é bem simples e sem segredo. 12helm repo add argo https://argoproj. github. io/argo-helmhelm install argo-rollouts argo/argo-rolloutsPara a instalação da Dashboard, será necessário adicionar o parâmetro --set dashboard. enabled=true 1helm install argo-rollouts argo/argo-rollouts --set dashboard. enabled=true Instalação via Terraform: Uma opção interessante também é utilizar o provider do helm para o terraform. Como já abordamos esse processo em alguns outros artigos, vou deixar o exemplo aqui também. Certeza que vai ser útil em algum momento pra alguém. 123456789101112resource  helm_release   argo_rollouts  {  name        =  argo-rollouts   chart        =  argo-rollouts   repository     =  https://argoproj. github. io/argo-helm   namespace      =  argo-rollouts   create_namespace  = true  set {      name =  dashboard. enabled       value =  true     }} A Estrutura de um Rollout: Uma coisa que precisa ficar evidente, na lata, quando começamos a utilizar o Argo-Rollouts, é que vamos parar de utilizar os manifestos de Deployment. Isso é a premissa inicial da ferramenta, mas calma. É muito simples. No lugar do que escreveríamos nossos pod templates e replicas, no Deployment vamos começar a utilizar o objeto Rollout, disponível a partir dos CRD’s do Argo Rollouts no lugar. De forma genérica, migraríamos um Deployment convencional que já estamos acostumados disso: 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector:  matchLabels:   app: nginx replicas: 10 template:  metadata:   labels:    app: nginx  spec:   containers:   - name: nginx    image: nginx:1. 14. 2    ports:    - name: http     containerPort: 80     protocol: TCPPara algo parecido com o item abaixo, onde temos a mesma estrutura de template/spec para os pods, porém adicionamos o campo strategy onde vamos descrever como será executado os rollouts de versão. Não se atente a esse modelo agora, vamos evoluir bastante o case a seguir: 12345678910111213141516171819202122232425262728293031apiVersion: argoproj. io/v1alpha1kind: Rolloutmetadata: name: nginx-deploymentspec: replicas: 10 revisionHistoryLimit: 2 selector:  matchLabels:   app: nginx template:  metadata:   labels:    app: nginx  spec:   containers:   - name: nginx    image: nginx:1. 14. 2    ports:    - name: http     containerPort: 80     protocol: TCP strategy:  canary:   steps:   - setWeight: 20   - pause: {duration: 30}   - setWeight: 50   - pause: {duration: 60}   - setWeight: 80   - pause: {} Canary Release: O Canary Deployment é um padrão de deploy onde uma nova versão de uma aplicação é implantada em uma quantidade determinada e progressiva de instâncias para fins de teste e validação a quente antes de ser totalmente implantada em todo o ambiente de produção. Esse processo também é conhecido sem estrangeirismo como Canário. Durante o período de teste, o tráfego é direcionado para o canário, permitindo que a nova versão do aplicativo seja validada em um ambiente real, mas com um menor risco de interrupção para o restante dos usuários,** recebendo uma quantidade pequena, porém progressiva, do tráfego** até que seja promovida a versão estável. O ideal, é que se houver algum problema durante os testes, a implantação do canário pode ser revertida com facilidade, minimizando o impacto para o restante dos usuários. Os Canary Deployments são amplamente utilizados em ambientes de produção, especialmente em implantações críticas em que os erros podem ter consequências graves, ou onde queremos validar um de-para a quente de alguma feature, modificação, migração. Essa técnica ajuda a garantir a estabilidade e a qualidade do aplicativo, ao mesmo tempo em que permite que as equipes de desenvolvimento e operações realizem testes e validações com segurança junto ao tráfego real.  Tentei elaborar mentalmente alguns requisitos de plataforma nos quais o Canary Release proposto pelo Argo Rollouts resolveria.  “Eu preciso que meu canário progrida automaticamente 20% a cada 30 segundos sozinho, de depois promova a versão”: Essa talvez seja a abordagem inicial dos espectros do canary automatizado. Temos alguns time-boxes que podem variar de segundos, minutos, horas ou dias e queremos que a progressão da porcentagem do uso progrida dentro desses intervalos. Tudo dependendo do nível de exigência e criticidade da aplicação ou o quão específica e crítica aquela mudança pode ser em relação ao todo. Então para este primeiro cenário, vamos aplicar o seguinte manifesto, adicionando vários steps com o peso desejado da porcentagem do rollout e informando as pausas desejadas para cada uma das progressões de carga. 123456789101112131415161718192021222324---apiVersion: argoproj. io/v1alpha1kind: Rolloutmetadata: name: chip namespace: chipspec: replicas: 10 strategy:  canary:   steps:   - setWeight: 20   - pause: {duration: 10}   - setWeight: 40   - pause: {duration: 10}   - setWeight: 60   - pause: {duration: 10}   - setWeight: 80   - pause: {duration: 10}   - setWeight: 100   revisionHistoryLimit: 2 selector:  matchLabels:   app: chipPara acompanhar os rollouts localmente, vamos utilizar a função watch do plugin do argo que instalamos no kubectl, onde podemos acompanhar a devida progressão dos pods entre as duas versões. 1kubectl argo rollouts get rollout chip -n chip --watch Para ilustrar, na imagem abaixo tem um loop de consumo de uma API que retorna a devida versão da mesma. Assim podemos acompanhar visualmente como funciona a progressão a quente para os consumidores do serviço em questão. Vamos usar essa abordagem a partir daqui.  Após todos os steps finalizarem, teremos nossa revision:2 marcada como stable e a revision:1 descontinuada. Porém ela vai ficar ali podendo ser reativada num possível rollback. Trataremos disso mais pra frente.  Visualmente, durante nossas interações com as requisições que retornam a versão da aplicação de teste, podemos ver ela totalmente promovida: Exemplo completo — Github “Eu preciso que meu canário progrida sozinho até 80%, porém eu gostaria de promover ou abortar o restante manualmente”: Um outro tipo de cenário de uso é uma progressão gradual controlada, porém que seja necessária uma intervenção humana para progressão da release em si. Nesse cenário hipotético, vamos imaginar que eu gostaria que a progressão funcione como o primeiro exemplo, de x em x tempo, porém pare em 80%. E a partir daí, alguém aprova ou aborta a mudança de versão. 123456789101112131415161718192021---apiVersion: argoproj. io/v1alpha1kind: Rolloutmetadata: name: chip namespace: chipspec: replicas: 10 strategy:  canary:   steps:   - setWeight: 20   - pause: {duration: 10}   - setWeight: 60   - pause: {duration: 10}   - setWeight: 80   - pause: {}  revisionHistoryLimit: 2 selector:  matchLabels://. . . Nesse caso, temos a opção de colocar uma instrução de pause sem uma duração definida, e nosso rollout entrará num status de Paused indefinidademente quando chegar no step. Nesse caso em especifico, precisaremos dar uma instrução para o argo manualmente continuar o rollout, ou abortá-lo e retornar a versão anterior.  Essas instruções são os comandos promote e abort também vindos do plugin do Argo Rollouts. O promote irá progredir o step atual, o abort irá cancelar o rollout e retornar todos os pods da aplicação para a versão anterior. Um adendo ao abort é que ele pode ser executado em qualquer momento do ciclo de vida do rollout. 12kubectl argo rollouts promote chip -n chipkubectl argo rollouts abort chip -n chipExemplo Completo — Github “Eu preciso que meu canário progrida até 20%, depois necessite de uma intervenção manual para continuar o rollout”: Esse processo é exatamente igual ao anterior, porém vamos pensar num case onde eu queria ser um pouco mais conservador e chato quanto ao meu rollout, e queria promover uma pequena porcentagem para o meu canary, olhar minha observability, acompanhar com mais calma antes de prosseguir com o rollout automatizado. Nesse caso somente precisamos adicionar o step pause depois de promover a primeira porcentagem. Assim precisaremos dar o promote para prosseguir, ou o abort logo de começo. 123456789101112131415161718192021222324---apiVersion: argoproj. io/v1alpha1kind: Rolloutmetadata: name: chip namespace: chipspec: replicas: 10 strategy:  canary:   steps:   - setWeight: 20   - pause: {}   - setWeight: 40   - pause: {duration: 10}   - setWeight: 60   - pause: {duration: 10}   - setWeight: 80   - pause: {duration: 10}   - setWeight: 100    revisionHistoryLimit: 2 selector:  matchLabels://. . . 1kubectl argo rollouts promote chip -n chipUma abordagem que deriva dessa é fazer o inverso, promover automaticamente até uma porcentagem menor, e pedir uma intervenção manual para progredir automaticamente até a estabilização da versão. Exemplo Completo — Github “Eu gostaria que meu canário tivesse passos de 10 em 10%, mas que eu consiga promover todos manualmente”: Pra uma abordagem mais conservadora e crítica, é possível que os rollouts aconteçam de forma contínua com baixa progressão de volume, em que cada step seja promovido manualmente. Esse cenário é interessante pra produtos que sejam extremamente sensíveis a falhas com volumes altos de clientes e que possam gerar prejuízos grandes em casos de erro. Em todo caso, uma abordagem mais cuidadosa. Imagine nesse cenário acima, você está trocando a API de um fornecedor, fez alguma melhoria de performance, trocou um flavor de banco, algum driver, atualizou versão de framework e etc, e gostaria que esse rollout acontecesse de forma extremamente validada. Nesse caso, vamos utilizar o step pause em seguida de cada progressão de porcentagem do rollout, sem as definições de duration. Nesse sentido em cada promoção de step, o argo vai paus Blue / Green Deployments: O Blue/Green Deployment, é diferente do Canary em sua concepção, no qual cumpre o objetivo de realizar uma validação prévia do deploy. A diferença para o canary, é que a a nova versão do aplicativo é implantada em paralelo ao ambiente de produção atual (ambiente Blue), sem afetar o tráfego do usuário da versão estável (ambiente Green), porém é configurada uma rota customizada para que seja possível realizar testes durante o processo de deploy antes de promover a versão nova. A estratégia Blue/Green Deployment é comumente usada em ambientes de alta disponibilidade, onde interrupções ou erros no ambiente de produção podem ter um grande impacto na experiência do usuário, basicamente é a melhor opção onde a aplicação é muito sensível a erros no geral. Essa técnica tem muito a agregar na maior parte do tempo, porém é considerada um pouco mais cara, e não permite fazer uma validação gradual de uma feature com o cliente real, por exemplo.  Configuração Inicial do Service para o Blue/Green: Diferente do modelo do canary que utiliza o mesmo Service precisamos de antemão criar 2 services, um que vai representar a versão active e outra que funcionará como a versão preview, ou trazendo pro termo genérico um será a versão blue e outra será a versão green. 1234567891011121314151617---apiVersion: v1kind: Servicemetadata: name: chip-active namespace: chip  labels:  app. kubernetes. io/name: chip  app. kubernetes. io/instance: chip spec: ports: - name: web  port: 8080  protocol: TCP selector:  app: chip type: ClusterIP123456789101112131415161718---apiVersion: v1kind: Servicemetadata: name: chip-preview namespace: chip   labels:  app. kubernetes. io/name: chip  app. kubernetes. io/instance: chip spec: ports: - name: web  port: 8080  protocol: TCP selector:  app: chip type: ClusterIP--- Também será necessário criar duas rotas no seu ingress, uma para validação no seu preview e outra para a versão ativa, algo parecido. No Istio, teríamos algo parecido com isso: 12345678910111213141516171819202122232425262728293031apiVersion: networking. istio. io/v1alpha3kind: Gatewaymetadata: name: chip-gateway namespace: chipspec: selector:  istio: ingressgateway  servers: - port:   number: 80   name: http   protocol: HTTP  hosts:  -  chip. k8s. raj. ninja ---apiVersion: networking. istio. io/v1alpha3kind: Gatewaymetadata: name: chip-gateway-preview namespace: chipspec: selector:  istio: ingressgateway  servers: - port:   number: 80   name: http   protocol: HTTP  hosts:  -  chip-preview. k8s. raj. ninja  “Eu gostaria de ter a capacidade de validar minha versão Green através de uma rota específica, e promover manualmente”: Escrevendo nosso rollout de blue/green, precisamos parametrizar inicialmente o activeService e o previewService de forma com que nosso rollout saiba quais services controlar durante as viradas de cargas e validação. E como a proposta desse cenário é validar e promover manualmente depois de certas validações, é importante setar o parâmetro autoPromotionEnabled como false. 12345678910111213141516171819---apiVersion: argoproj. io/v1alpha1kind: Rolloutmetadata: name: chip namespace: chipspec: replicas: 10 strategy:  blueGreen:    activeService: chip-active   previewService: chip-preview   autoPromotionEnabled: false revisionHistoryLimit: 2 selector:  matchLabels:   app: chip template://. . . Ao aplicar uma nova versão configurada como blue/green, o Argo mantém ambas as versões operando simultaneamente, com a mesma capacidade, aguardando side by side.  A configuração do seu ingress deve ser configurada para permitir o acesso às versões de diversas maneiras, seja por header, path, host ou outros métodos.  Para monitorar a transição entre as versões, vamos criar um loop para consumir ambas as versões da API em paralelo.  Após validar os comportamentos, podemos promover a nova versão da seguinte maneira: 1kubectl argo rollouts promote chip -n chip Agora aguardaremos até a versão revision:2 estabilizar.  Exemplo Completo — Github “Eu gostaria de que minha versão de preview recebesse alguns requests para warm up do meu runtime ou realizar testes antes de promover para a versão ativa”: O interessante do Blue Green é que ele trabalhe de forma com que você consiga validar o comportamento da sua versão nova manualmente, por processos automatizados, ferramentas de teste antes de promover a versão nova para o cliente final. Neste cenário irei realizar uma ferramenta de HTTP Bench para realizar um número considerável de requests para minha versão nova afim de simular um “warm up” do runtime. Esse processo pode ser interessante para workloads criados em JVM que precisam de uma “esquentada” nos primeiros instantes de vida para performar da melhor forma. Uma ideia interessante é utilizar seus testes de fumaça de forma containerizada, subir seu roteiro num container e executá-lo da mesma forma. Precisamos criar um AnalysisTemplate descrevendo que tipo de análise vamos realizar para dar uma flag na nossa versão. Nesse caso vou executar um container do cassowary em uma rota qualquer do meu serviço, simulando um endpoint de verdade que precise desse tipo de estratégia. 12345678910111213141516171819202122232425262728---kind: AnalysisTemplateapiVersion: argoproj. io/v1alpha1metadata: name: chip-http-warm-up namespace: chipspec: metrics: - name: http-bench-analysis  failureLimit: 1  provider:   job:    spec:     backoffLimit: 1     template:      metadata:       labels:        istio-injection: disabled        sidecar. istio. io/inject:  false       spec:       containers:       - name: http-bench-analysis        image: rogerw/cassowary:v0. 14. 0        command: [ cassowary ]        args: [ run ,  -u ,  &lt;http://chip-preview. chip. svc. cluster. local:8080/healthcheck&gt; ,  -c ,  3 ,  -n ,  1000 ]       restartPolicy: Never  count: 1---Agora no nosso rollout, vamos utilizar o prePromotionAnalysis passando o nosso AnalysisTemplate criado 1234567891011121314151617181920---apiVersion: argoproj. io/v1alpha1kind: Rolloutmetadata: name: chip namespace: chipspec: replicas: 10 strategy:  blueGreen:    activeService: chip-active   previewService: chip-preview   autoPromotionEnabled: false   prePromotionAnalysis:    templates:    - templateName: chip-http-warm-up revisionHistoryLimit: 2 selector:  matchLabels:// . . . Podemos validar o seguinte cenário coletando algumas métricas dos dois services. Também é uma boa prática acompanhar as duas versões em paralelo no momento de um rollout. Nesse caso, podemos ver um pico de requisições acontecendo no service preview conforme parametrizado no AnalysisTemplate.  Agora que já temos nosso warm up finalizado, vamos promover nosso rollout com nossos hipotéticos runtimes warmados para receber o tráfego real. 1kubectl argo rollouts promote chip -n chip Exemplo Completo — Github “Preciso executar uma bateria de testes na minha versão de preview, e criar uma análise de métricas automatizada para validar se a versão está saudável. É possível?”: Sim, assim como podemos criar um AnalysisTemplate pra executar os testes, podemos criar outro em seguida que a partir de métricas do Prometheus, consegue dar um sinal verde ou vermelho pra nossa aplicação finalizar o rollout. Nesse caso seria interessantes tanto o autoPromote: false como o autoPromote: true para workloads que tenham mais confiança. Vamos reaproveitar o AnalysisTemplate do exemplo anterior nesse aqui como uma continuidade. Além dele vamos criar um outro template onde vamos definir uma query PromQL que será executada em uma instância de prometheus que esteja agregando as métricas do nosso cluster. Para isso vamos precisar configurar algumas coisas, sendo elas o provider do prometheus onde vamos informar a URL do Prometheus, nesse caso tenho uma instancia rondando no meu cluster então informarei a URL do service. Em seguida vamos definir qual será a query de consulta. No caso do exemplo estou fazendo um calculo de SLO de disponibilidade avaliando os 5 ultimos minutos. Por ultimo vamos informar o successRate onde vamos colocar uma condição do teste ser aceito ou não. 123456789101112131415161718192021222324---apiVersion: argoproj. io/v1alpha1kind: AnalysisTemplatemetadata: name: chip-error-rate-check namespace: chipspec: metrics: - name: success-rate  interval: 2m  successCondition: result[0] &gt;= 0. 95  failureLimit: 1  provider:   prometheus:    address: &lt;http://prometheus. istio-system. svc. cluster. local:9090&gt;    query: |     sum(irate(      istio_requests_total{destination_service=~ chip-preview. chip. svc. cluster. local ,response_code!~ 5. * }[5m]     )) /     sum(irate(      istio_requests_total{destination_service=~ chip-preview. chip. svc. cluster. local }[5m]     ))  count: 1     ---Como no exemplo anterior, adicionar apenas mais um step após o nosso “smoke test” avaliando as métricas de disponibilidade do mesmo: 123456789101112131415161718192021222324---apiVersion: argoproj. io/v1alpha1kind: Rolloutmetadata: name: chip namespace: chipspec: replicas: 10 strategy:  blueGreen:    activeService: chip-active   previewService: chip-preview   autoPromotionEnabled: false   prePromotionAnalysis:    templates:    - templateName: chip-http-warm-up    - templateName: chip-error-rate-check revisionHistoryLimit: 2 selector:  matchLabels:   app: chip template:  metadata://. . . Aplicando iremos ver correr o blue/green como já estamos acostumados, porém com um adicional no qual iremos ver rodando os AnalysisTemplates, um que vai executar nossos testes / warm up e o do Prometheus que irá sumarizar as métricas do teste.  Para maiores informações a respeitos das análises que fazemos através dos templates, podemos verificar os objetos AnalysisRun 12kubectl get analysisrun -n chipkubectl describe analysisrun &lt;id&gt; -n chip Exemplo Completo — Github Rollbacks de Versões Anteriores: É uma premissa do canary que seja possível validar a nova versão aos poucos e a partir do tráfego do cliente conseguir validar o sucesso de uma implantação. Esse sucesso pode ser medido de diversas formas, a olho nú ou de formas automatizadas como pudemos ver nesse artigo. Mas independente do modelo de deployment aplicado no no produto, é muito importante que existam ferramentas que possibilitem alternativas de rollback de forma rápida. Repetindo mais uma vez: “Melhor que entregar rápido, é voltar rápido” Vamos imaginar um cenário hipotético em que uma release de canário começou a ser promovida e durante os steps, podemos identificar que uma taxa de erros incomum começou a subir junto a progressão dos steps.  Como já vimos, existe a opção abort para reverter um rollout durante sua execução, podendo ser executado a qualquer momento. 1kubectl argo rollouts abort chip -n chip; Já vimos essa dinâmica antes, mas fui um pouco repetitivo propositalmente para mostrar uma alternativa pra quando a versão com erro já foi promovida e estabilizada e será necessário um rollback.  “Mesmo com o canário ou blue/green, eu comi bola e por um comportamento não previsto, preciso voltar a versão anterior rápido”: O comando de abort pode ser executado em qualquer momento do ciclo de vida de implementação do Rollout, porém quando a versão é promovida para stable o processo é um pouco diferente. Depois de uma implantação finalizada, é necessário o rollout de uma versão anterior do zero, nesse caso iremos utilizar o comando… pasmem, chamado rollback. Após o inicio do rollback será realizado um rollout novo, promovendo a versão anterior. 1kubectl argo rollouts rollback chip -n chip;Tanto no caso anterior do abort quando nessa do rollback o resultado final de uma implantação que teve um plano de retorno deverá ser parecida com a abaixo, onde a taxa de erro anômala identificada retorna aos níveis normais da aplicação.  Horizontal Pod Autoscaler / Vertical Pod Autoscaler: Agora vamos para algumas dicas úteis pra resolver alguns problemas que você venha a encontrar durante seus testes e migração do Argo. Caso você esteja utilizando HPA/VPA em seu workload, será necessário fazer algumas modificações no objeto do HorizontalPodAutoscaler e VerticalPodAutoscaler alterando o scaleTargetRef trocando o apiVersion de apps/v1 para argoproj. io/v1alpha1 e o Kind de Deployment para Rollout , assim conseguimos acertar as referencias. 123456789101112131415161718192021---apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: chip namespace: chipspec: maxReplicas: 10 minReplicas: 10 metrics:  - type: Resource   resource:    name: cpu    target:     type: Utilization     averageUtilization: 60 scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: chip---Para isso 123456789101112131415161718192021---apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: chip namespace: chipspec: maxReplicas: 10 minReplicas: 10 metrics:  - type: Resource   resource:    name: cpu    target:     type: Utilization     averageUtilization: 60 scaleTargetRef:  apiVersion: argoproj. io/v1alpha1  kind: Rollout  name: chip--- Dashboard, uma alternativa mais amigável para acompanhamento dos rollouts. : Para acessar a dashboard será necessário também expor o pod argo-rollouts-dashboard no seu ingress. Como no caso estou utilizando Istio, o exemplo seria esse: 123456789101112131415161718192021222324252627282930313233343536373839---apiVersion: networking. istio. io/v1alpha3kind: Gatewaymetadata: name: argo-dashboard namespace: argo-rolloutsspec: selector:  istio: ingressgateway  servers: - port:   number: 80   name: http   protocol: HTTP  hosts:  -  argo-rollouts-dashboard   -  dashboard-argo. k8s. raj. ninja ---apiVersion: networking. istio. io/v1alpha3kind: VirtualServicemetadata: name: argo-dashboard namespace: argo-rolloutsspec: hosts: -  argo-rollouts-dashboard  -  dashboard-argo. k8s. raj. ninja  gateways: - argo-dashboard http: - route:  - destination:    host: argo-rollouts-dashboard    port:     number: 3100  retries:   perTryTimeout: 500ms   retryOn: 5xx,gateway-error,connect-failure,refused-stream---Agora tendo acesso a dashboard, podemos utilizar a mesma para manipular nossos rollouts de forma visual, podendo abortar, promover, acompanhar e dar rollback a qualquer momento com poucos clicks.  Referencias / Materiais de Apoio  Argo-Rollouts — Site Oficial Argo-Rollouts — Analysis Template Argo-Rollouts — Prometheus Provider BlueGreenDeployment — Martin Fowler Software Delivery Guide Kubernetes Deployments ArgoProject — Argo RolloutsObrigado aos revisores:  Carlos Panato — @comedordexis Bernardo — @indiepagodeiro Caio Volpato — @caioauv Diego Murta Freire — @diegomurta Marcelo Freire — @marcelofreire28Me sigam no Twitter para acompanhar as paradinhas que eu compartilho por lá! Te ajudei de alguma forma? Me pague um café (Mentira, todos os valores doados nessa chave são dobrados por mim e destinados a ongs de apoio e resgate animal) Chave Pix: fe60fe92-ecba-4165-be5a-3dccf8a06bfc "
    }, {
    "id": 7,
    "url": "http://localhost:4000/karpenter-estrategias-para-resiliencia-no-uso-de-spot-instances-em-producao/",
    "title": "Karpenter — Estratégias para resiliência no uso de Spot Instances em produção",
    "body": "2022/10/09 - IntroduçãoEsse é o segundo artigo que eu publico sobre Karpenter. Dessa vez decidi trazer um ponto de vista bem legal que é a adoção de uso de Spots em produção. Utilizar spots é uma estratégia muito comum pra quem deseja algum saving na conta da AWS no fim do mês, podem ser utilizada em formas de EC2 diretamente, Containers, Workloads de data e etc. As instâncias spots nos permitem utilizar capacity ocioso do EC2 na AWS, gerando um saving de até 90% no custo das instâncias comparados aos preços On Demand. Porém a AWS não da uma garantia de que sua instância irá durar para sempre, podendo ser retirada do seu workload a qualquer momento. Por essa questão, o produto é desenhado pra aplicações que rodem no modelo mais stateless possível. É uma boa pratica ver ambientes de desenvolvimento e teste rodando 100% em spots para diminuir custo, mas ainda existe um certo “receio” em ver ambientes produtivos utilizando toda, ou parte, da sua carga de trabalho em spots. Mas é possível, se utilizarmos de algumas estratégias para isso. A ideia desse artigo é demonstrar possibilidades do uso do Karpenter para ganhar um pouco mais de resiliência em produção em ambientes que rodam totalmente, ou parcialmente com Spots. Caso você não tenha visto ainda, fiz um artigo sobre uma PoC onde descrevo como criar um ambiente em Amazon EKS sem Node Groups, utilizando somente o Karpenter pra suprir o capacity computacional. Update 17/11/2023 - Alguns manifestos mudam sua estrutura a partir da versão 0. 32. x do Karpenter. Nessa data de hoje aproveitei para atualizar os exemplos para os schemas mais novos. Confira o blogpost do Edson sobre o tema.                                   Provisionando um cluster de EKS sem Node Groups com Karpenter    :     A proposta dessa PoC é criar e gerenciar um cluster de EKS utilizando apenas (ou quase) o Karpenter como provisionamento de recursos…          medium. com   Caso você não tenha visto ainda, te convido para ler um artigo que eu escrevi sobre como utilizar o Istio para sobreviver a cenários de caos. Não tem nada a ver com o tema, mas eu acho que você vai gostar. #Confia.                                   Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS    :     Na sua casa você pode usar o que você quiser, aqui hoje vamos usar Istio. Sem tempo pra chorar irmão…          medium. com   Todos os exemplos aqui do texto estão feitos de forma resumida, porém você pode encontrá-los de forma completa neste repositório do Github REPOSITÓRIO - eks-karpenter-autonomous-cluster/examples/spots at main · msfidelis/eks-karpenter-autonomous-cluster Cenário Inicial: Nesse artigo iremos abordar as estratégias:  Multi-AZ Diversificação de Instâncias Diversificação entre capacity Spots x On DemandTodos os cenários vão seguir a mesma formula, vamos escalar todos os pods de 2 para 100 e ver como o provisionamento vai se comportar.  Multi-AZ: Inicialmente, vamos fazer o básico em relação a ambientes produtivos, sendo ele rodando em spots ou não. Rodar em Multi AZ é o arroz com feijão quando falamos sobre cloud pública no geral. E quando falamos em ambientes 100% spots, onde vamos executar esse primeiro cenário, é praticamente impossível ganhar qualquer tipo de estabilidade sem rodarmos Multi AZ. Vamos adicionar uma especificação sobre a label topology. kubernetes. io/zone tendo todas as AZ’s que sua aplicação deverá utilizar 400: Invalid requestAgora vamos realizar uma modificação no nosso deployment utilizando os topology spreads e skews. O controlador do Karpenter vai se basear nessa informação pra realizar o provisionamento dos nodes quando precisar suprir um capacity. 400: Invalid request Diversificação de Máquinas: Uma das estratégias mais efetivas pra se proteger contra compras bruscas de tipos de instancias especificas de spots é a diversificação. Isso significa subir mais de um tipo de familia e tamanho no workload. Assim, se subirmos um pool de c5. large, m5. large e r5. large, caso exista uma compra massiva de algum desses tamanhos, podemos proteger de forma segura a disponibilidade de nossas aplicações se elas forem distribuídas de forma inteligente entre os nodes. Primeiramente vamos adicionar/alterar no Provisioner a spec baseada na label node. kubernetes. io/instance-type dos nodes, e nela vamos adicionar uma lista contendo os tipos de familia que podem ser lançadas para suprir capacity. 400: Invalid requestVamos editar o deployment e adicionar o topology spread baseado na label node. kubernetes. io/instance-type também. Dessa forma vamos direcionar uma distribuição do nosso deployment entre os tipos de instancia, assim como fizemos com as AZ’s. 400: Invalid requestVamos fazer o scale do deployment de 2 para 100 pra ver como a distribuição irá ocorrer. 1kubectl scale --replicas 100 deploy/chip -n chip Dessa forma, conseguimos gerar uma distribuição bem tranquila entre os tamanhos de nodes do cluster pra suprir o novo capacity solicitado conforme o esperado.  Diversificação On Demand x Spots: Uma estratégia mais conservadora e segura de se usar spots em produção baseia-se em fazer uma diversificação entre instancias Spots e On Demand. Mantendo uma porcentagem do workload em extrema segurança. Nesse sentido, o Karpenter também nos permite selecionar mais de um tipo de Capacity Type na label karpenter. sh/capacity-type. 400: Invalid requestAgora vamos ajustar o Spread Constraint também como fizemos nos exemplos anteriores para distribuir os pods entre os tipos de nodes (já que agora temos não só o uso de spots, mas também nodes on-demand) assim como fizemos com as AZ’s e os tipos de instâncias. 400: Invalid requestDessa forma também conseguimos instruir o Karpenter pra subir de forma diversificada a quantidade de Spots vs On Demand.  Node Termination Handler - DEPRECATED: O Node Termination Handler é uma forma interessante de fazer Drain dos nossos nodes com base em notificações de Spot Interruptions, Rebalance Recommendations do Autoscale Group ou de um desligamento padrão das EC2. Esses eventos podem ser muito comuns quando tratamos de ambientes voláteis que utilizam estratégias de Spots.  Update 17/11/2023 - Segundo a comunidade do Karpenter, não é mais recomendado o uso do Node Termination handler pois o proprio componente agora faz Interruption Handling. - Link A Referencia continuará no post mas os detalhes de implementação serão removidos. Para consulta de referencia, os detalhes de implementação estão neste link No proximo topico abordaremos o Interruption Handler nativo do Karpenter. —&gt; O Node Termination Handler é uma forma interessante de fazer Drain dos nossos nodes com base em notificações de Spot Interruptions, Rebalance Recommendations do Autoscale Group ou de um desligamento padrão das EC2. Esses eventos podem ser muito comuns quando tratamos de ambientes voláteis que utilizam estratégias de Spots.  Não vou abordar muitos detalhes do provisionamento, porém vou deixar um exemplo completo no Github. Mas basicamente, precisamos provisionar o chart do aws-node-termination-handler, informando uma URL de um SQS e habilitando o enableSqsTerminationDraining. Questões como IAM estão detalhadas no repositório. 400: Invalid requestPrecisamos provisionar uma série de Event Rules recomendadas na documentação do projeto e enviá-las para o SQS informado no chart pela queueURL. 400: Invalid requestSempre que um evento de desligamento de Spot for informado, ou solicitado via console, cli, api, um evento será enviado para essa fila SQS, consumido pelo aws-node-termination-handler que se encarregará de fazer um drain dos pods a tempo suficiente para realocá-los em outros nodes.  &lt;—–&gt; Karpenter - Interruption Handling: Update 17/11/2023 - Como mencionado no topico anterior, agora o Karpenter possui a feature de realizar handling dos nodes que mudam de estado. Para fazer o provisionamento temos que criar a fila SQS da mesma forma como 400: Invalid requestReferências / Material de Apoio:  EKS Best Pratices https://aws. github. io/aws-eks-best-practices/karpenter/ Pod Topology Spread Constraints https://kubernetes. io/docs/concepts/scheduling-eviction/topology-spread-constraints/ Karpenter Provisioner API https://karpenter. sh/v0. 5. 6/provisioner/ Karpenter Topology Spread https://karpenter. sh/v0. 13. 2/tasks/scheduling/#topology-spread EC2 Spot Best Pratices https://docs. aws. amazon. com/AWSEC2/latest/UserGuide/spot-best-practices. html EC2 Spots https://aws. amazon. com/pt/ec2/spot/ Node Termination Handler https://github. com/aws/aws-node-termination-handlerObrigado aos revisores:  @Daniel_Requena Marcos Magalhães (@mmagalha) Rafael Gomes (@gomex) @indiepagodeiro Edson Celio (@tuxpilgrim)Me sigam no Twitter para acompanhar as paradinhas que eu compartilho por lá! Te ajudei de alguma forma? Me pague um café (Mentira, todos os valores doados nessa chave são dobrados por mim e destinados a ongs de apoio e resgate animal) Chave Pix: fe60fe92-ecba-4165-be5a-3dccf8a06bfc "
    }, {
    "id": 8,
    "url": "http://localhost:4000/provisionando-um-cluster-de-eks-sem-node-groups-com-karpenter/",
    "title": "Provisionando um cluster de EKS sem Node Groups com Karpenter",
    "body": "2022/08/05 - A proposta dessa PoC é criar e gerenciar um cluster de EKS utilizando apenas (ou quase) o Karpenter como provisionamento de recursos computacionais pro Workload produtivo, tirando a necessidade de Node Groups e Auto Scale Groups. Trazendo todo o gerenciamento de recursos pra dentro de CRD’s do Karpenter. Nesse cenário vamos assumir algumas premissas importantes:  O objetivo do Karpenter como tecnologia é prover um “just in time” scale, o que faz dele uma proposta interessante para workloads que tenham picos de acesso, processamentos agendados mais pesados e tenham um delta de escalabilidade computacional mais agressivos.  Essa proposta é excelente para muitos casos de uso, mas também é preciso assumir que gera uma volatilidade muito brusca na quantidade de nós e pods. Por isso é ideal que as aplicações e suas dependências estejam preparadas para morrer com segurança e aumentar ou diminuir o consumo de recursos na mesma proporção.  Como um cluster de Kubernetes é composto por várias “pecinhas de lego” muito importantes, e que muitas vezes não estão preparadas para lidar com essa volatilidade agressiva para qual essa PoC está sendo desenhada, o modo mais intuitivo que trouxe para resolver esse cenário foi colocar os namespaces de serviço, como prometheus, kube-system e outras aplicações “satélites” em nodes Fargate, para que eles sejam poupados dessas mudanças bruscas de capacity. ProvisionamentoVou omitir bastante detalhes do código como um todo para não transformar esse artigo numa bíblia, mas fique tranquilo que todo o desenvolvimento está sendo documentado neste repositório do GitHub.  Roles de IAM: Antes de qualquer coisa vamos precisar provisionar uma série de roles com as permissões necessárias para o provisionamento dos recursos e configurações dos componentes. Como qualquer cluster de Kubernetes vamos precisar providenciar com antecedência 3 tipos de roles. Uma para o Control Plane, outra que será usada como Instance Profile para as instâncias dos Nodes e outra para os Fargate Profiles. Roles de IAM — Cluster: Iniciando pela role do Control Plane precisamos associar 2 managed policies, AmazonEKSClusterPolicy e AmazonEKSServicePolicy. 400: Invalid request Roles de IAM — Nodes / Instance Profile: O provisionamento da Instance Profile dos nodes também não muda caso você fosse usar com Node Groups, com exceção de que vamos precisar criar a instance profile propriamente dita. Quando utilizamos Node Groups o próprio serviço do EKS se encarrega de fazer a criação desse recurso caso não exista previamente. Mas é simples. Vamos precisar associar algumas Managed Policies padrão também para funcionar. Mas sem segredo de outros tipos de provisionamento. 400: Invalid requestVamos criar uma associação de instance profile na role criada para os nodes para posteriormente criamos o Launch Configuration com ela. 400: Invalid request Roles de IAM — Fargate Profiles: O provisionamento da role dos Fargate Profiles também é padrão. Escrevendo esse artigo me vem aquela sensação de “pow, essas roles já poderiam existir na conta por default né? Chatão”. Pois é. Funciona no mesmo esquema das anteriores, precisamos anexar algumas managed policies padrão para que o serviço funcione. 400: Invalid request EKS Cluster: O provisionamento do cluster foi feito sem a base de um modulo ou facilitador. Até mesmo porque não seria interessante provisionar nada além do próprio control plane do EKS para PoC nesse primeiro momento. Vamos utilizar o recurso base do aws_eks_cluster nos atentando as tags de discovery do Karpenter que precisam estar presentes. 400: Invalid request Fargate Profile — Kube System: Como dito anteriormente nas premissas da PoC, tudo que se encaixar como um componente sistêmico do funcionamento da plataforma, e não como parte do workload será provisionado em Fargate Profiles para poupá-los da volatilidade de scale in / scale out que iremos trazer para o cluster com o Karpenter. Dito isso, vamos provisionar o fargate profile para o namespace do kube-system. 400: Invalid request CoreDNS Fix — Workaround: Uma das coisas mais chatas e sem sentido do uso de cluster Full Fargate é a limitação do CoreDNS de subir naturalmente em nodes que não sejam EC2 efetivamente. Até a data desse artigo, é necessário utilizar de algum artificio automatizado ou manual para remover a label de eks. amazonaws. com/compute-type de ec2 para que ele consiga ser provisonado em nodes fargate. Você pode fazer isso manualmente sem problemas diretamente com o kubectl. 1kubectl patch deployment coredns -n kube-system --type json -p='[{ op :  remove ,  path :  /spec/template/metadata/annotations/eks. amazonaws. com~1compute-type }]'Porém como preguiça pouca é besteira, eu vou utilizar uma lambda que após o provisionamento do cluster se encarrega de remover essa label através da API do control plane. Peguei a base inicial dessa lambda através do artigo Deploy CoreDNS on Amazon EKS with Fargate automatically using Terraform and Python escrito por Kevin Vaughan e Lorenzo Couto da AWS. Porém fiz algumas modificações de stepback e retry para realização dessa configuração porque algumas vezes falhava nas primeiras tentativas pela API não estar tão disponível quanto deveria. O provisionamento dela está em um repositório de exemplo separado para ajudar em casos isolados e também futuramente transformar em módulo que resolve esse problema. Dor de cabeça pro Matheus do futuro. No caso no Terraform iremos empacotar o script normalmente e criar a lambda na VPC que entregamos o cluster. 400: Invalid requestNenhuma trigger é necessária para esse primeiro momento. Ao invés disso na pipeline vamos chamar um aws_lambda_invocation passando o endpoint do cluster e um token temporário que será utilizado para fazer o request com o Patch removendo a label. Isso irá forçar um redeploy do coreDNS, porém fazendo ele subir em nodes fargate com o planejado na própria execução da pipeline. Ganhando bastante tempo e diminuindo “gols de mão” do processo. 400: Invalid requestGitHub - msfidelis/eks-coredns-fargate-fix: Lambda setup to fix CoreDNS deployments to run on Fargate Clusters Provisionamento do Karpenter: O provisionamento do Karpenter com Terraform e Helm não tem segredo. Exemplo foi adaptado direto da excelente documentação do projeto. São passos bem semelhantes dos que vimos até agora. Onde será necessário providenciar uma Role para o serviço com um assume role federado para o OIDC do cluster (exemplo completo no repositório).  IAM Role: A role do karpenter precisa ter algumas permissões bem semelhantes ao Cluster Autoscaler caso você já tenha utilizado. Ele precisa de algumas permissões de controle de EC2 para poder lançar e deletar elas sob demanda. Sem segredo por aqui. 400: Invalid request Karpenter — Fargate: Segundo passo é colocar o Karpenter para rodar em Fargate Profile semelhante como fizemos com o kube-system, para impedir de que um drain de nodes afete o próprio karpenter e dê algum problema no processo de uma forma geral. Então, seguindo a premissa de que se não é workload, está seguro em fargate, vamos subir ele também. 400: Invalid request Karpenter — Helm: O Setup do Helm é baseado no da documentação do Karpenter com Terraform também. Vamos passar a role que criamos amarrada ao OIDC e ao WebIdentityProvider na Service Account para que o controlador possa executar operações nas API’s da AWS. 400: Invalid request Após o provisionamento teremos também o pod do Karpenter com os dois containers internos em estado de Ready/Running rodando em um Node Fargate idêntico aos do exemplo do kube-system.  Karpenter — Provisioner e Templates: Agora vamos trabalhar com o real diferencial dessa PoC com os demais tipos de provisionamento mais comuns. Caso você já tenha trabalhado com o provisionamento de clusters de EKS com Node Groups com Launch Templates customizados, essa parte será bem parecida. No caso vamos criar um launch template versionado utilizando as AMI’s recomendadas da AWS e a instance profile que criamos de antemão. Alguns passos de configuração como user-data foram omitidos do artigo, mas ressaltando que podem ser consultados no repositório de exemplo do artigo. 400: Invalid requestEm seguida vamos criar dois objetos pelo objeto kubectl_manifest do provider do kubectl para fazer deploy de dois recursos do CRD do Karpenter, um deles sendo o Provisioner onde vamos especificar os tamanhos de instancias, limites de CPU e memória e coisas relacionadas a capacity e outro sendo um AWSNodeTemplate onde vamos especificar os launch templates dos nodes. 400: Invalid requestPara facilitar eu optei por usar templatefile para criar os manifestos que seriam aplicados pelo provider do kubectl. No caso para ficar mais evidente, coloquei algumas variáveis para fazer o build dos YAMLs via template dessa forma: 400: Invalid request400: Invalid requestNo final será criado um resource parecido com o abaixo: 400: Invalid requestDisclaimer: Durante a PoC tentei utilizar o provider do kubernetes para criar os objetos customizados do Karpenter diretamente pelo kubernetes_manifests, porém existe um bug de dependências no resource que inviabiliza o provisionamento de CRD’s juntamente com o cluster. Por isso precisei utilizar o kubectl provider para que continue sendo possível o provisionamento de toda a infraestrutura de uma única vez. Abri uma issue que permanece aberta (até esse momento) pra isso: Error: Failed to construct REST client on kubernetes_manifest resource · Issue #1775 · hashicorp/terraform-provider-kubernetes Aplicação de TestesAgora que temos todos os recursos do cluster minimamente provisionados, vamos testar o funcionamento do Karpenter. Vamos fazer deploy de uma aplicação de exemplo para ver se os nodes vão ser provisionados para suprir o novo capacity solicitado. 12345❯ kubectl apply -f files/deploy/demo/chip. yamlnamespace/chip createddeployment. apps/chip createdservice/chip createdhorizontalpodautoscaler. autoscaling/chip createdNo caso foi provisionado para suprir os 2 novos pods solicitados para a aplicação de exemplo. Agora vamos executar os cenários de scale in e out para ver como o ambiente se comporta.   Cenário 1 — Scale In: Vamos exemplificar o cenário onde temos 4 nodes iniciais no cluster, e vamos fazer o scale de um deployment de 2 replicas para 100 de forma brusca para ver como o Karpenter vai lidar com essa mudança de capacity solicitado. 1kubectl scale --replicas 100 deployment/chip -n chip Replicas iniciais do deployment: 4Replicas desejadas do deployment: 100CPU Requests: 250mRAM Requests: 512mQuantidade de Nodes Iniciais: 4Quantidade de Nodes final: 25Horário do Apply: 17:10:35Horário do scale finalizado: 17:13:50Tempo Total: 00:03:25 Conseguimos provisionar um capacity para suprir uma demanda brusca de 4 para 100 unidades computacionais que necessitavam de nodes em 3 minutos.  Cenário 2 — Scale Out: Agora vamos testar o cenário inverso, onde vamos fazer o scale out do ambiente de forma brusca para avaliar como karpenter vai lidar com esse capacity fora de uso.  1kubectl scale --replicas 4 deployment/chip -n chipReplicas iniciais do deployment: 100Replicas desejadas do deployment: 4CPU Requests: 250mRAM Requests: 512mQuantidade de Nodes Iniciais: 25Quantidade de Nodes final: 4Horário do Apply: 17:18:55Horário do scale finalizado: 17:19:50Tempo Total: 00:00:55 Para o scale out de nodes em desuso foi ainda melhor que scale in, fazendo um desligamento em massa de 25 nodes para 4 em 55 segundos. Lembrando que toda a PoC foi disponibilizada no Github.  Obrigado aos Revisores::  Rafael Silva — @rafaotetra Gabriel Machado — @gmsantos_ Somatorio — @somatorio Referencias::  Karpenter — Getting Started with Terraform — https://karpenter. sh/v0. 5. 3/getting-started-with-terraform/ Karpenter Best Pratices — https://aws. github. io/aws-eks-best-practices/karpenter/ Karpenter — Topology Spreads — https://karpenter. sh/v0. 13. 2/tasks/scheduling/#topology-spread Deploy CoreDNS on Amazon EKS with Fargate automatically using Terraform and Python — https://docs. aws. amazon. com/prescriptive-guidance/latest/patterns/deploy-coredns-on-amazon-eks-with-fargate-automatically-using-terraform-and-python. html Me sigam no Twitter para acompanhar as paradinhas que eu compartilho por lá! Te ajudei de alguma forma? Me pague um café (mentira, todas as doações são convertidas para abrigos de animais da minha cidade) Chave Pix: fe60fe92-ecba-4165-be5a-3dccf8a06bfc "
    }, {
    "id": 9,
    "url": "http://localhost:4000/sobrevivendo-a-cenarios-de-caos-no-kubernetes-com-istio-e-amazon-eks/",
    "title": "Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS",
    "body": "2021/11/13 - Na sua casa você pode usar o que você quiser, aqui hoje vamos usar Istio. Sem tempo pra chorar&nbsp;irmão…IntroduçãoO objetivo desse post é apresentar alguns mecanismos de resiliência que podemos agregar em nosso Workflow para sobreviver em (alguns) cenários de caos e desastres utilizando recursos do Istio e no Amazon EKS. Obviamente isso não é “plug n’ play” pra qualquer cenário, então espero que você absorva os conceitos e as ferramentas e consiga adaptar para o seu próprio caso. A ideia é estabelecer uma linha de pensamento progressiva apresentando cenários de desastre de aplicações, dependências e infraestrutura e como corrigi-los utilizando as ferramentas apresentadas. Para este post foi criado um laboratório simulando um fluxo síncrono onde temos 4 microserviços que se comunicam entre si, simulando um sistema distribuído de pedidos, onde todos são estimulados por requisições HTTP.  Premissas Iniciais::  O ambiente roda em um EKS utilizando a versão 1. 20 do Kubernetes em 3 zonas de disponibilidade (us-east-1a, us-east-1b e us-east-1c) Vamos trabalhar com um SLO de 99,99% de disponibilidade para o cliente final O ambiente já possui Istio default com Gateways e VirtualServices Vanilla configurados pra todos os serviços O objetivo é aumentar a resiliência diretamente no Istio, por isso nenhuma aplicação tem fluxo de circuit breaker ou retry pragmaticamente implementados.  Vamos utilizar o Kiali, Grafana para visualizar as métricas Vamos utilizar o K6 para injetar carga no workload Vamos utilizar o Chaos Mesh para injetar falhas de plataforma do Kubernetes Vamos utilizar o gin-chaos-monkey para injetar falhas a nível de aplicação diretamente no runtime O objetivo não é avaliar arquitetura de solução, e sim focar nas ferramentas apresentadas para aumentar resiliência. Topologia do Sistema: Esta é a representação do fluxo de comunicação entre as aplicações do teste. Todas são mocks mas executam chamadas entre si simulando clientes e servers reais de domínio.  Hipótese 1: Resiliência em falhas de aplicaçãoO objetivo é coletar as métricas de disponibilidade do fluxo síncrono com qualquer componente podendo falhar a qualquer momento. Primeiro teste tem o objetivo de injetar uma carga de 60s, simulando 20 VUS (Virtual Users) em todos os cenários, para ver como esses erros se comportam em cascata até chegar no cliente final à medida que vamos criando mecanismos de resiliência. Todas as aplicações implementando um middleware de chaos que injeta falhas durante a requisição HTTP, portanto em qualquer momento, qualquer uma delas poderá sofrer um:  Memory Assault — gerando memory leaks constantes CPU Assault — injetando overhead de recursos Latency Assault — incrementando o response time entre 1000ms e 5000ms Exception Assault — devolvendo aleatoriamente um status de 503 com falha na requisição AppKiller Assault — fazendo o runtime entrar em panic()Logo a hipótese a ser testada é:  “Minhas aplicações podem gerar erros sistêmicos aleatoriamente que mesmo assim estarei resiliente para meu cliente final” Cenários 1. 1 — Cenário Inicial: Vamos rodar o teste de carga do k6 no ambiente para ver como vamos nos sair sem nenhum tipo de mecanismo de resiliência: 1k6 run --vus 20 --duration 60s k6/loadtest. js Podemos ver que o chaos monkey da aplicação cumpriu seu papel, injetando falhas aleatórias em todas as dependências da malha de serviços, ofendendo drasticamente nosso SLO de disponibilidade para 88. 10%, estourando nosso Error Budget para este período do teste.  Podemos ver também que todas as aplicações da malha, em algum momento apresentaram falhas aleatórias, gerando erros em cascata.  Sumário do teste 1. 1 — Cenário inicial::  Tempo do teste: 60s Total de requisições: 13905 Requests por segundo: 228. 35/s Taxa de erros a partir do client: 11. 91% Taxa real de sucesso do serviço principal orders-api: 88. 10% Taxa de sucesso dos consumidores do orders-api: 88. 10% SLO Cumprido: Não Yaml dos testes executados no cenário Cenário 1. 2 — Implementando retry para Falhas HTTP: O objetivo do cenário é implementar a política de retentativas para falhas HTTP nos VirtualServices previamente configurados das aplicações. Vamos as opções 5xx, gateway-error, connect-failure. Podendo ocorrer até 3 tentativas de retry com um timeout de 500ms. 400: Invalid requestAs opções de retentativas iniciais de acordo com a documentação, possuem as seguintes funções:  5xx: Ocorrerá uma nova tentativa se o servidor upstream responder com qualquer código de resposta 5xx gateway-error: Uma política parecida com o 5xx, porém voltadas a falhas específicas de gateway como 502, 503, ou 504 no geral. Nesse caso, é redundante, porém fica de exemplo.  connect-failure: Será realizada mais uma tentativa em caso de falha de conexão por parte do upstream ou em casos de timeout. Vamos rodar novamente os testes simulando 20 usuários por 60 segundos com os 3 retries aplicados.  Conseguimos uma melhoria de mais de 6% de disponibilidade entre o que o serviço degradado respondeu com o que o cliente recebeu, utilizando apenas 3 tentativas de retry entre todos os serviços. Tivemos já um grande saving de disponibilidade para o cliente final, porém ainda ofendemos nosso SLO, batendo 99,25% de disponibilidade.  Sumário do teste 1. 2 — Retry para falhas HTTP::  Tempo do teste: 60s Total de requisições: 17873 Requests por segundo: 293. 17/s Taxa de erros a partir do cliente: 0. 07% Taxa real de sucesso do serviço principal orders-api: 93. 08% Taxa de sucesso dos consumidores do orders-api: 99. 25% SLO Cumprido: Não Yaml dos testes executados no cenário Cenário 1. 3 — Ajustando a quantidade de retries para suprir o cenário: Para resolver o problema, aumentei o número de retries de 3 para 5 e repeti os testes nos mesmos cenários. Em ambientes reais, esse tipo de ajuste pode ser derivado de um número “mágico”, encontrado após várias repetições do mesmo teste. Executei mais algumas vezes até atingir 0% de erros consistentemente.  Neste cenário, conseguimos superar os 93. 09% de disponibilidade fornecidos pelos upstreams, garantindo 100% de disponibilidade para o cliente com os cenários de retry. Mesmo com falhas aleatórias sendo injetadas, a disponibilidade final para o cliente foi assegurada. 400: Invalid request Sumário do teste 1. 3 — Ajustes na quantidade de retries para a hipótese::  Tempo do teste: 60s Total de requisições: 18646 Requests por segundo: 308. 79/s Taxa de erros a partir do cliente: 0. 00% Taxa real de sucesso do serviço principal orders-api: 93. 09% Taxa de sucesso dos consumidores do orders-api: 100. 00% SLO Cumprido: Sim Yaml dos testes executados no cenárioHipótese concluída com sucesso! Hipótese 2: Resiliência em falhas dos PodsPara realizar os testes de infraestrutura nos pods, utilizaremos o Chaos Mesh como ferramenta para injetar falhas nos componentes do nosso fluxo, e desligaremos o chaos-monkey no runtime das aplicações. A partir deste ponto, não injetaremos mais falhas intencionais a partir da aplicação para testar puramente falhas a nível de plataforma. O objetivo é analisar como o nosso fluxo síncrono se comporta diante da perda brusca de unidades computacionais em diversos cenários, e como nosso processo de melhoria contínua com retries pode nos ajudar e agregar ainda mais valor como plataforma. Portanto, a hipótese é:  “Posso perder pods e unidades computacionais de várias maneiras, mas minha aplicação continuará resiliente para o cliente final. ” Cenário 2. 1 — Injetando falhas de healthcheck nos componentes do workload: Neste primeiro cenário, vamos injetar o mesmo volume de requisições e, durante o processo, aplicar o cenário de pod-failure em nosso fluxo. Em todas as aplicações, realizaremos um teste de 30 segundos no qual perderemos 90% dos nossos pods repentinamente devido a falhas de healthcheck. Este é um teste bastante agressivo, com o propósito de verificar como as estratégias que implementamos até o momento agregam valor nesse cenário. 400: Invalid requestVamos iniciar o teste nos mesmos cenários e, no meio do processo, aplicar os cenários de PodFailure do Chaos Mesh em todas as aplicações. 123456kubectl apply -f chaos-mesh/01-pod-failture/podchaos. chaos-mesh. org/cc-pod-failure createdpodchaos. chaos-mesh. org/clients-pod-failure createdpodchaos. chaos-mesh. org/orders-pod-failure createdpodchaos. chaos-mesh. org/payment-pod-failure createdVamos conferir o status dos&nbsp;pods: 1234567891011❯ kubectl get pods -n ordersNAME READY STATUS RESTARTS AGEorders-api-fb5c94987-225zp 0/2 Running 2 100sorders-api-fb5c94987-8rpjb 0/2 Running 2 14morders-api-fb5c94987-bmnqm 0/2 Running 2 85sorders-api-fb5c94987-d9c4f 0/2 Running 2 14morders-api-fb5c94987-gd745 0/2 Running 2 14morders-api-fb5c94987-htbcn 2/2 Running 0 100sorders-api-fb5c94987-rzqkg 0/2 Running 2 100sorders-api-fb5c94987-st8l2 0/2 Running 2 85s123456❯ kubectl get pods -n ccNAME READY STATUS RESTARTS AGEcc-api-548bb458-78p77 2/2 Running 0 14mcc-api-548bb458-nkjmj 0/2 Running 2 14mcc-api-548bb458-sgfrb 0/2 Running 2 14m123456❯ kubectl get pods -n clientsNAME READY STATUS RESTARTS AGEclients-api-5c8d89b4d-8nvsd 2/2 Running 0 14mclients-api-5c8d89b4d-wd8rq 0/2 Running 4 14mclients-api-5c8d89b4d-xm4ln 0/2 Running 4 14mVamos analisar os resultados: Neste teste, 90% de todos os pods do nosso fluxo de trabalho pararam de responder aos healthchecks repentinamente por 30 segundos durante o nosso teste de 60 segundos. Mesmo com nosso cenário de retries entre os VirtualServices, ainda registramos 1. 22% de erros retornados ao cliente. Conseguimos reduzir em quase 5% os erros, mas ainda assim não atendemos ao nosso SLO de 99,99%.  Sumário do Teste 2. 1 — Injetando falhas de Healthcheck nas aplicações::  Tempo do teste: 60s Total de requisições: 14340 Requests por segundo: 233. 75/s Taxa de erros a partir do client: 1. 22% Taxa real de sucesso do serviço principal orders-api: 93. 97% Taxa de sucesso dos consumidores do orders-api: 98. 69% SLO Cumprido: Não Cenário 2. 2 — Adicionando retry por conexões perdidas / abortadas: No teste anterior, mesmo com a perda repentina de 90% dos recursos computacionais e a implementação de políticas de retentativas, não alcançamos o SLO de disponibilidade desejado. Portanto, implementamos políticas adicionais de retentativas para casos de falhas em cascata, incluindo os códigos 5xx, gateway-error, connect-failure, refused-stream, reset, unavailable, cancelled no nosso retryOn. O objetivo é mitigar erros causados por perda de conexões em quedas bruscas de pods. 400: Invalid requestAs opções de retentativas são, de acordo com a documentação para HTTP:  reset : Será feita uma tentativa de retry em caso de disconnect/reset/read timeout vindo do upstreamCaso você esteja utilizando algum backend gRPC, tomei a liberdade de adicionar as outras opções no exemplo, caso seu backend seja exclusivamente HTTP, as mesmas não serão necessárias, mas fica como estudo:  resource-exhausted: retry em chamadas gRPC em caso de headers contendo o termo “resource-exhausted” unavailable: retry em chamadas gRPC em caso de headers contendo o termo “unavailable” cancelled: retry em chamadas gRPC em caso de headers contendo o termo “cancelled”Executar novamente os testes para avaliar o quanto de melhoria temos colocando o retry por disconnect/reset/timeout adicionais Neste cenário, com as novas políticas de retentativa, tivemos um saving significativo de disponibilidade, com apenas 0. 03% de erros contabilizados no cliente, correspondendo a 5 erros em 15633 requisições. Caso você esteja utilizando algum backend gRPC, tomei a liberdade de adicionar as outras opções no exemplo, caso seu backend seja exclusivamente HTTP, as mesmas não serão necessárias, mas fica como estudo: Sumário do teste 2. 2 — Adicionando Retry por Conexões Abortadas::  Tempo do teste: 60s Total de requisições: 15633 Requests por segundo: 258. 4/s Taxa de erros a partir do client: 0. 03% Taxa real de sucesso do serviço principal orders-api: 92. 81% Taxa de sucesso dos consumidores do orders-api: 99. 99% SLO Cumprido: Não Yaml dos testes executados no cenário Yaml dos testes do Chaos Mesh Cenário 2. 3 — Adicionando Circuit Breakers nos upstreams: Para a cereja do bolo pro assunto de resiliência em service-mesh, nesse caso o istio, são os circuit breakers. É um conceito muito legal que não é tão fácil de compreender como os retry. Circuit breakers nos ajudam a sinalizar pros clientes que um determinado serviço está fora, poupando esforço para consumi-lo e junto com as retentativas estar sempre validando se os mesmos estão de volta “a ativa” ou não. Isso vai nos ajudar a “não tentar” mais requisições nos hosts que atenderem aos requisitos de circuito quebrado. Além de poder limitar a quantidade de requisições ativas que nosso backend consegue atender, para evitar uma degradação maior ou gerar uma falha não prevista. Para isso vamos adicionar um recurso chamado DestinationRule em todas as aplicações da malha de serviço. 400: Invalid requestAs coisas mais importantes desse novo objeto são consecutive5xxErrors e baseEjectionTime. Os consecutive5xxErrors é o numero de erros que um upstream pode retornar para que o mesmo seja considerado com o circuito aberto. Já o baseEjectionTime é o tempo que o host ficará com o circuito aberto antes de retornar para a lista de upstreams. Peguei essas duas imagens deste post excelente a respeito de circuit-breaking do Istio mais conceitual, e de como ele funciona em conjunto com os retries que já implementamos.  A partir do momento que o baseline de erros de um upstream ativa a quebra de circuito, o upstream fica inativa na lista pelo período recomendado pelo Pool Ejection, e com as considerações de retry, podemos iterar na lista até encontrar um host saudável para aquela requisição em específico. Seguindo essa lógica, vamos aos testes: Neste teste finalmente conseguimos atingir os 100% de disponibilidade com falha temporária e repentina de 90% do healthcheck das aplicações da malha. No Kiali, podemos ver que o circuit breaker foi implementado em todas as pontas do workflow.  Sumário do teste 2. 3 — Adicionando Circuit Breaker com os&nbsp;Retry::  Tempo do teste: 60s Total de requisições: 20557 Requests por segundo: 342/s Taxa de erros a partir do client: 0. 00% Taxa real de sucesso do serviço principal orders-api: 100 % Taxa de sucesso dos consumidores do orders-api: 100 % SLO Cumprido: SIM Yaml dos testes executados no&nbsp;cenário Yaml dos testes do Chaos&nbsp;Mesh Cenário 2. 3 — Morte instantânea de 90% dos&nbsp;pods: Vamos avaliar um outro cenário, parecido mas não igual. No cenário anterior validamos a action pod-failure, que injeta uma falha de healthcheck nos pods mas não os mata definitivamente. Nesta vamos executar a action pod-kill, onde 90% dos pods vão sofrer um force terminate. Vamos iniciar o teste de carga e no meio dela vamos injetar a falha no workload. Link do meio 400: Invalid request123456kubectl apply -f chaos-mesh/02-pod-killpodchaos. chaos-mesh. org/cc-pod-kill createdpodchaos. chaos-mesh. org/clients-pod-kill createdpodchaos. chaos-mesh. org/orders-pod-kill createdpodchaos. chaos-mesh. org/payment-pod-kill created123456789❯ kubectl get pods -n paymentNAME READY STATUS RESTARTS AGEpayment-api-645c7958cd-c25nf 2/2 Running 0 2m40spayment-api-645c7958cd-clbpg 1/2 Running 0 6spayment-api-645c7958cd-h6fgh 0/2 Running 0 6spayment-api-645c7958cd-lt5bp 0/2 PodInitializing 0 6spayment-api-645c7958cd-s2gzx 0/2 Running 0 6spayment-api-645c7958cd-v6c8w 0/2 Running 0 6s123456❯ kubectl get pods -n ordersNAME READY STATUS RESTARTS AGEorders-api-86b4c65f9b-6wdg5 1/2 Running 0 18sorders-api-86b4c65f9b-pvqv4 1/2 Running 0 18sorders-api-86b4c65f9b-wbkt2 2/2 Running 0 4m13s123456❯ kubectl get pods -n ccNAME READY STATUS RESTARTS AGEcc-api-58b558fc8f-6dqlh 2/2 Running 0 15mcc-api-58b558fc8f-7zz8t 1/2 Running 0 30scc-api-58b558fc8f-wnjcs 1/2 Running 0 30s12345678❯ kubectl get pods -n clientsNAME READY STATUS RESTARTS AGEclients-api-59b5cf8bc-46cws 1/2 Running 0 47sclients-api-59b5cf8bc-4rkvp 1/2 Running 0 47sclients-api-59b5cf8bc-hdngf 1/2 Running 0 47sclients-api-59b5cf8bc-txcb4 2/2 Running 0 16mclients-api-59b5cf8bc-vb8lh 1/2 Running 0 47s Desta vez passamos de primeira no teste de uma queda brusca de pods com carga quente. Os retries com circuit breaker dos pods agiram muito rápido evitando uma quantidade significativa de retries, melhorando muito até o response time e tput.  Sumário do teste 2. 3 — Morte repentina dos pods das aplicações::  Tempo do teste: 60s Total de requisições: 24948 Requests por segundo: 415,56/s Taxa de erros a partir do client: 0. 00% Taxa real de sucesso do serviço principal orders-api: 100 % Taxa de sucesso dos consumidores do orders-api: 100 % SLO Cumprido: SIM Yaml dos testes executados no cenário Yaml dos testes do Chaos MeshHipótese validada com sucesso! Hipótese 3: Queda de Uma Zona de Disponibilidade no EKSNeste laboratório estamos rodando o EKS com 3 AZ’s na região de us-east-1, sendo us-east-1a, us-east-1b, us-east-1c rodando com 2 EC2 em cada uma delas.  A ideia é matar todas as instâncias de uma determinada zona de disponibilidade especifica para validar se a quantidade de retries e circuit breaker que implementamos até o momento são capazes de suprir esse cenário em disponibilidade. Logo a hipótese é:  Os recursos computacionais de uma zona de disponibilidade especifica podem cair a qualquer momento que estarei resiliente para meus clientes finais Cenário 3. 1 — Morte de uma zona de disponibilidade da AWS: Antes de mais nada, vamos utilizar o recurso do PodAffinity / PodAntiAffinity para criar uma sugestão de regra para o scheduler: “divida-se igualmente entre os hosts utilizando a referência à label failure-domain. beta. kubernetes. io/zone”, na qual é preenchida nos nodes do EKS com a zona de disponibilidade que aquele node está rodando, o que irá acarretar em garantir um Multi-AZ do workload. 123456789101112131415161718192021❯ kubectl describe node ip-10-0-89-102. ec2. internalName: ip-10-0-89-102. ec2. internalRoles: &lt;none&gt;Labels: beta. kubernetes. io/arch=amd64beta. kubernetes. io/instance-type=t3. largebeta. kubernetes. io/os=linuxeks. amazonaws. com/capacityType=ON_DEMANDeks. amazonaws. com/nodegroup=eks-cluster-node-groupeks. amazonaws. com/nodegroup-image=ami-0ee7f482baec5230ffailure-domain. beta. kubernetes. io/region=us-east-1failure-domain. beta. kubernetes. io/zone=us-east-1cingress/ready=truekubernetes. io/arch=amd64kubernetes. io/hostname=ip-10-0-89-102. ec2. internalkubernetes. io/os=linuxnode. kubernetes. io/instance-type=t3. largetopology. kubernetes. io/region=us-east-1topology. kubernetes. io/zone=us-east-1c &lt;----------- AQUIAnnotations: node. alpha. kubernetes. io/ttl: 0volumes. kubernetes. io/controller-managed-attach-detach: trueEntão, em todos os nossos arquivos de deployment vamos adicionar as notações de&nbsp;affinity 400: Invalid requestVamos olhar os pods de uma aplicação especifica para encontrar os IPs que eles assumiram na VPC para identificar a distribuição via&nbsp;console. 123456❯ kubectl get pods -n orders -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESorders-api-56f8bf5b7c-7wbz4 2/2 Running 0 2m10s 10. 0. 54. 38 ip-10-0-58-137. ec2. internal &lt;none&gt; &lt;none&gt;&lt;/none&gt;orders-api-56f8bf5b7c-pcqtd 2/2 Running 0 2m10s 10. 0. 89. 56 ip-10-0-81-235. ec2. internal &lt;none&gt; &lt;none&gt;&lt;/none&gt;orders-api-56f8bf5b7c-zlwgd 2/2 Running 0 2m10s 10. 0. 78. 253 ip-10-0-76-14. ec2. internal &lt;none&gt; &lt;none&gt;&lt;/none&gt;Levando os IP’s dos pods para o painel, podemos ver se a sugestão está funcionando entre as 3 zonas de disponibilidade.  Este teste não será tão inteligente. Vou selecionar todos os nodes da zona us-east-1a e dar um halt via SSM enquanto nosso teste roda.  Vamos aos resultados do teste  Sumário dos testes 3. 1 — Perda de uma AZ::  Tempo do teste: 60s Total de requisições: 23136 Requests por segundo: 385. 11/s Taxa de erros a partir do client: 0. 00% Taxa real de sucesso do serviço principal orders-api: 100 % Taxa de sucesso dos consumidores do orders-api: 100 % SLO Cumprido: SIM Yaml dos testes executados no cenárioHipótese validada com sucesso! Considerações finais, e importantes: Mecanismo de resiliência é igual itaipava que seu tio trouxe em dia de churrasco: Todo mundo fica bravo de ter que dar espaço na geladeira, no fim todo mundo vai acabar bebendo e sempre vai faltar.  A resiliência a nível de plataforma é uma parte da composição da resiliência de uma aplicação, não a solução completa pra ela.  O fluxo de retry deve ser implementado somente se as aplicações atrás delas tiverem mecanismos de idempotência para evitar duplicidades de registros, principalmente, falando em HTTP, de requests que são naturalmente não idempotentes como POST por exemplo.  Os retry e circuit breaker dos meshs em geral não devem ser tratados como mecanismo de resiliência principal da solução. Não substitui a resiliência pragmática.  Não substitui a resiliência a nível de código / aplicação. Repetindo algumas vezes pra fixar.  Os circuit breakers e retentivas devem ser implementados a nível de código independente da plataforma suportar isso. Procure por soluções como Resilience4J, Hystrix, GoBreaker. Só pra constar.  A busca por circuit breakers pragmáticos tende a prioridade em caso de downtime total de uma dependência, principalmente para buscar fluxos alternativos como fallback, não apenas para serem usados para “dar erro mais rápido”. Pense em “posso ter um SQS como fallback para fazer temporariamente offload para os eventos que eu iria produzir no meu kafka que está com falha?”, “tenho um sistema de apoio para enfileirar as requisições que estão dando falha para processamento tardio”?, “eu posso reprocessar tudo que falhou quando minhas dependências voltarem?” antes de qualquer coisa, beleza?Fico por aqui, espero ter ajudado! Lembrando que todos os arquivos e aplicações estão neste repositório do Github. Referencias / Material de Apoio:  Istio Traffic Management (https://istio. io/latest/docs/concepts/traffic-management/) Istio Traffic Management — Circuit Breaker (https://istio. io/latest/docs/tasks/traffic-management/circuit-breaking/) Envoy — Retry Policy — https://www. envoyproxy. io/docs/envoy/latest/configuration/http/http_filters/router_filter#x-envoy-retry-on Chaos Mesh — Quickstart https://chaos-mesh. org/docs/quick-start/ K6 Loading Testing — https://k6. io/docs/getting-started/running-k6/ Setup do EKS com Istio e Terraform — https://github. com/msfidelis/eks-with-istioObrigado aos revisores:  Rebeca Maia (@rebecamaia_p) Bruno Padilha (@brunopadz) Leandro Grillo (@leandrocgrillo)Me sigam no Twitter para acompanhar as paradinhas que eu compartilho por lá! Te ajudei de alguma forma? Me pague um café Chave Pix: fe60fe92-ecba-4165-be5a-3dccf8a06bfc "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});