<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo.png">

<title>Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS | Matheus Fidelis</title>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS | Matheus Fidelis Blog</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS" />
<meta name="author" content="matheus" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Na sua casa você pode usar o que você quiser, aqui hoje vamos usar Istio. Sem tempo pra chorar irmão…" />
<meta property="og:description" content="Na sua casa você pode usar o que você quiser, aqui hoje vamos usar Istio. Sem tempo pra chorar irmão…" />
<meta property="og:site_name" content="Matheus Fidelis Blog" />
<meta property="og:image" content="/assets/images/istio-logo-vg.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-13T00:00:00-03:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="/assets/images/istio-logo-vg.png" />
<meta property="twitter:title" content="Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"matheus"},"dateModified":"2021-11-13T00:00:00-03:00","datePublished":"2021-11-13T00:00:00-03:00","description":"Na sua casa você pode usar o que você quiser, aqui hoje vamos usar Istio. Sem tempo pra chorar irmão…","headline":"Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS","image":"/assets/images/istio-logo-vg.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://medium.com/@fidelissauro/sobrevivendo-a-cen%C3%A1rios-de-caos-no-kubernetes-com-istio-e-amazon-eks-4fb8469a73da?source=rss-fc2fda5e9bc2------2"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"},"name":"matheus"},"url":"https://medium.com/@fidelissauro/sobrevivendo-a-cen%C3%A1rios-de-caos-no-kubernetes-com-istio-e-amazon-eks-4fb8469a73da?source=rss-fc2fda5e9bc2------2"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

</head>




<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="/">
    <img src="/assets/images/logo.png" alt="Matheus Fidelis">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="/index.html">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="/about">Sobre</a>
                </li>

                <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">Matheus Fidelis</h1>
    <p class="lead">
        Technical Blog
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS&url=/sobrevivendo-a-cenarios-de-caos-no-kubernetes-com-istio-e-amazon-eks/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=/sobrevivendo-a-cenarios-de-caos-no-kubernetes-com-istio-e-amazon-eks/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=/sobrevivendo-a-cenarios-de-caos-no-kubernetes-com-istio-e-amazon-eks/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="https://www.gravatar.com/avatar/5700ede08434f0ab49b75fe5543f129b?s=250&d=mm&r=x" alt="Matheus Fidelis">
                        
                    </div>
                    <div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left">
                        <a target="_blank" class="link-dark" href="https://msfidelis.github.io/">Matheus Fidelis</a><a target="_blank" href="https://twitter.com/fidelissauro" class="btn follow">Follow</a>
                        <span class="author-description">Staff Engineer, Lifelong Learner, Site Reliability Engineer, Cloud and Containers Wizard, Software Alchemist and Home Bartender</span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid lazyimg" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="/assets/images/istio-logo-vg.png" alt="Sobrevivendo a cenários de caos no Kubernetes com Istio e Amazon EKS">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                <!-- End Toc -->
                <blockquote>Na sua casa você pode usar o que você quiser, aqui hoje vamos usar Istio. Sem tempo pra chorar&nbsp;irmão…
</blockquote>

<h1 id="introdução">Introdução</h1>

<p>O objetivo desse post é apresentar alguns mecanismos de resiliência que podemos agregar em nosso Workflow para sobreviver em (alguns) cenários de caos e desastres utilizando recursos do <strong>Istio</strong> e no <strong>Amazon EKS</strong>.</p>

<p>Obviamente isso não é “plug n’ play” pra qualquer cenário, então espero que você absorva os conceitos e as ferramentas e consiga adaptar para o seu próprio caso.</p>

<p>A ideia é estabelecer uma linha de pensamento progressiva apresentando cenários de desastre de aplicações, dependências e infraestrutura e como corrigi-los utilizando as ferramentas apresentadas.</p>

<p>Para este post foi criado um laboratório simulando um fluxo síncrono onde temos <strong>4 microserviços</strong> que se comunicam entre si, simulando um sistema distribuído de pedidos, onde todos são estimulados por requisições HTTP.</p>

<p><br></p>

<h3 id="premissas-iniciais">Premissas Iniciais:</h3>

<ul>
  <li>O ambiente roda em um EKS utilizando a versão 1.20 do Kubernetes em 3 zonas de disponibilidade (us-east-1a, us-east-1b e us-east-1c)</li>
  <li>Vamos trabalhar com um SLO de 99,99% de disponibilidade para o cliente final</li>
  <li>O ambiente já possui <strong>Istio</strong> default com <strong>Gateways</strong> e <strong>VirtualServices</strong> Vanilla configurados pra todos os serviços</li>
  <li>O objetivo é aumentar a resiliência diretamente no Istio, por isso nenhuma aplicação tem fluxo de circuit breaker ou retry pragmaticamente implementados.</li>
  <li>Vamos utilizar o <a href="https://kiali.io"><strong>Kiali</strong></a>, <a href="https://grafana.com"><strong>Grafana</strong></a> para visualizar as métricas</li>
  <li>Vamos utilizar o <a href="https://k6.io/docs/getting-started/running-k6/"><strong>K6</strong></a> para injetar carga no workload</li>
  <li>Vamos utilizar o <a href="https://chaos-mesh.org"><strong>Chaos Mesh</strong></a> para injetar falhas de plataforma do Kubernetes</li>
  <li>Vamos utilizar o <a href="https://github.com/msfidelis/gin-chaos-monkey"><strong>gin-chaos-monkey</strong></a> para injetar falhas a nível de aplicação diretamente no runtime</li>
  <li>O objetivo não é avaliar arquitetura de solução, e sim focar nas ferramentas apresentadas para aumentar resiliência.</li>
</ul>

<p><br></p>

<h2 id="topologia-do-sistema">Topologia do Sistema</h2>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*06zbZCi-J8bjNklg.png" alt="Topologia do Sistema"></p>

<p>Esta é a representação do fluxo de comunicação entre as aplicações do teste. Todas são mocks mas executam chamadas entre si simulando clientes e servers reais de domínio.</p>

<p><br></p>

<h1 id="hipótese-1-resiliência-em-falhas-de-aplicação">Hipótese 1: Resiliência em falhas de aplicação</h1>

<p>O objetivo é coletar as métricas de disponibilidade do fluxo síncrono com <strong>qualquer componente podendo falhar a qualquer momento.</strong></p>

<p>Primeiro teste tem o objetivo de injetar uma carga de <strong>60s</strong>, simulando <strong>20 VUS (Virtual Users)</strong> em todos os cenários, para ver como esses erros se comportam em cascata até chegar no cliente final à medida que vamos criando mecanismos de resiliência.</p>

<p>Todas as aplicações implementando um middleware de chaos que injeta falhas durante a requisição HTTP, portanto em qualquer momento, qualquer uma delas poderá sofrer um:</p>

<ul>
  <li><strong>Memory Assault</strong> — gerando memory leaks constantes</li>
  <li><strong>CPU Assault</strong> — injetando overhead de recursos</li>
  <li><strong>Latency Assault</strong> — incrementando o response time entre 1000ms e 5000ms</li>
  <li><strong>Exception Assault</strong> — devolvendo aleatoriamente um status de 503 com falha na requisição</li>
  <li><strong>AppKiller Assault</strong> — fazendo o runtime entrar em panic()</li>
</ul>

<p>Logo a hipótese a ser testada é:</p>

<blockquote>
  <p>“Minhas aplicações podem gerar erros sistêmicos aleatoriamente que mesmo assim estarei resiliente para meu cliente final”</p>
</blockquote>

<p><br></p>

<h2 id="cenários-11cenário-inicial">Cenários 1.1 — Cenário Inicial</h2>

<p>Vamos rodar o teste de carga do k6 no ambiente para ver como vamos nos sair sem nenhum tipo de mecanismo de resiliência:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>k6 run <span class="nt">--vus</span> 20 <span class="nt">--duration</span> 60s k6/loadtest.js
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*8dnV7-W_UiZX3ulF.png" alt="Resultado do Teste de Carga"></p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*6mxr7uhG3y_8O8Qf.png" alt="Chaos Monkey Injections"></p>

<p>Podemos ver que o chaos monkey da aplicação cumpriu seu papel, injetando falhas aleatórias em todas as dependências da malha de serviços, ofendendo drasticamente nosso <strong>SLO</strong> de disponibilidade para <strong>88.10%</strong>, estourando nosso <strong>Error Budget</strong> para este período do teste.</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*lM7tKCboFv60UIRH.png" alt="SLO Offense"></p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*gbjiWkG73kofNmBD.png" alt="Service Mesh Failures"></p>

<p>Podemos ver também que todas as aplicações da malha, em algum momento apresentaram falhas aleatórias, gerando erros em cascata.</p>

<p><br></p>

<h3 id="sumário-do-teste-11cenário-inicial">Sumário do teste 1.1 — Cenário inicial:</h3>

<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 13905</li>
  <li><strong>Requests por segundo:</strong> 228.35/s</li>
  <li><strong>Taxa de erros a partir do client:</strong> 11.91%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 88.10%</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 88.10%</li>
  <li><strong>SLO Cumprido:</strong> Não</li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/istio/00-raw-workload"><strong>Yaml dos testes executados no cenário</strong></a></li>
</ul>

<p><br></p>

<h2 id="cenário-12implementando-retry-para-falhas-http">Cenário 1.2 — Implementando retry para Falhas HTTP</h2>

<p>O objetivo do cenário é implementar a política de retentativas para falhas HTTP nos VirtualServices previamente configurados das aplicações. Vamos as opções <strong>5xx, gateway-error, connect-failure</strong>. Podendo ocorrer até <strong>3 tentativas</strong> de retry com um <strong>timeout de 500ms</strong>.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/e8f2ebc3bbfc50f1b1e09bf25e3dc77b.js"> </script>

<p>As opções de retentativas iniciais de acordo com a documentação, possuem as seguintes funções:</p>

<ul>
  <li><strong>5xx:</strong> Ocorrerá uma nova tentativa se o servidor upstream responder com qualquer código de resposta 5xx</li>
  <li><strong>gateway-error:</strong> Uma política parecida com o 5xx, porém voltadas a falhas específicas de gateway como 502, 503, ou 504 no geral. Nesse caso, é redundante, porém fica de exemplo.</li>
  <li><strong>connect-failure:</strong> Será realizada mais uma tentativa em caso de falha de conexão por parte do upstream ou em casos de timeout.</li>
</ul>

<p>Vamos rodar novamente os testes simulando 20 usuários por 60 segundos com os 3 retries aplicados.</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*XL9nRme1a6MaAohe.png" alt="Teste com Retries"></p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*CJS0_b-aCCP0T36S.png" alt="Melhoria na Disponibilidade"></p>

<p>Conseguimos uma melhoria de mais de <strong>6% de disponibilidade</strong> entre o que o serviço degradado respondeu com o que o cliente recebeu, utilizando apenas 3 tentativas de retry entre todos os serviços. Tivemos já um grande saving de disponibilidade para o cliente final, porém ainda ofendemos nosso SLO, batendo <strong>99,25%</strong> de disponibilidade.</p>

<p><br></p>

<h3 id="sumário-do-teste-12--retry-para-falhas-http">Sumário do teste 1.2 — Retry para falhas HTTP:</h3>

<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 17873</li>
  <li><strong>Requests por segundo:</strong> 293.17/s</li>
  <li><strong>Taxa de erros a partir do cliente:</strong> 0.07%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 93.08%</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 99.25%</li>
  <li><strong>SLO Cumprido:</strong> Não</li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/istio/01-retry"><strong>Yaml dos testes executados no cenário</strong></a></li>
</ul>

<p><br></p>

<h2 id="cenário-13--ajustando-a-quantidade-de-retries-para-suprir-o-cenário">Cenário 1.3 — Ajustando a quantidade de retries para suprir o cenário</h2>

<p>Para resolver o problema, aumentei o número de retries de 3 para 5 e repeti os testes nos mesmos cenários. Em ambientes reais, esse tipo de ajuste pode ser derivado de um número “mágico”, encontrado após várias repetições do mesmo teste. Executei mais algumas vezes até atingir 0% de erros consistentemente.</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*Ai4Dzv9e-73koWDz.png" alt="">
<img src="https://cdn-images-1.medium.com/max/1024/0*M05f0F2d-k8u-9DS.png" alt=""></p>

<p>Neste cenário, conseguimos superar os 93.09% de disponibilidade fornecidos pelos upstreams, garantindo 100% de disponibilidade para o cliente com os cenários de retry. Mesmo com falhas aleatórias sendo injetadas, a disponibilidade final para o cliente foi assegurada.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/071e05ad28c5988abb03cec56abd8470.js"> </script>

<p><br></p>

<h3 id="sumário-do-teste-13--ajustes-na-quantidade-de-retries-para-a-hipótese">Sumário do teste 1.3 — Ajustes na quantidade de retries para a hipótese:</h3>

<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 18646</li>
  <li><strong>Requests por segundo:</strong> 308.79/s</li>
  <li><strong>Taxa de erros a partir do cliente:</strong> 0.00%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 93.09%</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 100.00%</li>
  <li><strong>SLO Cumprido:</strong> Sim</li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/istio/02-retry-2"><strong>Yaml dos testes executados no cenário</strong></a></li>
</ul>

<p>Hipótese concluída com sucesso!</p>

<p><br></p>

<h1 id="hipótese-2-resiliência-em-falhas-dos-pods">Hipótese 2: Resiliência em falhas dos Pods</h1>

<p>Para realizar os testes de infraestrutura nos pods, utilizaremos o <strong>Chaos Mesh</strong> como ferramenta para injetar falhas nos componentes do nosso fluxo, e <strong>desligaremos o chaos-monkey</strong> no runtime das aplicações. <strong><em>A partir deste ponto, não injetaremos mais falhas intencionais a partir da aplicação</em></strong> para testar puramente falhas a nível de plataforma. O objetivo é analisar como o nosso fluxo síncrono se comporta diante da perda brusca de unidades computacionais em diversos cenários, e como nosso processo de melhoria contínua com retries pode nos ajudar e agregar ainda mais valor como plataforma.</p>

<p>Portanto, a hipótese é:</p>

<blockquote>
  <p>“Posso perder pods e unidades computacionais de várias maneiras, mas minha aplicação continuará resiliente para o cliente final.”</p>
</blockquote>

<p><br></p>

<h2 id="cenário-21--injetando-falhas-de-healthcheck-nos-componentes-do-workload">Cenário 2.1 — Injetando falhas de healthcheck nos componentes do workload</h2>

<p>Neste primeiro cenário, vamos injetar o mesmo volume de requisições e, durante o processo, aplicar o cenário de pod-failure em nosso fluxo. Em todas as aplicações, realizaremos um teste de 30 segundos no qual perderemos <strong>90% dos nossos pods repentinamente devido a falhas de healthcheck</strong>. Este é um teste bastante agressivo, com o propósito de verificar como as estratégias que implementamos até o momento agregam valor nesse cenário.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/df63cc4b8c45e1c0e0648401e9f1b046.js"> </script>

<p>Vamos iniciar o teste nos mesmos cenários e, no meio do processo, aplicar os cenários de <strong>PodFailure</strong> do <strong>Chaos Mesh</strong> em todas as aplicações.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>kubectl apply <span class="nt">-f</span> chaos-mesh/01-pod-failture/

podchaos.chaos-mesh.org/cc-pod-failure created
podchaos.chaos-mesh.org/clients-pod-failure created
podchaos.chaos-mesh.org/orders-pod-failure created
podchaos.chaos-mesh.org/payment-pod-failure created
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Vamos conferir o status dos&nbsp;pods:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> orders

NAME READY STATUS RESTARTS AGE
orders-api-fb5c94987-225zp 0/2 Running 2 100s
orders-api-fb5c94987-8rpjb 0/2 Running 2 14m
orders-api-fb5c94987-bmnqm 0/2 Running 2 85s
orders-api-fb5c94987-d9c4f 0/2 Running 2 14m
orders-api-fb5c94987-gd745 0/2 Running 2 14m
orders-api-fb5c94987-htbcn 2/2 Running 0 100s
orders-api-fb5c94987-rzqkg 0/2 Running 2 100s
orders-api-fb5c94987-st8l2 0/2 Running 2 85s
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> cc

NAME READY STATUS RESTARTS AGE
cc-api-548bb458-78p77 2/2 Running 0 14m
cc-api-548bb458-nkjmj 0/2 Running 2 14m
cc-api-548bb458-sgfrb 0/2 Running 2 14m
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> clients

NAME READY STATUS RESTARTS AGE
clients-api-5c8d89b4d-8nvsd 2/2 Running 0 14m
clients-api-5c8d89b4d-wd8rq 0/2 Running 4 14m
clients-api-5c8d89b4d-xm4ln 0/2 Running 4 14m
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Vamos analisar os resultados:</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*BWX0nJk52Yg2OX9S.png" alt="Imagem representativa do teste">
<img src="https://cdn-images-1.medium.com/max/1024/0*DbVYcS7BHsiMcZSS.png" alt="Imagem representativa do teste"></p>

<p>Neste teste, 90% de todos os pods do nosso fluxo de trabalho pararam de responder aos healthchecks repentinamente por 30 segundos durante o nosso teste de 60 segundos. Mesmo com nosso cenário de retries entre os VirtualServices, ainda registramos <strong>1.22%</strong> de erros retornados ao cliente. Conseguimos reduzir em quase <strong>5% os erros</strong>, mas ainda assim não atendemos ao nosso SLO de 99,99%.</p>

<p><br></p>

<h3 id="sumário-do-teste-21--injetando-falhas-de-healthcheck-nas-aplicações">Sumário do Teste 2.1 — Injetando falhas de Healthcheck nas aplicações:</h3>

<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 14340</li>
  <li><strong>Requests por segundo:</strong> 233.75/s</li>
  <li><strong>Taxa de erros a partir do client:</strong> 1.22%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 93.97%</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 98.69%</li>
  <li><strong>SLO Cumprido:</strong> Não</li>
</ul>

<p><br></p>

<h2 id="cenário-22adicionando-retry-por-conexões-perdidas--abortadas">Cenário 2.2 — Adicionando retry por conexões perdidas / abortadas</h2>

<p>No teste anterior, mesmo com a perda repentina de 90% dos recursos computacionais e a implementação de políticas de retentativas, não alcançamos o SLO de disponibilidade desejado. Portanto, implementamos políticas adicionais de retentativas para casos de falhas em cascata, incluindo os códigos <strong>5xx, gateway-error, connect-failure, refused-stream, reset, unavailable, cancelled</strong> no nosso <strong>retryOn</strong>. O objetivo é mitigar erros causados por perda de conexões em quedas bruscas de pods.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/1d8e6d42b6d9d6469a12f4fa57608069.js"> </script>

<p>As opções de retentativas são, de acordo com a documentação para HTTP:</p>

<ul>
  <li><strong>reset</strong> : Será feita uma tentativa de retry em caso de disconnect/reset/read timeout vindo do upstream</li>
</ul>

<p>Caso você esteja utilizando algum backend <strong>gRPC</strong>, tomei a liberdade de adicionar as outras opções no exemplo, caso seu backend seja exclusivamente HTTP, as mesmas não serão necessárias, mas fica como estudo:</p>

<ul>
  <li><strong>resource-exhausted</strong>: retry em chamadas gRPC em caso de headers contendo o termo “resource-exhausted”</li>
  <li><strong>unavailable</strong>: retry em chamadas gRPC em caso de headers contendo o termo “unavailable”</li>
  <li><strong>cancelled</strong>: retry em chamadas gRPC em caso de headers contendo o termo “cancelled”</li>
</ul>

<p>Executar novamente os testes para avaliar o quanto de melhoria temos colocando o retry por disconnect/reset/timeout adicionais</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*MB3fsIzA5QlEgA2s.png" alt="Imagem representativa do teste">
<img src="https://cdn-images-1.medium.com/max/1024/0*FgBEn7ANDQEugq-j.png" alt="Imagem representativa do teste"></p>

<p>Neste cenário, com as novas políticas de retentativa, tivemos um saving significativo de disponibilidade, com apenas <strong>0.03%</strong> de erros contabilizados no cliente, correspondendo a 5 erros em 15633 requisições.</p>

<p>Caso você esteja utilizando algum backend <strong>gRPC</strong>, tomei a liberdade de adicionar as outras opções no exemplo, caso seu backend seja exclusivamente HTTP, as mesmas não serão necessárias, mas fica como estudo:</p>

<p><br></p>

<h3 id="sumário-do-teste-22adicionando-retry-por-conexões-abortadas">Sumário do teste 2.2 — Adicionando Retry por Conexões Abortadas:</h3>

<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 15633</li>
  <li><strong>Requests por segundo:</strong> 258.4/s</li>
  <li><strong>Taxa de erros a partir do client:</strong> 0.03%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 92.81%</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 99.99%</li>
  <li><strong>SLO Cumprido:</strong> Não</li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/istio/03-retry-connection"><strong>Yaml dos testes executados no cenário</strong></a></li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/chaos-mesh/01-pod-failture"><strong>Yaml dos testes do Chaos Mesh</strong></a></li>
</ul>

<p><br></p>

<h2 id="cenário-23adicionando-circuit-breakers-nos-upstreams">Cenário 2.3 — Adicionando Circuit Breakers nos upstreams</h2>

<p>Para a cereja do bolo pro assunto de resiliência em service-mesh, nesse caso o istio, são os circuit breakers. É um conceito muito legal que não é tão fácil de compreender como os retry. Circuit breakers nos ajudam a sinalizar pros clientes que um determinado serviço está fora, poupando esforço para consumi-lo e junto com as retentativas estar sempre validando se os mesmos estão de volta “a ativa” ou não. Isso vai nos ajudar a “não tentar” mais requisições nos hosts que atenderem aos requisitos de circuito quebrado. Além de poder limitar a quantidade de requisições ativas que nosso backend consegue atender, para evitar uma degradação maior ou gerar uma falha não prevista. Para isso vamos adicionar um recurso chamado <strong>DestinationRule</strong> em todas as aplicações da malha de serviço.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/612ef3a52e20099a2a85013236e17c23.js"> </script>

<p>As coisas mais importantes desse novo objeto são <strong>consecutive5xxErrors</strong> e <strong>baseEjectionTime</strong>.</p>

<p>Os <strong>consecutive5xxErrors</strong> é o numero de erros que um upstream pode retornar para que o mesmo seja considerado com o circuito aberto.</p>

<p>Já o <strong>baseEjectionTime</strong> é o tempo que o host ficará com o circuito aberto antes de retornar para a lista de upstreams.</p>

<p>Peguei essas duas imagens deste <a href="https://sfeir.github.io/kubernetes-istio-workshop/kubernetes-istio-workshop/1.0.0/istio/07_circuit-breaker.html">post excelente a respeito de circuit-breaking do Istio</a> mais conceitual, e de como ele funciona em conjunto com os retries que já implementamos.</p>

<p><img src="https://cdn-images-1.medium.com/max/650/0*oXPDpAO-L0mnkv5f.png" alt="Circuit breaker conceptual image">
<img src="https://cdn-images-1.medium.com/max/651/0*foWKYRRU_5GlP-GY.png" alt="Circuit breaker flow image"></p>

<p>A partir do momento que o baseline de erros de um upstream ativa a quebra de circuito, o upstream fica inativa na lista pelo período recomendado pelo <strong>Pool Ejection</strong>, e com as considerações de retry, podemos iterar na lista até encontrar um host saudável para aquela requisição em específico.</p>

<p>Seguindo essa lógica, vamos aos testes:</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*8mj6t2h8l2mtY_3S.png" alt="Test results">
<img src="https://cdn-images-1.medium.com/max/1024/0*UGSKcM34HWbjm5J2.png" alt="Test results 2"></p>

<p>Neste teste finalmente conseguimos atingir os 100% de disponibilidade com falha temporária e repentina de 90% do healthcheck das aplicações da malha. No <strong>Kiali</strong>, podemos ver que o circuit breaker foi implementado em todas as pontas do workflow.</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*RpfIlWVVRzOsDxFb.png" alt="Kiali circuit breaker implementation"></p>

<p><br></p>

<h3 id="sumário-do-teste-23adicionando-circuit-breaker-com-osretry">Sumário do teste 2.3 — Adicionando Circuit Breaker com os&nbsp;Retry:</h3>
<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 20557</li>
  <li><strong>Requests por segundo:</strong> 342/s</li>
  <li><strong>Taxa de erros a partir do client:</strong> 0.00%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 100 %</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 100 %</li>
  <li><strong>SLO Cumprido:</strong> SIM</li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/istio/04-circuit-breaker"><strong>Yaml dos testes executados no&nbsp;cenário</strong></a></li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/chaos-mesh/01-pod-failture"><strong>Yaml dos testes do Chaos&nbsp;Mesh</strong></a></li>
</ul>

<p><br></p>

<h2 id="cenário-23morte-instantânea-de-90-dospods">Cenário 2.3 — Morte instantânea de 90% dos&nbsp;pods</h2>
<p>Vamos avaliar um outro cenário, parecido mas não igual. No cenário anterior validamos a action pod-failure, que injeta uma falha de healthcheck nos pods mas não os mata definitivamente. Nesta vamos executar a action <strong>pod-kill,</strong> onde 90% dos pods vão sofrer um force terminate.</p>

<p>Vamos iniciar o teste de carga e no meio dela vamos injetar a falha no workload. <a href="https://medium.com/media/ce62cf2aa128e0c5c0379a36b0f3031c/href">Link do meio</a></p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/48c09d0e8f577e978676aecdd8865611.js"> </script>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>kubectl apply <span class="nt">-f</span> chaos-mesh/02-pod-kill

podchaos.chaos-mesh.org/cc-pod-kill created
podchaos.chaos-mesh.org/clients-pod-kill created
podchaos.chaos-mesh.org/orders-pod-kill created
podchaos.chaos-mesh.org/payment-pod-kill created
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> payment

NAME READY STATUS RESTARTS AGE
payment-api-645c7958cd-c25nf 2/2 Running 0 2m40s
payment-api-645c7958cd-clbpg 1/2 Running 0 6s
payment-api-645c7958cd-h6fgh 0/2 Running 0 6s
payment-api-645c7958cd-lt5bp 0/2 PodInitializing 0 6s
payment-api-645c7958cd-s2gzx 0/2 Running 0 6s
payment-api-645c7958cd-v6c8w 0/2 Running 0 6s
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> orders

NAME READY STATUS RESTARTS AGE
orders-api-86b4c65f9b-6wdg5 1/2 Running 0 18s
orders-api-86b4c65f9b-pvqv4 1/2 Running 0 18s
orders-api-86b4c65f9b-wbkt2 2/2 Running 0 4m13s
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> cc

NAME READY STATUS RESTARTS AGE
cc-api-58b558fc8f-6dqlh 2/2 Running 0 15m
cc-api-58b558fc8f-7zz8t 1/2 Running 0 30s
cc-api-58b558fc8f-wnjcs 1/2 Running 0 30s
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> clients

NAME READY STATUS RESTARTS AGE
clients-api-59b5cf8bc-46cws 1/2 Running 0 47s
clients-api-59b5cf8bc-4rkvp 1/2 Running 0 47s
clients-api-59b5cf8bc-hdngf 1/2 Running 0 47s
clients-api-59b5cf8bc-txcb4 2/2 Running 0 16m
clients-api-59b5cf8bc-vb8lh 1/2 Running 0 47s
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*-KYAb_g8yLMawC1C.png" alt="imagem">
<img src="https://cdn-images-1.medium.com/max/1024/0*-usGzAssLp06vQj-.png" alt="imagem">
<img src="https://cdn-images-1.medium.com/max/1024/0*nG5I6FCTYos_MMig.png" alt="imagem"></p>

<p>Desta vez passamos de primeira no teste de uma queda brusca de pods com carga quente. Os retries com circuit breaker dos pods agiram muito rápido evitando uma quantidade significativa de retries, melhorando muito até o response time e tput.</p>

<p><br></p>

<h3 id="sumário-do-teste-23morte-repentina-dos-pods-das-aplicações">Sumário do teste 2.3 — Morte repentina dos pods das aplicações:</h3>
<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 24948</li>
  <li><strong>Requests por segundo:</strong> 415,56/s</li>
  <li><strong>Taxa de erros a partir do client:</strong> 0.00%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 100 %</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 100 %</li>
  <li><strong>SLO Cumprido:</strong> SIM</li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/istio/04-circuit-breaker"><strong>Yaml dos testes executados no cenário</strong></a></li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/chaos-mesh/02-pod-kill"><strong>Yaml dos testes do Chaos Mesh</strong></a></li>
</ul>

<p>Hipótese validada com sucesso!</p>

<p><br></p>

<h1 id="hipótese-3-queda-de-uma-zona-de-disponibilidade-no-eks">Hipótese 3: Queda de Uma Zona de Disponibilidade no EKS</h1>

<p>Neste laboratório estamos rodando o EKS com 3 AZ’s na região de <strong>us-east-1,</strong> sendo <strong>us-east-1a</strong>, <strong>us-east-1b</strong>, <strong>us-east-1c</strong> rodando com 2 EC2 em cada uma delas.</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*H1_kA2meMfrpKcOM.png" alt="imagem"></p>

<p>A ideia é matar todas as instâncias de uma determinada zona de disponibilidade especifica para validar se a quantidade de retries e circuit breaker que implementamos até o momento são capazes de suprir esse cenário em disponibilidade.</p>

<p>Logo a hipótese é:</p>

<blockquote>
  <p>Os recursos computacionais de uma zona de disponibilidade especifica podem cair a qualquer momento que estarei resiliente para meus clientes finais</p>
</blockquote>

<p><br></p>

<h2 id="cenário-31morte-de-uma-zona-de-disponibilidade-da-aws">Cenário 3.1 — Morte de uma zona de disponibilidade da AWS</h2>

<p>Antes de mais nada, vamos utilizar o recurso do <strong>PodAffinity</strong> / <strong>PodAntiAffinity</strong> para criar uma sugestão de regra para o scheduler: “<em>divida-se igualmente entre os hosts utilizando a referência à label failure-domain.beta.kubernetes.io/zone</em>”, na qual é preenchida nos nodes do <strong>EKS</strong> com a zona de disponibilidade que aquele node está rodando, o que irá acarretar em garantir um <strong>Multi-AZ</strong> do workload.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre>❯ kubectl describe node ip-10-0-89-102.ec2.internal

Name: ip-10-0-89-102.ec2.internal
Roles: &lt;none&gt;
Labels: beta.kubernetes.io/arch<span class="o">=</span>amd64
beta.kubernetes.io/instance-type<span class="o">=</span>t3.large
beta.kubernetes.io/os<span class="o">=</span>linux
eks.amazonaws.com/capacityType<span class="o">=</span>ON_DEMAND
eks.amazonaws.com/nodegroup<span class="o">=</span>eks-cluster-node-group
eks.amazonaws.com/nodegroup-image<span class="o">=</span>ami-0ee7f482baec5230f
failure-domain.beta.kubernetes.io/region<span class="o">=</span>us-east-1
failure-domain.beta.kubernetes.io/zone<span class="o">=</span>us-east-1c
ingress/ready<span class="o">=</span><span class="nb">true
</span>kubernetes.io/arch<span class="o">=</span>amd64
kubernetes.io/hostname<span class="o">=</span>ip-10-0-89-102.ec2.internal
kubernetes.io/os<span class="o">=</span>linux
node.kubernetes.io/instance-type<span class="o">=</span>t3.large
topology.kubernetes.io/region<span class="o">=</span>us-east-1
topology.kubernetes.io/zone<span class="o">=</span>us-east-1c &lt;<span class="nt">-----------</span> AQUI
Annotations: node.alpha.kubernetes.io/ttl: 0
volumes.kubernetes.io/controller-managed-attach-detach: <span class="nb">true</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Então, em todos os nossos arquivos de deployment vamos adicionar as notações de&nbsp;affinity</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/be9a5c6233be841174e1dcb7ff2c5c50.js"> </script>

<p>Vamos olhar os pods de uma aplicação especifica para encontrar os IPs que eles assumiram na VPC para identificar a distribuição via&nbsp;console.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>❯ kubectl get pods <span class="nt">-n</span> orders <span class="nt">-o</span> wide

NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
orders-api-56f8bf5b7c-7wbz4 2/2 Running 0 2m10s 10.0.54.38 ip-10-0-58-137.ec2.internal &lt;none&gt; &lt;none&gt;&lt;/none&gt;
orders-api-56f8bf5b7c-pcqtd 2/2 Running 0 2m10s 10.0.89.56 ip-10-0-81-235.ec2.internal &lt;none&gt; &lt;none&gt;&lt;/none&gt;
orders-api-56f8bf5b7c-zlwgd 2/2 Running 0 2m10s 10.0.78.253 ip-10-0-76-14.ec2.internal &lt;none&gt; &lt;none&gt;&lt;/none&gt;
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Levando os IP’s dos pods para o painel, podemos ver se a sugestão está funcionando entre as 3 zonas de disponibilidade.</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*11JWmXUiwDPZBnEX.png" alt="imagem"></p>

<p>Este teste não será tão inteligente. Vou selecionar todos os nodes da zona us-east-1a e dar um halt via SSM enquanto nosso teste roda.</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*pZIQqoTzpVeXphAJ.png" alt="imagem"></p>

<p>Vamos aos resultados do teste</p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*Dpuuhfq8qV0ogOcU.png" alt="imagem"></p>

<p><img src="https://cdn-images-1.medium.com/max/1024/0*SBuHF7Twxx7sKE17.png" alt="imagem"></p>

<p><br></p>

<h3 id="sumário-dos-testes-31--perda-de-uma-az">Sumário dos testes 3.1 — Perda de uma AZ:</h3>

<ul>
  <li><strong>Tempo do teste:</strong> 60s</li>
  <li><strong>Total de requisições:</strong> 23136</li>
  <li><strong>Requests por segundo:</strong> 385.11/s</li>
  <li><strong>Taxa de erros a partir do client:</strong> 0.00%</li>
  <li><strong>Taxa real de sucesso do serviço principal orders-api:</strong> 100 %</li>
  <li><strong>Taxa de sucesso dos consumidores do orders-api:</strong> 100 %</li>
  <li><strong>SLO Cumprido:</strong> SIM</li>
  <li><a href="https://github.com/msfidelis/istio-disaster-recovery/tree/main/istio/05-multi-az"><strong>Yaml dos testes executados no cenário</strong></a></li>
</ul>

<p>Hipótese validada com sucesso!</p>

<p><br></p>

<h1 id="considerações-finais-e-importantes">Considerações finais, e importantes:</h1>

<ul>
  <li><em>Mecanismo de resiliência é igual itaipava que seu tio trouxe em dia de churrasco: Todo mundo fica bravo de ter que dar espaço na geladeira, no fim todo mundo vai acabar bebendo e sempre vai faltar.</em></li>
  <li>A resiliência a nível de plataforma é uma parte da composição da resiliência de uma aplicação, <strong>não a solução completa pra ela</strong>.</li>
  <li>O fluxo de retry deve ser implementado somente se as aplicações atrás delas tiverem <strong>mecanismos de idempotência</strong> para evitar duplicidades de registros, principalmente, falando em HTTP, de requests que são naturalmente não idempotentes como <strong>POST</strong> por exemplo.</li>
  <li>Os retry e circuit breaker dos meshs em geral <strong>não devem ser tratados como mecanismo de resiliência principal da solução</strong>. Não substitui a resiliência pragmática.</li>
  <li>Não substitui a resiliência a nível de código / aplicação. <em>Repetindo algumas vezes pra fixar</em>.</li>
  <li>Os circuit breakers e retentivas devem ser implementados a nível de código independente da plataforma suportar isso. Procure por soluções como <a href="https://github.com/resilience4j/resilience4j"><strong>Resilience4J</strong></a>, <a href="https://github.com/Netflix/Hystrix"><strong>Hystrix</strong></a>, <a href="https://github.com/sony/gobreaker"><strong>GoBreaker</strong></a>. Só pra constar.</li>
  <li>A busca por circuit breakers pragmáticos tende a prioridade em caso de downtime total de uma dependência, principalmente para buscar fluxos alternativos como fallback, não apenas para serem usados para “<em>dar erro mais rápido</em>”. Pense em “<em>posso ter um SQS como fallback para fazer temporariamente offload para os eventos que eu iria produzir no meu kafka que está com falha?</em>”, “<em>tenho um sistema de apoio para enfileirar as requisições que estão dando falha para processamento tardio</em>”?, “<em>eu posso reprocessar tudo que falhou quando minhas dependências voltarem?</em>” antes de qualquer coisa, beleza?</li>
</ul>

<p>Fico por aqui, espero ter ajudado! <a href="https://github.com/msfidelis/istio-disaster-recovery"><strong>Lembrando que todos os arquivos e aplicações estão neste repositório do Github.</strong></a></p>

<p><strong>Referencias / Material de Apoio:</strong></p>

<ul>
  <li><strong>Istio Traffic Management</strong> (<a href="https://istio.io/latest/docs/concepts/traffic-management/">https://istio.io/latest/docs/concepts/traffic-management/</a>)</li>
  <li><strong>Istio Traffic Management — Circuit Breaker</strong> (<a href="https://istio.io/latest/docs/tasks/traffic-management/circuit-breaking/">https://istio.io/latest/docs/tasks/traffic-management/circuit-breaking/</a>)</li>
  <li><strong>Envoy — Retry Policy</strong> — <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/router_filter#x-envoy-retry-on">https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/router_filter#x-envoy-retry-on</a></li>
  <li><strong>Chaos Mesh — Quickstart</strong> <a href="https://chaos-mesh.org/docs/quick-start/">https://chaos-mesh.org/docs/quick-start/</a></li>
  <li><strong>K6 Loading Testing —</strong> <a href="https://k6.io/docs/getting-started/running-k6/">https://k6.io/docs/getting-started/running-k6/</a></li>
  <li><strong>Setup do EKS com Istio e Terraform</strong> — <a href="https://github.com/msfidelis/eks-with-istio">https://github.com/msfidelis/eks-with-istio</a></li>
</ul>

<p><strong>Obrigado aos revisores:</strong></p>

<ul>
  <li>Rebeca Maia (<a href="https://twitter.com/rebecamaia_p">@rebecamaia_p</a>)</li>
  <li>Bruno Padilha (<a href="https://twitter.com/brunopadz">@brunopadz</a>)</li>
  <li>Leandro Grillo (<a href="https://twitter.com/leandrocgrillo">@leandrocgrillo</a>)</li>
</ul>

<p>Me <a href="https://twitter.com/fidelissauro"><strong>sigam no Twitter</strong></a> para acompanhar as paradinhas que eu compartilho por lá!</p>

<p>Te ajudei de alguma forma? Me pague um café</p>

<p><strong>Chave Pix:</strong> fe60fe92-ecba-4165-be5a-3dccf8a06bfc</p>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2021-11-13">13 Nov 2021</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="/categories#disaster-recovery">disaster-recovery</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#istio">istio</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#kubernetes">kubernetes</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="/categories#service-mesh">service-mesh</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            
            <a class="next d-block col-md-6 text-lg-right" href="//provisionando-um-cluster-de-eks-sem-node-groups-com-karpenter/">Provisionando um cluster de EKS sem Node Groups com Karpenter &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = ''; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="/assets/images/logo.png" alt="Matheus Fidelis Blog"> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://wowthemes.us11.list-manage.com/subscribe/post?u=8aeb20a530e124561927d3bd8&amp;id=8c3d2d214b" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="/categories#kubernetes">kubernetes (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#istio">istio (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#disaster-recovery">disaster-recovery (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#service-mesh">service-mesh (1)</a>
                
                    <a class="mt-1 mb-1" href="/categories#aws">aws (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#arquitetura">arquitetura (3)</a>
                
                    <a class="mt-1 mb-1" href="/categories#terraform">terraform (4)</a>
                
                    <a class="mt-1 mb-1" href="/categories#karpenter">karpenter (2)</a>
                
                    <a class="mt-1 mb-1" href="/categories#argo-rollouts">argo-rollouts (1)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2023 Matheus Fidelis 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>


<script src="/assets/js/lazyload.js"></script>


<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//.disqus.com/count.js"></script>



<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4Z30K3KNFW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4Z30K3KNFW');
</script>



</body>
</html>
