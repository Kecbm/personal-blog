---
layout: post
title: Provisionando um cluster de EKS sem Node Groups com Karpenter
canonical_url: https://medium.com/@fidelissauro/provisionando-um-cluster-de-eks-sem-node-groups-com-karpenter-4d302b32b620?source=rss-fc2fda5e9bc2------2
image: https://cdn-images-1.medium.com/max/1024/0*ZQy5ZyZidbgJThzP.png
---
<p><em>A proposta dessa PoC é criar e gerenciar um cluster de EKS utilizando apenas (ou quase) o
    </em><strong><em>Karpenter</em></strong><em> como provisionamento de recursos computacionais pro Workload produtivo,
        tirando a necessidade de Node Groups e Auto Scale Groups</em>. Trazendo todo o gerenciamento de recursos pra
    dentro de CRD&#39;s do <strong><em>Karpenter</em></strong>.</p>
<p><strong>Nesse cenário vamos assumir algumas premissas importantes:</strong></p>
<ul>
    <li>O objetivo do <strong>Karpenter</strong> como tecnologia é prover um &quot;<em>just in time</em>&quot; scale, o
        que faz dele uma proposta interessante para workloads que tenham picos de acesso, processamentos agendados mais
        pesados e tenham um delta de escalabilidade computacional mais agressivos.</li>
    <li>Essa proposta é excelente para muitos casos de uso, mas também é preciso assumir que gera uma volatilidade muito
        brusca na quantidade de nós e pods. Por isso é ideal que as aplicações e suas dependências estejam preparadas
        para morrer com segurança e aumentar ou diminuir o consumo de recursos na mesma proporção.</li>
    <li>Como um cluster de Kubernetes é composto por várias &quot;<em>pecinhas de lego</em>&quot; muito importantes, e
        que muitas vezes não estão preparadas para lidar com essa volatilidade agressiva para qual essa PoC está sendo
        desenhada, o modo mais intuitivo que trouxe para resolver esse cenário foi colocar os namespaces de serviço,
        como <strong>prometheus</strong>, kube<strong>-</strong>system e outras aplicações
        &quot;<em>satélites</em>&quot; em nodes Fargate, para que eles sejam poupados dessas mudanças bruscas de
        capacity.</li>
</ul>
<h3>Provisionamento</h3>
<p>Vou omitir bastante detalhes do código como um todo para não transformar esse artigo numa bíblia, mas fique tranquilo
    que todo o desenvolvimento está sendo documentado <a
        href="https://github.com/msfidelis/eks-karpenter-autonomous-cluster"><strong>neste repositório
            do GitHub</strong></a>.</p>
<p><a href="https://github.com/msfidelis/eks-karpenter-autonomous-cluster">GitHub -
        msfidelis/eks-karpenter-autonomous-cluster: Elastic Kubernetes Service fully managed using Karpenter Autoscaler,
        without Node Groups</a></p>
<h4>Roles de IAM — Cluster</h4>
<p>Como qualquer cluster de Kubernetes vamos precisar providenciar com antecedência 3 tipos de roles. Uma para o Control
    Plane, outra que será usada como Instance Profile para as instâncias dos Nodes e outra para os Fargate Profiles.
    Esse provisionamento não muda nada dos casos que você provavelmente já conhece.</p>
<p>Iniciando pela role do Control Plane precisamos associar 2 managed policies, <strong>AmazonEKSClusterPolicy</strong>
    e <strong>AmazonEKSServicePolicy</strong>.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/4ae2d41cdf60e0d35dd2b97278ac281a/href">https://medium.com/media/4ae2d41cdf60e0d35dd2b97278ac281a/href</a></iframe>
<h4>Roles de IAM — Nodes / Instance Profile</h4>
<p>O provisionamento da Instance Profile dos nodes também não muda caso você fosse usar com Node Groups, com exceção de
    que vamos precisar criar a instance profile propriamente dita. Quando utilizamos Node Groups o próprio serviço do
    EKS se encarrega de fazer a criação desse recurso caso não exista previamente. Mas é simples.</p>
<p>Vamos precisar associar algumas Managed Policies padrão também para funcionar. Mas sem segredo de outros tipos de
    provisionamento.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/b17b458dd002e6551e843522054cb063/href">https://medium.com/media/b17b458dd002e6551e843522054cb063/href</a></iframe>
<p>Vamos criar uma associação de instance profile na role criada para os nodes para posteriormente criamos o Launch
    Configuration com ela.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/51c3a19a02e7e99b435c94e83cded716/href">https://medium.com/media/51c3a19a02e7e99b435c94e83cded716/href</a></iframe>
<h4>Roles de IAM — Fargate Profiles</h4>
<p>O provisionamento da role dos Fargate Profiles também é padrão. Escrevendo esse artigo me vem aquela sensação de
    &quot;<em>pow, essas roles já poderiam existir na conta por default né? Chatão</em>&quot;. Pois é.</p>
<p>Funciona no mesmo esquema das anteriores, precisamos anexar algumas managed policies padrão para que o serviço
    funcione.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/1f39e7290719d9138b760719cb57d661/href">https://medium.com/media/1f39e7290719d9138b760719cb57d661/href</a></iframe>
<h3>EKS Cluster</h3>
<p>O provisionamento do cluster foi feito sem a base de um modulo ou facilitador. Até mesmo porque não seria
    interessante provisionar nada além do próprio control plane do EKS para PoC nesse primeiro momento.</p>
<p>Vamos utilizar o recurso base do <strong><em>aws_eks_cluster</em></strong> nos atentando as tags de discovery do
    Karpenter que precisam estar presentes.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/703bdac3a49cfda5f8d4a8a25395725d/href">https://medium.com/media/703bdac3a49cfda5f8d4a8a25395725d/href</a></iframe>
<h3>Fargate Profile — Kube System</h3>
<p>Como dito anteriormente nas premissas da PoC, tudo que se encaixar como um componente sistêmico do funcionamento da
    plataforma, e não como parte do workload será provisionado em <strong><em>Fargate Profiles</em></strong> para
    poupá-los da volatilidade de scale in / scale out que iremos trazer para o cluster com o Karpenter. Dito isso, vamos
    provisionar o fargate profile para o namespace do <strong><em>kube-system</em></strong>.</p><iframe src="" width="0"
    height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/7d131966c9069ee987158a999b29aa17/href">https://medium.com/media/7d131966c9069ee987158a999b29aa17/href</a></iframe>
<h3>CoreDNS Fix — Workaround</h3>
<p>Uma das coisas mais chatas e sem sentido do uso de cluster Full Fargate é a limitação do
    <strong><em>CoreDNS</em></strong> de subir naturalmente em nodes que não sejam EC2 efetivamente. Até a data desse
    artigo, é necessário utilizar de algum artificio automatizado ou manual para remover a label de
    <strong><em>eks.amazonaws.com/compute-type</em></strong> de ec2 para que ele consiga ser provisonado em
    nodes fargate.</p>
<p>Você pode fazer isso manualmente sem problemas diretamente com o kubectl.</p>
<pre>kubectl patch deployment coredns -n kube-system --type json -p=&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type&quot;}]</pre>
<p><em>Porém como preguiça pouca é besteira</em>, eu vou utilizar uma lambda que após o provisionamento do cluster se
    encarrega de remover essa label através da API do control plane.</p>
<p>Peguei a base inicial dessa lambda através do artigo <a
        href="https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-coredns-on-amazon-eks-with-fargate-automatically-using-terraform-and-python.html"><strong><em>Deploy
                CoreDNS on Amazon EKS with Fargate automatically using Terraform and Python</em></strong></a> escrito
    por <strong>Kevin Vaughan</strong> e <strong>Lorenzo Couto</strong> da AWS. Porém fiz algumas modificações de
    <em>stepback</em> e <em>retry</em> para realização dessa configuração porque algumas vezes falhava nas primeiras
    tentativas pela API não estar tão disponível quanto deveria.</p>
<p>O provisionamento dela está em um repositório de exemplo separado para ajudar em casos isolados e também futuramente
    transformar em módulo que resolve esse problema. <em>Dor de cabeça pro Matheus do futuro</em>.</p>
<p>No caso no Terraform iremos empacotar o script normalmente e criar a lambda na VPC que entregamos o cluster.</p>
<iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/2474f203dc3af8047dc4c0365a758cbe/href">https://medium.com/media/2474f203dc3af8047dc4c0365a758cbe/href</a></iframe>
<p>Nenhuma trigger é necessária para esse primeiro momento. Ao invés disso na pipeline vamos chamar um
    <strong><em>aws_lambda_invocation</em></strong> passando o <em>endpoint</em> do cluster e um <em>token</em>
    temporário que será utilizado para fazer o request com o Patch removendo a label. Isso irá forçar um redeploy do
    coreDNS, porém fazendo ele subir em nodes fargate com o planejado na própria execução da pipeline. Ganhando bastante
    tempo e diminuindo &quot;gols de mão&quot; do processo.</p><iframe src="" width="0" height="0" frameborder="0"
    scrolling="no"><a
        href="https://medium.com/media/add79cc3f948829172c15b52f364358a/href">https://medium.com/media/add79cc3f948829172c15b52f364358a/href</a></iframe>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*p18OyAlPritHNMoe5fHB5g.png" /></figure>
<p><a href="https://github.com/msfidelis/eks-coredns-fargate-fix">GitHub - msfidelis/eks-coredns-fargate-fix: Lambda
        setup to fix CoreDNS deployments to run on Fargate Clusters</a></p>
<h3>Provisionamento do Karpenter</h3>
<p>O provisionamento do Karpenter com Terraform e Helm não tem segredo. Exemplo foi adaptado direto da excelente
    documentação do projeto. São passos bem semelhantes dos que vimos até agora. Onde será necessário providenciar uma
    Role para o serviço com um assume role federado para o OIDC do cluster (exemplo completo no repositório).</p>
<h4>IAM Role</h4>
<p>A role do karpenter precisa ter algumas permissões bem semelhantes ao Cluster Autoscaler caso você já tenha
    utilizado. Ele precisa de algumas permissões de controle de EC2 para poder lançar e deletar elas sob demanda. Sem
    segredo por aqui.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/2129eb0627cf6fc9b775a9431ef4282b/href">https://medium.com/media/2129eb0627cf6fc9b775a9431ef4282b/href</a></iframe>
<h4>Karpenter — Fargate</h4>
<p>Segundo passo é colocar o Karpenter para rodar em Fargate Profile semelhante como fizemos com o
    <strong><em>kube-system</em></strong>, para impedir de que um <em>drain</em> de nodes afete o próprio karpenter e dê
    algum problema no processo de uma forma geral. Então, seguindo a premissa de que se não é workload, está seguro em
    fargate, vamos subir ele também.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/4738ebe14dbef5b78a3bf17da0f88158/href">https://medium.com/media/4738ebe14dbef5b78a3bf17da0f88158/href</a></iframe>
<h3>Karpenter — Helm</h3>
<p>O Setup do Helm é baseado no da documentação do Karpenter com Terraform também. Vamos passar a role que criamos
    amarrada ao OIDC e ao <strong><em>WebIdentityProvider</em></strong> na Service Account para que o controlador possa
    executar operações nas API&#39;s da AWS.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/9cb6b8e2ed47354dc4bc4a6568dbf8bc/href">https://medium.com/media/9cb6b8e2ed47354dc4bc4a6568dbf8bc/href</a></iframe>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/770/1*VAlTbC20qhEBdkBWfcJ0Ag.png" /></figure>
<p>Após o provisionamento teremos também o pod do Karpenter com os dois containers internos em estado de Ready/Running
    rodando em um Node Fargate idêntico aos do exemplo do <strong><em>kube-system</em></strong>.</p>
<h3>Karpenter — Provisioner e Templates</h3>
<p>Agora vamos trabalhar com o real diferencial dessa PoC com os demais tipos de provisionamento mais comuns. Caso você
    já tenha trabalhado com o provisionamento de clusters de EKS com Node Groups com Launch Templates customizados, essa
    parte será bem parecida.</p>
<p>No caso vamos criar um launch template versionado utilizando as AMI&#39;s recomendadas da AWS e a instance profile
    que criamos de antemão. Alguns passos de configuração como user-data foram omitidos do artigo, mas ressaltando que
    podem ser consultados no repositório de exemplo do artigo.</p><iframe src="" width="0" height="0" frameborder="0"
    scrolling="no"><a
        href="https://medium.com/media/3176bf620ebf5e7d2753f2de102ee803/href">https://medium.com/media/3176bf620ebf5e7d2753f2de102ee803/href</a></iframe>
<p>Em seguida vamos criar dois objetos pelo objeto <strong><em>kubectl_manifest</em></strong> do provider do kubectl
    para fazer deploy de dois recursos do CRD do Karpenter, um deles sendo o Provisioner onde vamos especificar os
    tamanhos de instancias, limites de CPU e memória e coisas relacionadas a capacity e outro sendo um AWSNodeTemplate
    onde vamos especificar os launch templates dos nodes.</p><iframe src="" width="0" height="0" frameborder="0"
    scrolling="no"><a
        href="https://medium.com/media/2227001c7aecf6f7c5ee17c364f91bda/href">https://medium.com/media/2227001c7aecf6f7c5ee17c364f91bda/href</a></iframe>
<p>Para facilitar eu optei por usar <strong><em>templatefile</em></strong> para criar os manifestos que seriam aplicados
    pelo provider do kubectl. No caso para ficar mais evidente, coloquei algumas variáveis para fazer o build dos YAMLs
    via template dessa forma:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/9e6dfd1a1d1808eb5cb0a7d94e5e937a/href">https://medium.com/media/9e6dfd1a1d1808eb5cb0a7d94e5e937a/href</a></iframe><iframe
    src="" width="0" height="0" frameborder="0" scrolling="no"><a
        href="https://medium.com/media/7a64448fed147e78e8304a55fbb31c3e/href">https://medium.com/media/7a64448fed147e78e8304a55fbb31c3e/href</a></iframe>
<p>No final será criado um resource parecido com o abaixo:</p><iframe src="" width="0" height="0" frameborder="0"
    scrolling="no"><a
        href="https://medium.com/media/a3070e811b7e4641f74d31df3c2ad816/href">https://medium.com/media/a3070e811b7e4641f74d31df3c2ad816/href</a></iframe>
<p><strong>Disclaimer:</strong> <em>Durante a PoC tentei utilizar o provider do kubernetes para criar os objetos
        customizados do Karpenter diretamente pelo kubernetes_manifests, porém existe um bug de dependências no resource
        que inviabiliza o provisionamento de CRD&#39;s juntamente com o cluster. Por isso precisei utilizar o kubectl
        provider para que continue sendo possível o provisionamento de toda a infraestrutura de uma única vez.</em></p>
<p><em>Abri uma issue que permanece aberta (até esse momento) pra isso:</em></p>
<p><a href="https://github.com/hashicorp/terraform-provider-kubernetes/issues/1775">Error: Failed to construct REST
        client on kubernetes_manifest resource · Issue #1775 · hashicorp/terraform-provider-kubernetes</a></p>
<h3>Aplicação de Testes</h3>
<p>Agora que temos todos os recursos do cluster minimamente provisionados, vamos testar o funcionamento do Karpenter.
    Vamos fazer deploy de uma aplicação de exemplo para ver se os nodes vão ser provisionados para suprir o novo
    capacity solicitado.</p>
<pre>❯ kubectl apply -f files/deploy/demo/chip.yaml<br>namespace/chip created<br>deployment.apps/chip created<br>service/chip created<br>horizontalpodautoscaler.autoscaling/chip created</pre>
<p>No caso foi provisionado para suprir os 2 novos pods solicitados para a aplicação de exemplo. Agora vamos executar os
    cenários de scale in e out para ver como o ambiente se comporta.</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DOlh-cIezLOTNGpjO1GWOQ.png" /></figure>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ylV5ifomSJw1UHOvIk0bQg.png" /></figure>
<h3>Cenário 1 — Scale In</h3>
<p>Vamos exemplificar o cenário onde temos 4 nodes iniciais no cluster, e vamos fazer o scale de um deployment de 2
    replicas para 100 de forma brusca para ver como o Karpenter vai lidar com essa mudança de capacity solicitado.</p>
<pre>kubectl scale --replicas 100 deployment/chip -n chip</pre>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*wYPdjUBQZPWPQPmCmiUNGw.png" /></figure>
<p><strong>Replicas iniciais do deployment:</strong> 4<br><strong>Replicas desejadas do deployment:</strong>
    100<br><strong>CPU Requests: </strong>250m<br><strong>RAM Requests:</strong> 512m<br><strong>Quantidade de Nodes
        Iniciais:</strong> 4<br><strong>Quantidade de Nodes final: </strong>25<br><strong>Horário do Apply:</strong>
    17:10:35<br><strong>Horário do scale finalizado:</strong> 17:13:50<br><strong>Tempo Total: </strong>00:03:25</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/proxy/1*27pFs8GxbM1F4FLA6aTlEg.png" /></figure>
<p>Conseguimos provisionar um capacity para suprir uma demanda brusca de 4 para 100 unidades computacionais que
    necessitavam de nodes em 3 minutos.</p>
<h3>Cenário 2 — Scale Out</h3>
<p>Agora vamos testar o cenário inverso, onde vamos fazer o scale out do ambiente de forma brusca para avaliar como
    karpenter vai lidar com esse capacity fora de uso</p>
<figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*ClVeK_eB__7F8osL1kYmRg.png" /></figure>
<pre>kubectl scale --replicas 4 deployment/chip -n chip</pre>
<p><strong>Replicas iniciais do deployment:</strong> 100<br><strong>Replicas desejadas do deployment:</strong>
    4<br><strong>CPU Requests:</strong> 250m<br><strong>RAM Requests:</strong> 512m<br><strong>Quantidade de Nodes
        Iniciais: </strong>25<br><strong>Quantidade de Nodes final:</strong> 4<br><strong>Horário do Apply:
    </strong>17:18:55<br><strong>Horário do scale finalizado:</strong> 17:19:50<br><strong>Tempo
        Total:</strong> 00:00:55</p>
<p>Para o scale out de nodes em desuso foi ainda melhor que scale in, fazendo um desligamento em massa de 25 nodes para
    4 em 55 segundos.</p>
<p><strong>Lembrando que toda a PoC foi disponibilizada no Github.</strong></p>
<p><a href="https://github.com/msfidelis/eks-karpenter-autonomous-cluster">GitHub -
        msfidelis/eks-karpenter-autonomous-cluster: Elastic Kubernetes Service fully managed using Karpenter Autoscaler,
        without Node Groups</a></p>
<h4>Obrigado aos Revisores:</h4>
<ul>
    <li><strong>Rafael Silva</strong> — <a href="https://mobile.twitter.com/rafaotetra"><em>@rafaotetra</em></a></li>
    <li><strong>Gabriel Machado</strong> — <a href="https://twitter.com/gmsantos__"><em>@gmsantos_</em></a></li>
    <li><strong>Somatorio </strong>— @somatorio</li>
</ul>
<h4>Referencias:</h4>
<ul>
    <li><strong>Karpenter — Getting Started with Terraform </strong>— <a
            href="https://karpenter.sh/v0.5.3/getting-started-with-terraform/">https://karpenter.sh/v0.5.3/getting-started-with-terraform/</a>
    </li>
    <li><strong>Karpenter Best Pratices</strong> — <a
            href="https://aws.github.io/aws-eks-best-practices/karpenter/">https://aws.github.io/aws-eks-best-practices/karpenter/</a>
    </li>
    <li><strong>Karpenter — Topology Spreads</strong> — <a
            href="https://karpenter.sh/v0.13.2/tasks/scheduling/#topology-spread">https://karpenter.sh/v0.13.2/tasks/scheduling/#topology-spread</a>
    </li>
    <li><strong>Deploy CoreDNS on Amazon EKS with Fargate automatically using Terraform and Python</strong> — <a
            href="https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-coredns-on-amazon-eks-with-fargate-automatically-using-terraform-and-python.html">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-coredns-on-amazon-eks-with-fargate-automatically-using-terraform-and-python.html</a>
    </li>
</ul>
<p>Me <a href="https://twitter.com/fidelissauro"><strong>sigam no Twitter</strong></a> para acompanhar as paradinhas que
    eu compartilho por lá!</p>
<p>Te ajudei de alguma forma? Me pague um café (<em>mentira, todas as doações são convertidas para abrigos de animais da
        minha cidade</em>)</p>
<p><strong>Chave Pix:</strong> fe60fe92-ecba-4165-be5a-3dccf8a06bfc</p><img
    src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4d302b32b620" width="1"
    height="1" alt="">