---
layout: post
image: assets/images/system-design/caching.png
author: matheus
featured: false
published: true
categories: [ system-design, engineering, cloud ]
title: System Design - Cache
---

Existem várias estratégias e tipos de cache. No inicio desse capítulo você vai notar uma constante evolução. Vamos começar com definições generalistas que podem ser reaproveitadas na maioria dessas possibilidades, até começarmos a abordar essas estratégias de formas mais específicas com suas particularidades.

<br>

# Definindo Cache

Cache, de uma forma simplificada, pode ser descrito como uma **técnica de otimização que se consiste entre criar uma camada de dados intermediários entre dois componentes**. Pode representar técnicas de otimização usadas para armazenar temporariamente dados que são caros ou demorados para recuperar de sua origem, ou também como camadas temporárias de resiliência. 

Normalmente, os dados armazenados em um cache **são o resultado de uma operação anterior ou cópias de dados armazenados em outro lugar**. Isso significa que o cache pode ser utilizado para evitar a sobrecarga de dependências e diminuir a consulta de dados que não mudam com grande frequencia deixando os mesmos em uma camada de dados "mais proxima" do cliente, ou armazenando os dados de um lugar mais caro, em outro mais barato. 

![Cache Exemplo](/assets/images/system-design/cache-exemplo.png)

> Exemplo de cacheamento em camadas de dados de uma interação entre cliente e servidor

Os dados armazenados no cache podem ser qualquer coisa, desde **resultados de consultas de banco de dados**, **dados solicitados de outros sistemas** dependentes até **assets e páginas da web completas**. **O cache é especialmente útil quando os dados não mudam frequentemente, mas são acessados com frequência.**

Existem algumas formas de implementar estratégias de cache. Erroneamente se associa o conceito de cache diretamente a flavors específicos de memory-databases, CDN's ou [proxies reversos](), mas é importante ressaltar que apesar de serem importantes, **essas tecnologias apenas implementam e possibilitam capacidades de cache, não as definem**. 

<br>

# Principios Básicos de Cache

Quando olhamos para diferentes estratégias e possibilidades de caching, alguns conceitos e desafios podem transitar entre eles de forma comum, independente da finalidade para qual as estratégias foram implementadas. O objetivo dessa sessão é detalhar alguns conceitos e tópicos interessantes que podem nos ajudar a compreender e projetar soluções de caching de forma mais inteligente e eficiente. 

## Consistencia de Dados 

A consistência entre o cache e o armazenamento de dados principal é crucial, e também pode apresentar um desafio caso esses dados tenham uma importancia muito grande em questão de atualização e consistência. Estratégias desenhadas para esse cenário devem garantir que o cache reflita as mudanças mais recentes no dado principal. Em sistemas muito distribuídos, esse pode representar o maior desafio. 

Por exemplo, em cenários hipotéticos onde exista a necessidade de cachear os dados cadastrais de um usuário em um sistema de compras, uma estratégia de cache seria interessante, afinal *"quantas vezes os dados cadastrais de um usuário podem mudar?"* Principalmente quando observado dentro de determinados períodos de tempo? Agora imagine o cenário onde** um usuário mudou seu endereço residencial**, ou por algum motivo mais complexo, **esse usuário foi desativado, exigindo sistemicamente que nenhuma outra ação do mesmo possa ser executada em nenhuma parte do sistema**. Para evitar uma inconsistência dessas modificações importantes entre o dado real e o dado em cache, evitando que produtos sejam enviados para o endereço errado, ou possibilitando que um usuário com atividade suspeita continue operando o sistema, **as operações que executam as escritas no dado original tem por obrigação deletar as chaves de cache que correspondam ao dado em questão, ou refazê-las com o estado atualizado**. 

## Time to Live (TTL)

O **Time to Live**, ou mais conhecido simplesmente por **TTL**, é uma configuração ou capacidade que define **um período de vida de um item no cache**, e quando expirado, ele é automaticamente removido ou marcado como inválido, ambas com algumas diferenças de implementação, mas servem para sinalizar que o item seja renovado. O TTL é uma capacidade quase mandatória para sistemas de larga escala, pois permite que dados muito antigos acabem sendo um ofensor para a consistência de dados, garantindo uma reciclagem periódica, e também para evitar o consumo de recursos caso a implementação guarde itens que não são mais necessários e sem acesso. 


## Politicas de Evicção e Substituição

A **Evicção**, **Eviction** ou **Politica de Substituição**, refere-se a **politicas e mecanismos que um sistema de cache usa para decidir quais itens remover quando a capacidade de alocação de cache atinge seu máximo**. Imagine qum mecanismo de cache que possua capacidade de alocar 1000 itens que foi totalmente utilizado, esse mecanismo recebe a solicitação de salvar um item novo, porém não há espaço. De acordo com a politica estabelecida, a operação irá excluir o item mais antigo, menos acessado e irá remove-lo para dar espaço para esse novo item. As estratégias de eviction podem ser utilizadas para garantir que os itens mais relevantes e frequentemente acessados serão mantidos nesses casos, deletando itens que raramente são requisitados primeiro. As politicas de evição incluem: 

* **Least Recently Used** (`LRU`): Neste método, o item que não foi usado há mais tempo é removido primeiro. Isso é baseado na suposição de que se um item não foi usado recentemente, é menos provável que seja usado no futuro próximo.

* **Least Frequently Used** (`LFU`): Evita os itens que são menos frequentemente acessados, este método remove os itens que foram usados com menos frequência. Isso pode ser mais eficiente do que o LRU em alguns casos, mas é mais difícil de implementar porque requer rastreamento da frequência de uso de cada item.

* **First In, First Out** (`FIFO`): Evita os itens na ordem em que foram adicionados, este é um método simples onde o primeiro item a entrar no cache é o primeiro a sair. Isso é fácil de implementar, mas pode não ser o mais interessante, pois não leva em consideração a frequência de uso dos itens.

* **Random Replacement** (`RR`): Neste método, um item aleatório é selecionado para ser removido. Embora seja simples de implementar, não leva em consideração a frequência de uso dos itens.

## Invalidação de Itens em Cache

A **Invalidação de Cache** é o processo ou capacidade de remover ou marcar dados no cache como inválidos. Essa operação pode ocorrer pragmaticamente pela lógica da execução de um algoritmo onde itens específicos possam ser excluídos de forma individual por não terem mais utilidade para o processo, de forma manual onde através de comandos ou operações os itens possam ser invalidados individualmente ou em grupo, ou através do TTL, onde a invalidação do item acontece depois de um período específico. 

## Eventos de Hit Rate, Cache Hit e Cache Miss

Em sistemas que fazem uso de estratégias de cache para otimizar o acesso a dados, dois eventos são presentes e deveriam ser monitorados: `cache hit` e `cache miss`. Esses eventos contibuem para a avaliação da eficiência de um sistema de cache dando dados para serem avaliados em questão de desempenho, performance e efetividade.

#### Cache Hit

Um evento de **cache hit ocorre quando uma solicitação de dados encontra o conteúdo desejado já armazenado no cache**. Isso significa que o sistema pode **entregar o dado solicitado diretamente do cache, sem a necessidade de acessar a fonte de dados original**, como um banco de dados ou um sistema de arquivos, que seria significativamente mais lento. A **alta taxa de cache hits é geralmente indicativa de um sistema de cache bem otimizado**, que efetivamente reduz acessos a fontes de dados mais lentas.

#### Cache Miss

Um evento de **cache miss acontece quando a solicitação de dados não encontra o conteúdo desejado no cache**. Isso significa que o sistema **precisa buscar o dado na fonte de dados original**. Um cache miss geralmente resulta em maior latência para a solicitação, pois acessar a fonte de dados original é mais demorado e custoso recuperar o mesmo dado em relação ao cache. Gerenciar e **minimizar cache misses é um aspecto fundamental do design e otimização de sistemas de cache**, envolvendo estratégias como a **previsão de padrões de acesso a dados e a otimização de políticas de evicção de cache**.
 
Em operações onde **limpezas totais ou parciais de cache podem resultar em um pico de cache misses por algum período de tempo** até os itens em cache serem refeitos, porém sistemas que possuem um **volume alto e constante de cache misses em comparação com os cache hits, podem indicar uma ineficiência do uso do cache** em questão e uma oportunidade de otimização. 

#### Hit Rate - Taxa de Acertos

A relação entre cache hits e cache misses são diretamente vinculádos a eficácia de um sistema de cache. A taxa de acerto, hit rate, é calculada como o número de cache hits dividido pelo número total de solicitações (hits + misses), geralmente expressa em porcentagem. **Uma taxa de acerto mais alta indica uma maior eficiência do cache, enquanto uma taxa de acerto baixa sugere que há espaço para otimização** ou até mesmo justificando a remoção dessa camada de cache.

\begin{equation}
\text{Total de Solicitações} =  {\text{Cache Hits}} + {\text{Cache Miss}} 
\end{equation}

\begin{equation}
\text{Taxa de Acertos (Hit Rate)} = \left( \frac{\text{Cache Hits}}{\text{Total de Solicitações}} \right) \times 100
\end{equation}

Vamos supor que em um sistema, temos dentro de um período de tempo, **800 cache hits** e **200 cache misses**. A taxa de acertos seria calculada da seguinte forma:

\begin{equation}
\text{Total de Solicitações} =  {\text{800}} + {\text{200}} 
\end{equation}

\begin{equation}
\text{Taxa de Acertos (Hit Rate)} = \left( \frac{\text{800}}{\text{1000}} \right) \times 100
\end{equation}

\begin{equation}
\text{Taxa de Acertos (Hit Rate)} = \text{80%}
\end{equation}



<br>

# Implementações de Cache 

Como já foi mencionado anteriormente, o cache em si é uma estratégia que visa cumprir uma finalidade específica e não uma tecnologia em específico, e também vimos que existem vários tipos de implementação de caching possíveis. O objetivo dessa sessão é detalhar alguns dos principais usos e aplicações de caching. A

## Cache em Memória (Hashmap)

O Cache em Memória é uma estratégia útil para otimizar o desempenho de aplicações em uma escala mais simplificada, pois mesmo que em condições isoladas a uma única execução ou processo, ainda é efetiva reduzindo o tempo de acesso a dados e diminuindo a carga sobre recursos mais lentos. Dentro das várias implementações de cache, o uso de estruturas de dados baseadas em hashmap é a minha alternativa favorita devido à sua simplicidade e performance no tempo de acesso, e facilidade de executar operações básicas.

Essa estratégia é muito comum em estruturas de dados, e se consiste em criar um mapa de itens baseados em chave-valor dentro de uma lista em memória disponível localmente para a aplicação. Uma vez existindo essa lista chave-valor onde cada item salvo num HashMap pode ser referenciado por uma chave única atrelada a ele para recuperar o seu valor, temos uma capacidade local interessante de cache dentro de uma execução ou processo. 

Abaixo temos uma implementação simples do uso de hashmap para criar uma capacidade de cache. Lembrando que implementações que vão trabalhar com quantidades significativas de itens por um longo período de tempo devem se atentar a implementar estratégias de invalidação desses itens para evitar problemas de leaks e saturação de memória disponível. 

```go
package main

// ...

// Define a estrutura para o nosso cache em memória com hashmap
type MemoryCache struct {
	items map[string]interface{}
	mutex sync.RWMutex // mutex simples para garantir a sincronização durante a leitura/escrita
}

// cacheInstance é uma instância do cache, será usado para implementar o padrão singleton
// Garantindo que a criação do cache seja realizada apenas uma vez, independente de quantas
// Vezes for recuperada pela aplicação
var cacheInstance *MemoryCache
var once sync.Once

// GetCacheInstance retorna a instância única do cache
func GetCacheInstance() *MemoryCache {
	once.Do(func() {
		cacheInstance = &MemoryCache{
			items: make(map[string]interface{}),
		}
	})
	return cacheInstance
}

// Adiciona ou atualiza um valor no cache com a chave fornecida
func (c *MemoryCache) Set(key string, value interface{}) {
	c.mutex.Lock()
	defer c.mutex.Unlock()
	c.items[key] = value
}

// Get retorna um valor do cache se ele existir
func (c *MemoryCache) Get(key string) (interface{}, bool) {
	c.mutex.RLock()
	defer c.mutex.RUnlock()
	value, found := c.items[key]
	return value, found
}

// Utilizando o padrão de cache criado
func main() {

	// Obtendo a instância do cache
	cache := GetCacheInstance()

	// Adicionando alguns usuários hipotéticos ao cache
	cache.Set("user:1", "Matheus Fidelis")
	cache.Set("user:2", "Tarsila Bianca")

	// Teste 1: Recuperando valores do cache
	if userName, found := cache.Get("user:1"); found {
		fmt.Println("Found user:1 ->", userName)
	} else {
		fmt.Println("user:1 not found in cache")
	}

	// Teste 2: Recuperando valores do cache
	if userName, found := cache.Get("user:2"); found {
		fmt.Println("Found user:2 ->", userName)
	} else {
		fmt.Println("user:2 not found in cache")
	}

	// Teste 3: Procurando um item que não existe em cache
	if userName, found := cache.Get("user:3"); found {
		fmt.Println("Found user:3 ->", userName)
	} else {
		fmt.Println("user:3 não encontrado em cache")
	}

}
```

### Caching em Sistemas Distribuídos

### Cache em Databases e Camadas de Dados

Os [Bancos de dados](/teorema-cap/) são muitas vezes o maior gargalo em aplicações de software devido ao custo computacional associado à execução de escrita, leitura, concorrência e persistência dos dados a longo prazo. Devido a maioria das opções mais comuns de mercado não serem sensíveis a escalabilidade horizontal, a camada de dados tente a ser uma das partes mais complexas de se lidar em questão de escala. O cache quando aplicado a resolver problemas de escalabilidade de databases ajuda a mitigar esse gargalo, armazenando resultados de consultas inteiras ou registros frequentemente acessados em uma outra camada em memória mais barata e rápida de ser consultada. 

Existem algumas estratégias que podemos adotar para resolver esse problema. Vamos abordar algumas de forma simplificada a seguir. 

#### Cache-Aside (Lazy Load)

Uma estratégia de cache-aside é a estratégia mais comum disponível quando olhamos para implementações de  caching de databases. A lógica pode ser resumida na propria aplicação criar o cache sob demanda, conforme os dados são consultados. Quando a execução de algum algoritmo precisa ler dados de um banco de dados, primeiro ele verifica no seu sistema de cache para saber se os dados buscados estão disponíveis. Caso o dado já esteja em cache e forem retornados (cache hit), a resposta é retornada da camada de cache e o processamento segue adiante. Caso o dado não esteja disponível (cache miss), o banco de dados principal é consultado, as informações são imediatamente colocadas em cache para as próximas consultas, e a execução segue para os proximos passos. Invariávelmente essa ação de criar o cache pela primeira vez tente a demorar um pouco mais. 


![Database](/assets/images/system-design/cache-database.png)
> Lógica de consulta e construção de cache em bancos de dados utilizando estratégias de Cache-Aside

Embora essa estratégia de cache ofereça melhorias significativas de performance, ele também introduz complexidade na gestão de consistência de dados como já mencionado anteriormente. 

```go
package main

import (
	"context"
	"database/sql"
	"fmt"
	"log"

	"github.com/go-redis/redis/v8"
)

var ctx = context.Background()

func main() {

	// Conexão com o Redis.
	rdb := redis.NewClient(&redis.Options{
		Addr:     "localhost:6379",
		Password: "",
		DB:       0,
	})

	// Conexão com o Banco de Dados
	mysqlDSN := "usuario:senha@tcp(localhost:3306)/produtos"
	db, err := sql.Open("mysql", mysqlDSN)
	if err != nil {
		log.Fatal(err)
	}
	defer db.Close()

	// ID do pedido que desejamos buscar
	pedidoID := "1"

	// Busca no cache pela chave criada
	valor, err := rdb.Get(ctx, "pedido:"+pedidoID).Result()

	// Verifica se o pedido está ou não em cache
	if err == rdb.Nil {
		fmt.Println("Produto não encontrado no cache")

		// Se não estiver em cache, busca no database
		query := `SELECT valor FROM pedidos WHERE id = ?`

		err := db.QueryRow(query, pedidoID).Scan(&valor)
		if err != nil {
			log.Fatal(err)
		}

		// Armazena o resultado no cache Redis para consultas futuras
		err = rdb.Set(ctx, "pedido:"+pedidoID, valor, 0).Err()
		if err != nil {
			log.Fatal(err)
		}

		// Exibe o valor
		fmt.Println("Pedido recuperado do banco de dados e armazenado no cache:", valor)
	} else {
		fmt.Println("Pedido recuperado do cache:", valor)
	}

}
```

#### Write-Through (Escrita Dupla)

As abordagens de write-through, ou abordagens de escrita dupla, tendem a ser aplicadas onde o cache é pensado de forma mais durável. Basicamente essa estratégia se baseia em atualizar a versão mais recente do dado simultâneamente na camada de dados principal e no cache assim que é modificada. Para fins de consistência, é comum que as abordagens de cache-aside e write-through sejam implementadas de forma complementar. 


```go
package main

import (
	"context"
	"database/sql"
	"fmt"
	"log"

	_ "github.com/go-sql-driver/mysql"
	"github.com/go-redis/redis/v8"
)

func main() {
	var ctx = context.Background()

	// Simulação da conexão com o Redis
	// Substitua estas variáveis pelos seus valores reais para uma conexão funcional
	redisClient := redis.NewClient(&redis.Options{
		Addr:     "localhost:6379", // Endereço do servidor Redis
		Password: "",               // Senha, se houver
		DB:       0,                // Banco de dados padrão do Redis
	})

	// Simulação da conexão com o MySQL
	// Substitua estas variáveis pelos seus valores reais para uma conexão funcional
	mysqlDSN := "usuario:senha@tcp(localhost:3306)/nome_do_banco"
	db, err := sql.Open("mysql", mysqlDSN)
	if err != nil {
		log.Fatal(err)
	}
	defer db.Close()

	// Tentativa de ping no banco de dados para garantir a conexão
	err = db.Ping()
	if err != nil {
		log.Fatal("Falha ao conectar no banco de dados: ", err)
	}

	// Aqui começa a lógica de write-through
	pedidoID := "1"
	value := "20.00"

	// Primeiro, insere ou atualiza o valor no banco de dados
	query := "INSERT INTO pedidos (id, valor) VALUES (?, ?) ON DUPLICATE KEY UPDATE valor = VALUES(valor)"
	_, err = db.Exec(query, "pedido:"+pedidoID, value)
	if err != nil {
		log.Fatal("Erro ao inserir/atualizar no banco de dados: ", err)
	}

	// Imediatamente após, atualiza o valor no cache Redis
	err = redisClient.Set(ctx,  "pedido:"+pedidoID, value, 0).Err()
	if err != nil {
		log.Fatal("Erro ao atualizar o cache: ", err)
	}

	fmt.Println("Dado inserido no banco de dados e atualizado no cache com sucesso.")
}

```

#### Write-Behind (Lazy Writing)

### Cache e Consistência de Dados 

### Cache em Memória (In-Memory Cache)

<br>

### Cache de Conteúdo Distribuído (CDN Cache)

O **Cache de Conteúdo Distribuído**, também muito referido como **CDN** *(Content Delivery Network)*, é uma infraestrutura de rede estrategicamente distribuída que tem como objetivo otimizar a entrega de conteúdo. A CDN funciona **armazenando cópias de conteúdo estático, como imagens, vídeos, arquivos CSS e JavaScript, em vários servidores localizados em diferentes regiões geográficas**. 

![Cache: CDN](/assets/images/system-design/cdn-diagrama.png)

O processo começa quando um usuário solicita um arquivo hospedado em uma CDN. Ao invés de enviar esta solicitação diretamente ao servidor de origem, a CDN **redireciona a solicitação para o servidor mais próximo do usuário com o conteúdo em cache**, baseando-se em fatores como proximidade geográfica, checagens de saúde e latência da rede. 

O funcionamento de grande parte das opções de mercado se baseia na mesma lógica de cacheamento que vimos até agora, **indo buscar os arquivos estáticos no servidor de origem quando não estão em cache**, e **enviando os arquivos cacheados economizando requisições que seriam direcionadas para o mesmo**, com excessão que algumas opções podem implementar replicação dos arquivos em cache em diferentes pontos geográficos de forma assincrona. 

O objetivo dessa abordagem é **reduzir a distância em que os dados frequentemente acessados percorrem**, diminuindo a latência, carga do servidor de origem e melhorando a experiência do usuário final. **Essa estratégia é melhor empregada em aplicações web com muitos recursos estáticos**, onde **armazenar conteúdo que tem uma periodicidade de mudança relativamente baixa, mas com uma quantidade de solicitações muito alta** próximo aos usuários, o tempo de resposta das requisições até a origem é diminuída, resultando em carregamento de páginas mais rápidos.

Uma função agregada das CDNs é **poder lidar com picos súbitos de tráfego**, evitando que o servidor de origem fique sobrecarregado e potencialmente indisponível. Muitos dos produtos desenvolvidos para essa finalidade tem proteções contra ataques de negação de serviço DDoS *(Distributed Denial of Service)*, firewalls, filtros de pacotes e detecção de ameaças agregados na solução. 

Para aplicações que possuam um ciclo de desenvolvimento constante, e que façam uso de Cache Distribuído para ganho de performance é quase indispensável implementar **estratégias eficazes de invalidação de cache** nas suas pipelines e no seu ciclo de entrega de software para garantir que os usuários finais recebam a versão mais atualizada do conteúdo quando a mesma for disponibilizada.

Conceitualmente e resumidamente, o cache baseado em conteúdo se baseia na estratégia de oferecer uma camada intermediária sistemática entre cliente e servidor de origem como já explicamos. No algoritmo abaixo, criamos um servidor HTTP que faz a intermediação entre o cliente o site google.com.br. Quando recebemos a requisição para um recurso, criamos um hash identificador e checamos se esse arquivo existe em disco. Caso exista, a requisição para a origem não é realizada, retornando o conteúdo diretamente do conteúdo cacheado localmente. Caso exista, o recurso é solicitado para a origem e em seguida identificado e armazenado em cache. 

```go
// ...

const origem = "https://google.com.br"
const cacheDir = "./cache"
const port = 8080

// Função que cria um hash do conteúdo solicitado pela URL
func generateHash(input string) string {
	hash := sha1.New()
	hash.Write([]byte(input))
	return hex.EncodeToString(hash.Sum(nil))
}

func ProxyCacheHandler(w http.ResponseWriter, r *http.Request) {

	var body []byte

	// Tempo inicial da requisição
	startTime := time.Now()

	// Define o diretório do cache do recurso calculando a hash da URL
	cachePath := filepath.Join(cacheDir, generateHash(r.URL.Path))

	// Verifica se o recurso está em cache
	_, err := os.Stat(cachePath)

	// Caso não esteja, recupera o recurso do servidor
	if os.IsNotExist(err) {
		fmt.Println("Recurso não está presente em cache, buscando na origem:", r.URL.Path)

		// Constroi a URL do recurso
		url := fmt.Sprintf("%s%s", origem, r.URL.Path)

		// Realiza a requisição para a origem para buscar o recurso
		resp, err := http.Get(url)
		if err != nil {
			http.Error(w, "Server Error", http.StatusInternalServerError)
			log.Println("Falha ao buscar o recurso na origem:", err)
			return
		}
		defer resp.Body.Close()

		// Lê o conteúdo da resposta
		body, err = ioutil.ReadAll(resp.Body)
		if err != nil {
			http.Error(w, "Server Error", http.StatusInternalServerError)
			log.Println("Falha ao ler a resposta do servidor:", err)
			return
		}

		// Salva o arquivo em cache com o conteúdo do recurso
		ioutil.WriteFile(cachePath, body, 0644)
	} else {
		// Caso esteja em cache, lê o arquivo e retorna no response
		fmt.Println("Recurso está presente em cache:", r.URL.Path, cachePath)

		// Lê o arquivo em cache
		body, err = ioutil.ReadFile(cachePath)
		if err != nil {
			http.Error(w, "Server Error", http.StatusInternalServerError)
			log.Println("Falha ao ler o cache:", err)
			return
		}
	}

	// Tempo total da requisição
	fmt.Println(fmt.Sprintf("Tempo total da requisição para o recurso %v:  %v", r.URL.Path, time.Since(startTime)))

	// Resposta cacheada da requisição
	w.Write(body)

}

func main() {

	// Cria o diretório de armazenamento do cache
	fmt.Println("Criando diretório de cache:", cacheDir)
	if _, err := os.Stat(cacheDir); os.IsNotExist(err) {
		os.Mkdir(cacheDir, os.ModePerm)
	}

	// Cria um server HTTP simples para fazer handling dos requests
	fmt.Println("Iniciando Proxy para a origem:", origem)
	http.HandleFunc("/", ProxyCacheHandler)

	fmt.Println("Proxy iniciado na porta:", port)
	log.Fatal(http.ListenAndServe(fmt.Sprintf(":%v", port), nil))
}
```

#### Primeiro Acesso

```
Criando diretório de cache: ./cache
Iniciando Proxy para a origem: https://google.com.br
Proxy iniciado na porta: 8080
Recurso não está presente em cache, buscando na origem: /
Tempo total da requisição para o recurso /:  795.700625ms
Recurso não está presente em cache, buscando na origem: /client_204
Recurso não está presente em cache, buscando na origem: /images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png
Recurso não está presente em cache, buscando na origem: /textinputassistant/tia.png
Recurso não está presente em cache, buscando na origem: /images/nav_logo229.png
Recurso não está presente em cache, buscando na origem: /xjs/_/js/k=xjs.hp.en.v2grbV-lSNQ.O/am=AAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAA4AAAAAiAAAAAABgAAAAAAAAACABxERwAwAEcAAHgB/d=1/ed=1/rs=ACT90oES1zvGValamnA-977V6dGcCu-eaQ/m=sb_he,d
Tempo total da requisição para o recurso /xjs/_/js/k=xjs.hp.en.v2grbV-lSNQ.O/am=AAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAA4AAAAAiAAAAAABgAAAAAAAAACABxERwAwAEcAAHgB/d=1/ed=1/rs=ACT90oES1zvGValamnA-977V6dGcCu-eaQ/m=sb_he,d:  107.101708ms
Tempo total da requisição para o recurso /textinputassistant/tia.png:  132.588208ms
Tempo total da requisição para o recurso /images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png:  132.977041ms
Tempo total da requisição para o recurso /images/nav_logo229.png:  130.294708ms
Tempo total da requisição para o recurso /client_204:  211.213916ms
Recurso está presente em cache: /images/nav_logo229.png cache/d99d33e5ee22dee6f248b342095f1382cc3a9580
Tempo total da requisição para o recurso /images/nav_logo229.png:  411.792µs
Recurso não está presente em cache, buscando na origem: /gen_204
Tempo total da requisição para o recurso /gen_204:  179.880792ms
```


#### Segundo Acesso

```
Recurso está presente em cache: / cache/42099b4af021e53fd8fd4e056c2568d7c2e3ffa8
Tempo total da requisição para o recurso /:  357.791µs
Recurso está presente em cache: /images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png cache/ffa840af19c091b0e9304cae9327510f5c5c6e0d
Tempo total da requisição para o recurso /images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png:  306.625µs
Recurso está presente em cache: /textinputassistant/tia.png cache/d61ec3ad1d7c3687061c89561ac34d4a40659823
Tempo total da requisição para o recurso /textinputassistant/tia.png:  1.7355ms
Recurso está presente em cache: /xjs/_/js/k=xjs.hp.en.v2grbV-lSNQ.O/am=AAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAA4AAAAAiAAAAAABgAAAAAAAAACABxERwAwAEcAAHgB/d=1/ed=1/rs=ACT90oES1zvGValamnA-977V6dGcCu-eaQ/m=sb_he,d cache/0590c87359355c9a948ac0829a030a371e46d959
Tempo total da requisição para o recurso /xjs/_/js/k=xjs.hp.en.v2grbV-lSNQ.O/am=AAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAA4AAAAAiAAAAAABgAAAAAAAAACABxERwAwAEcAAHgB/d=1/ed=1/rs=ACT90oES1zvGValamnA-977V6dGcCu-eaQ/m=sb_he,d:  676.375µs
Recurso está presente em cache: /images/nav_logo229.png cache/d99d33e5ee22dee6f248b342095f1382cc3a9580
Tempo total da requisição para o recurso /images/nav_logo229.png:  103.5µs
Recurso está presente em cache: /client_204 cache/9036ec3b37560314f1df05b153d3486ae6a8f808
Tempo total da requisição para o recurso /client_204:  87.375µs
Recurso está presente em cache: /images/nav_logo229.png cache/d99d33e5ee22dee6f248b342095f1382cc3a9580
Tempo total da requisição para o recurso /images/nav_logo229.png:  112.833µs
Recurso está presente em cache: /gen_204 cache/b55d8b2989794808c756b64e38355d9a0920bd30
Tempo total da requisição para o recurso /gen_204:  118.541µs
```

### Referencias

[Cache Strategies](https://medium.com/@mmoshikoo/cache-strategies-996e91c80303)

[Caching patterns](https://docs.aws.amazon.com/whitepapers/latest/database-caching-strategies-using-redis/caching-patterns.html)

[Introduction to database caching](https://www.prisma.io/dataguide/managing-databases/introduction-database-caching)

[Top Caching Strategies](https://blog.bytebytego.com/p/top-caching-strategies)

[Cache Eviction Strategies Every Redis Developer Should Know](https://redis.com/blog/cache-eviction-strategies/)

[Cache Hit e Cache Miss](https://www.hostinger.com.br/tutoriais/cache-miss)

[Caching patterns](https://docs.aws.amazon.com/whitepapers/latest/database-caching-strategies-using-redis/caching-patterns.html)


[Azure Architecture: Cache-Aside](https://learn.microsoft.com/pt-br/azure/architecture/patterns/cache-aside)

{% include latex.html %}